{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI University","text":"<p>Welcome to the AI University Curriculum. Use the left menu to pick a week.</p>"},{"location":"cheat-sheet/","title":"Cheat Sheets","text":"<p>Quick Reminder Cheat Sheets</p>"},{"location":"cheat-sheet/#1-everyday-workflow-edit-preview-commit-deploy","title":"1. Everyday Workflow (Edit \u2192 Preview \u2192 Commit \u2192 Deploy)","text":"<p>Local preview</p> <p><pre><code>python -m mkdocs serve\n</code></pre> Open: http://127.0.0.1:8000/ (Leave terminal running; Ctrl+C stops it.)</p> <p>Commit &amp; push <pre><code>git add docs/...\ngit commit -m \"Your message\"\ngit push origin main\n</code></pre> Publish to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#2-common-git-commands","title":"2. Common Git Commands","text":"<pre><code>git status                 # see what changed\ngit add &lt;file&gt;             # stage a file\ngit add .                  # stage everything\ngit commit -m \"message\"    # commit staged changes\ngit push origin main       # push to GitHub\ngit pull origin main       # pull latest from GitHub\n</code></pre> <p>Undo a staged file: <pre><code>git restore --staged &lt;file&gt;\n</code></pre></p>"},{"location":"cheat-sheet/#3-mkdocs-basics","title":"3. MkDocs Basics","text":"<p>Edit pages in <code>docs/</code> Navigation lives in <code>mkdocs.yml</code> under <code>nav:</code></p> <p>Rebuild local site <pre><code>python -m mkdocs serve\n</code></pre></p> <p>Deploy to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#add-a-new-page","title":"Add a new page","text":"<ol> <li>Create <code>docs/new-page.md</code></li> <li>Add to <code>mkdocs.yml</code>: <pre><code>nav:\n  - Home: index.md\n  - Week 1: week-1.md\n  - Week 2: week-2.md\n  - Cheat Sheets: cheat-sheet.md\n  - New Page: new-page.md\n  ```\n\n## 4. VS Code Tips\nToggle terminal: **Ctrl + `**\nCommand Palette: **Ctrl + Shift + P**\nFind/Replace regex: **Ctrl + H**, click `.*`\nReload the window (fix UI glitches): **Developer: Reload Window**\n\n  ## 5. Troubleshooting\n```bash\npython -m mkdocs serve\n</code></pre></li> </ol> <p>Use <code>http://127.0.0.1:8000/</code> (colon, not dot).</p>"},{"location":"cheat-sheet/#github-pages-not-updated","title":"GitHub Pages not updated","text":"<pre><code>python -m mkdocs gh-deploy\n</code></pre> <p>Wait ~30 seconds and refresh the live site.</p>"},{"location":"cheat-sheet/#site-shows-up-in-git-changes","title":"<code>site/</code> shows up in Git changes","text":"<p>Ensure <code>.gitignore</code> contains <code>site/</code>.</p> <p>If it's already tracked: <pre><code>git rm -r --cached site\ngit commit -m \"Remove generated site\"\ngit push origin main\n</code></pre></p>"},{"location":"week-1/","title":"History of AI &amp; Math Foundations","text":""},{"location":"week-1/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives</p> <p>By the end of this lesson, you will be able to:</p> <ul> <li>Describe three pivotal AI milestones and their lasting impact.</li> <li>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</li> <li>Understand probability fundamentals\u2014random variables, expectation, variance\u2014and compute them by hand and in Python.</li> <li>Grasp key linear algebra concepts\u2014vectors, matrices, dot products, matrix multiplication\u2014and see how they underpin AI algorithms.</li> <li>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</li> </ul>"},{"location":"week-1/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition &amp; Citation Example Artificial Intelligence (AI) \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956 A chatbot that interprets questions and crafts human\u2011like responses. Machine Learning (ML) Algorithms that improve performance on tasks by learning from data rather than explicit programming. A regression model that learns to predict steel prices from historical sales. Data Science Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data. Cleaning and visualizing e\u2011commerce logs to uncover purchasing trends."},{"location":"week-1/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-1/#a-ai-milestones","title":"A. AI Milestones","text":"The Dartmouth Workshop (1956) <p>What happened: In summer 1956, John\u202fMcCarthy, Marvin\u202fMinsky, Nathaniel\u202fRochester, and Claude\u202fShannon met at Dartmouth College to ask: \u201cCan machines simulate human intelligence?\u201d They coined \u201cArtificial Intelligence\u201d and proposed studying how machines might \u201clearn from experience,\u201d \u201cmake abstractions,\u201d and \u201cuse language.\u201d</p> <p>Context &amp; significance: - Pre\u20111956, computers = number crunchers. Dartmouth reframed them as potential thinking machines. - Sparked optimism (and funding) that small teams could crack \u201cevery aspect of learning.\u201d</p> <p>First programs:     - Logic Theorist (1955) \u2013 Newell &amp; Simon proved logic theorems with a program.     - General Problem Solver (1957) \u2013 Early universal reasoning attempt.</p> <pre><code>!!! note \"Why this still matters\"\n    - Understanding the **hype \u2192 disappointment \u2192 AI winters** cycle helps you stay realistic about today\u2019s claims.  \n    - Symbolic reasoning/search ideas from this era live on in **knowledge graphs** and **constraint solvers**.\n</code></pre> Expert Systems Era (1970s\u20131980s) <p>Core idea: Encode expert knowledge as IF\u2013THEN rules.</p> <pre><code>IF symptom = fever AND symptom = rash\nTHEN suggest = measles\n</code></pre> <p>MYCIN (1972\u20131980): - ~600 rules to diagnose bacterial infections &amp; suggest antibiotics - Matched/surpassed human experts in blind tests</p> <p>Strengths vs. limits: - \u2705 Transparent logic (traceable to specific rules) - \u274c Hard to scale (thousands of hand\u2011written rules), weak with uncertainty</p> <p>Modern relevance</p> <ul> <li>Rule\u2011based logic still used in finance/healthcare compliance.  </li> <li>Today\u2019s hybrid systems: rules for regulation + ML models for scoring.</li> </ul> Deep Learning Boom (2010s\u2013Present) <p>Key breakthrough \u2013 AlexNet (2012): - 8\u2011layer CNN, cut ImageNet error rate in half (1.2M images, 1,000 classes) - Used ReLU, dropout, and GPU training.</p> <p>Why deep learning emerged: 1. Data: Huge labeled datasets (images, text, speech) 2. Compute: GPUs = fast parallel matrix ops 3. Algorithms: Batch norm, better backprop, new architectures</p> <p>Transformative apps: - Computer vision: self\u2011driving cars, medical imaging - NLP: translation, GPT\u2011style generation - Speech: voice assistants, real\u2011time translation</p> <p>Why this matters for you</p> <ul> <li>Modern frameworks (TensorFlow, PyTorch) are built around neural nets.  </li> <li>Explains why later terms focus on coding deep models &amp; leveraging pretrained architectures quickly.</li> </ul>"},{"location":"week-1/#c-probability-basics","title":"C. Probability Basics","text":"<p>Definition</p> <p>Probability Theory \u2013 \u201cThe mathematical framework for quantifying uncertainty and modeling random phenomena.\u201d</p>"},{"location":"week-1/#c1-gentle-introduction","title":"C1. Gentle Introduction","text":"1. What is Chance? <p>Analogy: Flipping a coin\u2014two outcomes, but you can\u2019t predict which. Key idea: Probability measures how likely something is (0 = impossible, 1 = certain). Example: A fair coin \u2192 P(heads) = 0.5.</p> 2. Simple Data &amp; Averages <p>Real example: Test scores: 80, 90, 70, 100, 60. Mean (average): <pre><code>(80 + 90 + 70 + 100 + 60) / 5 = 80\n</code></pre> Why it matters: The mean tells you what\u2019s \u201ctypical.\u201d</p> 3. Measuring Spread (Variance) <p>Analogy: Scores all near 80% \u2192 small spread; scores all over the place \u2192 big spread. Steps (using the score list above): 1. Subtract the mean (80): e.g. 60 \u2212 80 = \u221220 2. Square them: (\u221220)\u00b2 = 400 3. Average the squares \u2192 variance \u2248 280 4. Square root of variance \u2192 standard deviation \u2248 16.7 Why we care: Spread tells you how consistent or noisy data is\u2014critical for risk or quality control.</p>"},{"location":"week-1/#c2-formal-definitions-deep-dive","title":"C2. Formal Definitions &amp; Deep Dive","text":"1. Random Variables <p>A random variable (RV) assigns numbers to random outcomes.</p> <ul> <li>Discrete RV: countable values (die roll, number of returns)   Example: Fair die \u2192 <pre><code>X \u2208 {1,2,3,4,5,6},   P(X = k) = 1/6\n</code></pre></li> <li>Continuous RV: any value in a range (time between failures)   Example: Exponential distribution for time ( t \u2265 0 ): <pre><code>f(t) = \u03bb e^{\u2212\u03bb t}\n</code></pre></li> </ul> 2. Expectation (Mean) <p>Long\u2011run average outcome if you repeat forever.</p> <ul> <li>Discrete: <pre><code>E[X] = \u03a3 x_i \u00b7 P(X = x_i)\n</code></pre></li> <li>Continuous: <pre><code>E[X] = \u222b x f(x) dx\n</code></pre> Worked example (die): <pre><code>E[X] = (1+2+3+4+5+6) / 6 = 3.5\n</code></pre> Relevance: Loss functions (e.g., MSE) minimize expected error \u2192 expectation is baked into training.</li> </ul> 3. Variance &amp; Standard Deviation <p>Variance: average squared distance from the mean. <pre><code>Var(X) = E[(X \u2212 E[X])^2]\n</code></pre> Std. dev.: <pre><code>\u03c3 = \u221aVar(X)\n</code></pre> Die example: <pre><code>Var \u2248 2.92,  \u03c3 \u2248 1.71\n</code></pre> Why it matters: Tells you how uncertain predictions are, helps build confidence intervals, drives anomaly detection.</p> <p>Why Probability Matters in AI</p> <ul> <li>Model Training: Errors are expectations (means) over data.  </li> <li>Uncertainty: Variance underpins confidence, risk, anomaly flags.  </li> <li>Feature Engineering: Understanding distributions guides transformations (e.g., log scales for skewed data).</li> </ul>"},{"location":"week-1/#d-linear-algebra-basics","title":"D. Linear Algebra Basics","text":"<p>Definition</p> <p>Linear Algebra \u2013 \u201cThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.\u201d</p>"},{"location":"week-1/#d1-gentle-introduction","title":"D1. Gentle Introduction","text":"1. Vectors as Lists <p>Analogy: A grocery list: <code>[2 bananas, 1 loaf bread, 500 g cheese]</code> Key idea: A vector is just a list of numbers representing features.</p> 2. Matrices as Tables <p>Analogy: A seating chart (rows = tables, columns = seats): <pre><code>       S1  S2  S3\n    T1  A   B   C\n    T2  D   E   F\n</code></pre> Key idea: A matrix stacks many vectors into rows or columns.</p> 3. Dot Product Intuition <p>Example (bill splitting): - You &amp; a friend order appetizers <code>[3, 2]</code> and drinks <code>[1, 2]</code>. - Prices: appetizers = \\$5, drinks = \\$2 \u2192 <pre><code>[3, 2] \u00b7 [5, 2] = 3\u00d75 + 2\u00d72 = 19\n</code></pre> Why it matters: Same math as a simple regression prediction (weights \u00d7 features).</p> 4. Real\u2011World Matrix Uses <ul> <li>Recipe scaling: Multiply ingredient matrix by 1.5 to go from 4 to 6 servings.  </li> <li>School timetable: Days \u00d7 hours grid to schedule classes.</li> </ul>"},{"location":"week-1/#d2-formal-definitions-deep-dive","title":"D2. Formal Definitions &amp; Deep Dive","text":"1. Vectors &amp; Their Interpretation <p>A vector x \u2208 \u211d\u207f is an ordered list of n numbers (features). Example: <pre><code>x = [age, monthly_spend, num_orders] = [45, 320.5, 12]\n</code></pre></p> 2. Matrices &amp; Batch Operations <p>A matrix X \u2208 \u211d^{m\u00d7n} stacks m row\u2011vectors of length n. Example (customer table): <pre><code>X = [\n  [45, 320.5, 12],\n  [23, 150.0,  5],\n  ...\n]\n</code></pre></p> 3. Dot Product &amp; Linear Transformations <p>Dot product: <pre><code>a \u00b7 b = \u03a3 (a_i * b_i)\n</code></pre> Use in AI: - Regression:  \u0177 = w \u00b7 x + b - Neural nets:  z = w \u00b7 x + b, then apply activation (e.g., ReLU)</p> 4. Matrix Multiplication <p><pre><code>C = A \u00d7 B,   C_{ij} = \u03a3_k A_{ik} B_{kj}\n</code></pre> Example: Combine/transform features or chain neural network layers.</p> Why Linear Algebra Matters in AI <ul> <li>Speed: GPUs/NumPy rely on vectorized (matrix) ops for efficiency.  </li> <li>Model Insight: Weights, activations, attention maps are matrices/vectors.  </li> <li>Dimensionality Reduction: PCA, SVD use eigenvectors/values to compress data.</li> </ul>"},{"location":"week-1/#4-tools-installation-setup","title":"4. Tools Installation &amp; Setup","text":"<p>You\u2019ll do this once, then reuse the environment all term.</p>"},{"location":"week-1/#a-install-python-anaconda-windows-mac","title":"A. Install Python &amp; Anaconda (Windows &amp; Mac)","text":"<p><pre><code># Visit this in your browser:\nhttps://www.anaconda.com/products/distribution\n</code></pre> 1. Download the Python 3.x installer for your OS. 2. Run the installer, accept defaults. 3. Open Anaconda Navigator (Start Menu on Windows / Applications on Mac).</p>"},{"location":"week-1/#b-launch-jupyter-notebook","title":"B. Launch Jupyter Notebook","text":"<ol> <li>In Anaconda Navigator, click Launch under Jupyter Notebook.  </li> <li>A browser window opens showing your files.  </li> <li>Click New \u2192 Python 3.  </li> <li>Rename it to Week1_AI_Math.ipynb.</li> </ol>"},{"location":"week-1/#c-install-import-numpy-pandas","title":"C. Install &amp; Import NumPy &amp; pandas","text":"<p>In a notebook cell, run: <pre><code>!conda install numpy pandas -y\n</code></pre> Then import: <pre><code>import numpy as np\nimport pandas as pd\n</code></pre></p> <p>Why these tools?</p> <ul> <li>Python/Jupyter: interactive coding &amp; math demos  </li> <li>NumPy: fast vectors/matrices (used everywhere in ML)  </li> <li>pandas: quick data tables, cleaning, summaries</li> </ul>"},{"location":"week-1/#5-step-by-step-exercises","title":"5. Step by Step Exercises","text":"Exercise 1: Die Roll Simulation &amp; Statistics <p>1. Overview &amp; Purpose Simulate 1,000 rolls of a fair six\u2011sided die in Python and compute the empirical mean and variance. Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p> <p>2. Concept Reinforcement - Probability &amp; random variables - Expectation (mean) &amp; variance - Sampling variability / Law of Large Numbers</p> <p>3. Real\u2011World Relevance - Quality control: simulate defect rates in a batch - Risk modeling: Monte Carlo estimates of portfolio variance - Game design: balance randomness in mechanics</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\n# Simulate 1,000 die rolls\nnp.random.seed(42)           # optional: reproducibility\nrolls = np.random.randint(1, 7, size=1000)\n\n# Compute statistics\nmean_rolls = rolls.mean()\nvar_rolls  = rolls.var()\n\nprint(\"Simulated Mean:    \", mean_rolls)\nprint(\"Simulated Variance:\", var_rolls)\n</code></pre></p> <p>Notes: - <code>np.random.randint(1, 7, size=1000)</code> \u2192 integers 1\u20136 - <code>.mean()</code>, <code>.var()</code> \u2192 empirical mean &amp; variance (population variance by default)</p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 3.5, Variance \u2248 2.92 (\u00b1 sampling noise) - Larger sample sizes converge closer to the theoretical values</p> <p>6. Extensions &amp; Variations - Try sample sizes 100, 10,000 and compare stats - Simulate a weighted/unfair die - Plot histogram with <code>matplotlib</code></p> <p>7. Additional Notes &amp; Tips - Use <code>np.random.seed(...)</code> if you want the same results every run - Avoid Python loops when possible\u2014NumPy vectorization is faster</p> Exercise 2: Coin Flip Probability Estimation <p>1. Overview &amp; Purpose Simulate 10,000 coin flips to estimate the probability of heads and tails.</p> <p>2. Concept Reinforcement - Discrete random variables - Empirical vs. theoretical probability</p> <p>3. Real\u2011World Relevance - A/B testing: success/failure rates - Clinical trials: treatment vs. control outcomes</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                      # optional: reproducibility\nflips = np.random.choice(['H', 'T'], size=10000)\n\np_heads = np.mean(flips == 'H')\np_tails = np.mean(flips == 'T')\n\nprint(f\"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\")\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Both \u2248 0.5, with random fluctuation ~\u00b10.01 - Larger samples narrow the gap to 0.5</p> <p>6. Extensions &amp; Variations - Weighted coin: <code>p=['H':0.3, 'T':0.7]</code> - Plot counts with a bar chart</p> <p>7. Additional Notes &amp; Tips - Set <code>np.random.seed(...)</code> when you want reproducible runs</p> Exercise 3: Histogram of Die Rolls <p>1. Overview &amp; Purpose Visualize the distribution of the 1,000 die rolls from Exercise\u202f1.</p> <p>2. Concept Reinforcement - Frequency vs. probability - Basic data visualization</p> <p>3. Real\u2011World Relevance - Sales distribution by category - Error counts per batch in manufacturing</p> <p>4. Step-by-Step Instructions <pre><code>import matplotlib.pyplot as plt\n\nplt.hist(rolls, bins=range(1, 8), align='left', rwidth=0.8)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title('Histogram of 1,000 Die Rolls')\nplt.show()\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Bars roughly equal for faces 1\u20136 (random noise is okay)</p> <p>6. Extensions &amp; Variations - Normalized histogram: <code>plt.hist(..., density=True)</code> - Overlay the theoretical PMF as a line/bar plot</p> <p>7. Additional Notes &amp; Tips - <code>bins=range(1,8)</code> centers bars on integer faces - If you reused <code>rolls</code> from Exercise\u202f1, you don\u2019t need to re\u2011simulate</p> Exercise 4: Exponential Distribution Simulation <p>1. Overview &amp; Purpose Simulate 5,000 samples from an exponential distribution (mean\u202f=\u202f2) and compute mean/variance.</p> <p>2. Concept Reinforcement - Continuous random variables - Relationship between distribution parameters and statistics</p> <p>3. Real\u2011World Relevance - Time between machine failures - Call\u2011center interarrival times</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                         # optional\nsamples = np.random.exponential(scale=2, size=5000)\n\nprint(\"Empirical Mean:   \", samples.mean())\nprint(\"Empirical Variance:\", samples.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 2 - Variance \u2248 4 Small deviations are normal due to randomness.</p> <p>6. Extensions &amp; Variations - Change <code>scale</code> (mean) parameter - Plot histogram and overlay the theoretical PDF</p> <p>7. Additional Notes &amp; Tips - <code>scale</code> in NumPy\u2019s exponential is ( 1/\u03bb ) (i.e., the mean) - Use <code>matplotlib</code> or <code>seaborn</code> for quick visual checks</p> Exercise 5: Normal Distribution Sampling <p>1. Overview &amp; Purpose Draw 10,000 samples from a standard normal distribution (mean\u202f0, \u03c3\u202f=\u202f1) and verify statistics.</p> <p>2. Concept Reinforcement - Properties of the Gaussian distribution - Central Limit Theorem (preview)</p> <p>3. Real\u2011World Relevance - Measurement error modeling - Standardized test scores / z\u2011scores</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                    # optional\nnormals = np.random.randn(10000)     # mean=0, std=1\n\nprint(\"Mean:\", normals.mean())\nprint(\"Variance:\", normals.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 0, Variance \u2248 1 (allow small deviation)</p> <p>6. Extensions &amp; Variations - Use <code>np.random.normal(loc, scale, size)</code> for non\u2011standard normals - Make a QQ plot vs. theoretical normal to check normality</p> <p>7. Additional Notes &amp; Tips - <code>plt.hist(normals, density=True)</code> to visualize the bell curve</p> Exercise 6: Sampling Distribution of the Mean <p>1. Overview &amp; Purpose Run 1,000 \u201cmini\u2011experiments.\u201d Each experiment rolls a die 100 times, records the mean, and we plot the distribution of those means.</p> <p>2. Concept Reinforcement - Law of Large Numbers - Sampling variability decreases as sample size increases - Sampling distribution &amp; standard error</p> <p>3. Real\u2011World Relevance - Polling averages: many small samples \u2192 distribution of means - Quality control: batch averages instead of single measurements</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)  # optional\nmeans = [np.random.randint(1, 7, 100).mean() for _ in range(1000)]\n\nplt.hist(means, bins=20)\nplt.title('Sampling Distribution of Die Roll Means')\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.show()\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Histogram looks roughly normal, centered near 3.5 - Spread is much narrower than individual die outcomes</p> <p>6. Extensions &amp; Variations - Change experiment size: n=10 vs. n=1000 \u2192 compare spreads - Compute standard error: \u03c3 / \u221an (use \u03c3 \u2248 1.71 for a die)</p> <p>7. Additional Notes &amp; Tips - List comprehensions are fine here; for speed, you can vectorize with NumPy</p> Exercise 7: Weighted Dice Simulation <p>1. Overview &amp; Purpose Simulate 1,000 rolls of a biased die where P(6) = 0.5 and the other faces share the remaining probability.</p> <p>2. Concept Reinforcement - Custom discrete distributions - How bias shifts mean and variance</p> <p>3. Real\u2011World Relevance - Biased processes in manufacturing (defect more likely on one line) - Skewed customer behavior (one product far more popular)</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)  # optional\nfaces = [1, 2, 3, 4, 5, 6]\nprobs = [0.1]*5 + [0.5]     # 0.1 each for 1\u20135, 0.5 for 6\nrolls_biased = np.random.choice(faces, size=1000, p=probs)\n\nprint(\"Mean:\", rolls_biased.mean(), \"Variance:\", rolls_biased.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean &gt; 3.5 due to heavy weight on 6 - Variance will differ from the fair\u2011die case</p> <p>6. Extensions &amp; Variations - Tweak <code>probs</code> for different biases - Plot histograms for fair vs. biased dice side\u2011by\u2011side</p> <p>7. Additional Notes &amp; Tips - Ensure <code>sum(probs) == 1</code> or NumPy will error - You can simulate many biased scenarios to stress\u2011test models</p> Exercise 8: Vector Addition &amp; Scaling <p>1. Overview &amp; Purpose Demonstrate vector addition and scalar multiplication using simple feature vectors.</p> <p>2. Concept Reinforcement - Vector space operations (add, scale) - Geometric interpretation (direction &amp; length)</p> <p>3. Real\u2011World Relevance - Combine feature effects (e.g., marketing channels) - Scale normalized data or weights</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nv1 = np.array([2, 4, 6])\nv2 = np.array([1, 3, 5])\n\nsum_v   = v1 + v2        # vector addition\nscaled_v = 0.5 * v1      # scalar multiplication\n\nprint(\"Sum:   \", sum_v)\nprint(\"Scaled:\", scaled_v)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - <code>Sum</code> = <code>[3, 7, 11]</code> - <code>Scaled</code> = <code>[1, 2, 3]</code></p> <p>6. Extensions &amp; Variations - Compute the dot product: <code>v1.dot(v2)</code> - Visualize 2D/3D vectors (e.g., with matplotlib quiver)</p> <p>7. Additional Notes &amp; Tips - Ensure vectors have the same length for element\u2011wise ops - Scalar multiplication stretches/shrinks the vector length</p> Exercise 9: Matrix Multiplication Demonstration <p>1. Overview &amp; Purpose Multiply a 2\u00d73 matrix by a 3\u00d72 matrix to reinforce matrix\u2011multiplication rules and shape compatibility.</p> <p>2. Concept Reinforcement - Shape rules (inner dimensions must match) - Summation over the inner index (k)</p> <p>3. Real\u2011World Relevance - Transforming feature spaces - Chaining layers in neural networks (each layer = a matrix multiply)</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])          # shape (2, 3)\n\nB = np.array([[ 7,  8],\n              [ 9, 10],\n              [11, 12]])          # shape (3, 2)\n\nC = A.dot(B)            # or: C = A @ B in Python 3.5+\nprint(\"Result:\\n\", C)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation <pre><code>[[ 58  64]\n [139 154]]\n</code></pre> - You can verify: first row \u00d7 first column \u2192 17 + 29 + 3*11 = 58</p> <p>6. Extensions &amp; Variations - Try <code>B @ A</code> to see the shape error - Use larger random matrices to test performance</p> <p>7. Additional Notes &amp; Tips - Check shapes with <code>A.shape</code>, <code>B.shape</code> - <code>@</code> is shorthand for matrix multiply (<code>dot</code>) in NumPy/Python 3.5+</p> Exercise 10: PCA on Toy Dataset <p>1. Overview &amp; Purpose Perform PCA on a tiny 2\u2011D dataset, reduce it to 1\u2011D, and see how much variance is captured.</p> <p>2. Concept Reinforcement - Eigenvectors / eigenvalues - Dimensionality reduction &amp; variance explained</p> <p>3. Real\u2011World Relevance - Compressing image or sensor data - Feature extraction before clustering or modeling</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\n\nX = np.array([\n    [2.5, 2.4],\n    [0.5, 0.7],\n    [2.2, 2.9],\n    [1.9, 2.2],\n    [3.1, 3.0]\n])\n\npca = PCA(n_components=1)\nX_pca = pca.fit_transform(X)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Projected data:\\n\", X_pca)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - First component should capture ~98% of variance for this toy set - Projected 1\u2011D data preserves most \u201cinformation\u201d (spread)</p> <p>6. Extensions &amp; Variations - Plot original 2\u2011D vs. projected 1\u2011D points - Try <code>n_components=2</code> (no reduction) and inspect components</p> <p>7. Additional Notes &amp; Tips - Requires <code>scikit-learn</code> (<code>pip install scikit-learn</code> if missing) - PCA assumes linear structure; nonlinear data may need t\u2011SNE/UMAP</p>"},{"location":"week-1/#6-week-1-summary-what-you-can-now-do","title":"6. Week\u202f1 Summary &amp; What You Can Now Do","text":"<p>You can now\u2026</p> <ul> <li>Explain key AI milestones: Dartmouth (1956), Expert Systems (1970s\u201380s), Deep Learning boom (2010s\u2013present) and why each wave mattered.  </li> <li>Use probability concepts (random variables, mean, variance) and verify them empirically in Python.  </li> <li>Work with basic linear algebra objects (vectors, matrices, dot products, matrix multiplication) and see how they power ML models.  </li> <li>Install and run core tools (Anaconda, Jupyter, NumPy, pandas) to explore data and math interactively.</li> </ul>"},{"location":"week-1/#a-ai-history-in-one-breath","title":"A. AI History in One Breath","text":"<ul> <li>Dartmouth Workshop (1956): coined \u201cAI\u201d; optimism about simulating intelligence. Lesson: Ambition vs. realism\u2014avoid hype traps.  </li> <li>Expert Systems (\u201970s\u2013\u201980s): rule-based IF\u2013THEN logic (e.g., MYCIN). Lesson: Transparency is great, but brittle without probabilities.  </li> <li>Deep Learning (2010s\u2013): data + GPUs + better algorithms (AlexNet etc.) \u2192 breakthroughs. Lesson: Modern toolkits (TensorFlow/PyTorch) center on neural nets.</li> </ul>"},{"location":"week-1/#b-probability-from-intuition-to-math","title":"B. Probability: From Intuition to Math","text":"<ul> <li>Random Variables: discrete (die, coin), continuous (time-to-failure).  </li> <li>Expectation &amp; Variance: long-run average and spread\u2014computed by hand and via NumPy.  </li> <li>Why it matters: Loss functions, risk estimation, feature engineering all use these ideas.  </li> <li>Real world ties: A/B tests, forecasting demand spikes, Monte Carlo risk simulations.</li> </ul> <p>Anchor formulas</p> <ul> <li>Discrete mean: \\(E[X] = \\sum x_i P(X=x_i)\\)  </li> <li>Variance: \\(\\mathrm{Var}(X) = E[(X - E[X])^2]\\)</li> </ul>"},{"location":"week-1/#c-linear-algebra-the-language-of-ml","title":"C. Linear Algebra: The Language of ML","text":"<ul> <li>Vectors: feature lists (e.g., <code>[age, spend, orders]</code>).  </li> <li>Matrices: batches of vectors; all your data at once.  </li> <li>Dot Product: core of regression and neuron activations.  </li> <li>Matrix Multiplication: chaining transformations (layers) in neural nets.  </li> <li>Why it matters: Speed (GPU vectorization), interpretability (weights are matrices), compression (PCA).</li> </ul>"},{"location":"week-1/#d-tools-workflow-locked-in","title":"D. Tools &amp; Workflow Locked In","text":"<ul> <li>Anaconda &amp; Jupyter: you spun up a notebook and ran code.  </li> <li>NumPy/pandas: you handled arrays, stats, and basic data manipulation.  </li> <li>Workflow habits: preview locally (<code>mkdocs serve</code>), commit/push, deploy (<code>gh-deploy</code>).</li> </ul>"},{"location":"week-1/#e-exercises-recap-what-each-taught-you","title":"E. Exercises Recap (What each taught you)","text":"Exercise Core Idea Concept Reinforced Real\u2011World Parallel 1. Die roll stats Empirical vs. theoretical Mean, variance, sampling noise QC sampling, Monte Carlo 2. Coin flips Bernoulli trials Discrete RVs, proportions A/B tests, pass/fail outcomes 3. Histogram Visual distributions Frequency vs. prob., plotting Category sales distributions 4. Exponential sim Time-to-event model Continuous RVs, \u03bb &amp; scale Failure rates, wait times 5. Normal samples Gaussian basics CLT preview, z-scores Measurement error, scores 6. Sampling means Distribution of means Law of Large Numbers Polling averages, batch metrics 7. Weighted die Biased distributions Shifted mean/var, custom PMFs Skewed demand, unfair odds 8. Vector ops Add/scale vectors Vector spaces Feature scaling, weight tuning 9. Matrix multiply Shapes &amp; transforms Linear maps, dot sums NN layers, feature transforms 10. PCA toy data Dimensionality reduction Eigenvectors/variance explained Data compression, preprocessing"},{"location":"week-1/#7-additional-resources","title":"7. Additional Resources","text":"<p>Probability &amp; Statistics</p> <ul> <li>Khan Academy \u2013 Introduction to Probability   Beginner\u2011friendly videos and practice problems. https://www.khanacademy.org/math/statistics-probability/probability-library</li> <li>Seeing Theory (Brown University)   Interactive visual explanations of probability concepts. https://seeing-theory.brown.edu/</li> </ul> <p>Linear Algebra</p> <ul> <li>3Blue1Brown \u2013 Essence of Linear Algebra (YouTube series)   Beautiful visuals for vectors, matrices, dot products, eigenvectors. https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr</li> <li>Khan Academy \u2013 Linear Algebra   Step\u2011by\u2011step lessons with exercises. https://www.khanacademy.org/math/linear-algebra</li> </ul> <p>Python, NumPy &amp; pandas</p> <ul> <li>Official Python Docs \u2013 Syntax, stdlib, tutorials. https://docs.python.org/3/</li> <li>NumPy User Guide \u2013 Arrays, broadcasting, linear algebra. https://numpy.org/doc/stable/user/</li> <li>pandas Getting Started \u2013 DataFrames, cleaning, transforms. https://pandas.pydata.org/docs/getting_started/index.html</li> </ul> <p>AI History &amp; Overviews</p> <ul> <li>\u201cCompeting in the Age of AI\u201d (Iansiti &amp; Lakhani) \u2013 Ch.\u202f1\u20133 (already on your list) </li> <li>\u201cDeep Learning\u201d (Goodfellow, Bengio, Courville) \u2013 Intro &amp; Ch.\u202f6\u20137 (free online) https://www.deeplearningbook.org/</li> </ul> <p>Visualization &amp; Math Intuition</p> <ul> <li>Matplotlib Gallery \u2013 Quick plot recipes. https://matplotlib.org/stable/gallery/index.html</li> <li>Desmos Graphing Calculator \u2013 Fast function plots and geometry. https://www.desmos.com/calculator</li> </ul> <p>Bonus Cheat Sheets</p> <ul> <li>Markdown Syntax Cheat Sheet (for editing your site) https://www.markdownguide.org/cheat-sheet/</li> <li>NumPy/Pandas one\u2011pagers (various printable PDFs) https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf https://www.dataquest.io/blog/numpy-cheat-sheet/</li> </ul>"},{"location":"week-2/","title":"Week 2","text":"<p>testing the editing of this page 1x </p>"}]}