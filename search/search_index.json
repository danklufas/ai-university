{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI University","text":"<p>Welcome to the AI University Curriculum. Use the left menu to pick a week.</p>"},{"location":"week-1/","title":"Week 1","text":"<ol> <li>Lesson Overview Learning Objectives: By the end of this lesson, you will be able to: \u2022   Describe three pivotal AI milestones and their lasting impact. \u2022   Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples. \u2022   Understand probability fundamentals\u2014random variables, expectation, variance\u2014and compute them by hand and in Python. \u2022   Grasp key linear algebra concepts\u2014vectors, matrices, dot products, matrix multiplication\u2014and see how they underpin AI algorithms. \u2022   Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</li> </ol> <ol> <li>Core Definitions Term    Definition &amp; Example Artificial Intelligence (AI)    \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956 Example: A chatbot that interprets questions and crafts human like responses.  Machine Learning (ML)   Algorithms that improve performance on tasks by learning from data rather than explicit programming. Example: A regression model that learns to predict steel prices from historical sales. Data Science    Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data. Example: Cleaning and visualizing e commerce logs to uncover purchasing trends. </li> </ol> <ol> <li>Concept Sections A. AI Milestones Excerpt (Definition Box): Artificial Intelligence (AI) \u2013 \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956</li> <li>The Dartmouth Workshop (1956) In the summer of 1956, John\u202fMcCarthy, Marvin\u202fMinsky, Nathaniel\u202fRochester, and Claude\u202fShannon convened at Dartmouth College to explore a bold question: \u201cCan machines be made to simulate human intelligence?\u201d They coined the term \u201cArtificial Intelligence\u201d and launched a two month study to investigate how machines might \u201clearn from experience,\u201d \u201cmake abstractions,\u201d and \u201cuse language.\u201d o   Context &amp; Significance: Before Dartmouth, computers were viewed largely as number crunchers. This workshop reframed them as potential thinking machines, seeding optimism that a small team could tackle \u201cevery aspect of learning or any other feature of intelligence.\u201d o   First Programs: \uf0a7   Logic Theorist (1955): Developed by Newell\u202f&amp;\u202fSimon, it proved theorems in symbolic logic\u2014demonstrating that \u201cthinking\u201d tasks could be mechanized. \uf0a7   General Problem Solver (1957): An early attempt at a universal reasoning engine. o   Why It Matters Today: \uf0a7   Cycle of Hype &amp; AI Winters: The booms and busts following Dartmouth teach us to balance ambition with realism when evaluating modern AI breakthroughs. \uf0a7   Legacy in Modern Research: Symbolic reasoning and search algorithms from this era underpin today\u2019s knowledge graphs and constraint solving systems.</li> <li>Expert Systems Era (1970s\u20131980s) As symbolic AI matured, expert systems emerged\u2014rule based programs encoding human expertise as \u201cif\u2013then\u201d statements. o   Core Idea: Encode domain knowledge in production rules: text CopyEdit IF symptom = fever AND symptom = rash THEN suggest = measles o   Notable Example \u2013 MYCIN (1972\u20131980): \uf0a7   Built at Stanford, MYCIN contained ~600 rules for diagnosing bacterial infections and recommending antibiotics. \uf0a7   It queried patient data (age, symptoms), applied its rule base, and in blind tests matched or outperformed human experts. o   Strengths &amp; Limitations: \uf0a7   Strength: Transparent logic\u2014every recommendation traces back to specific rules. \uf0a7   Limitation: Required hand crafting thousands of rules and handled uncertainty poorly (no probabilistic reasoning). o   Modern Relevance: \uf0a7   Rule based approaches inform decision support in finance and healthcare. \uf0a7   Today\u2019s hybrid systems combine rules with statistical ML (e.g., regulatory checks plus model based scoring).</li> <li>Deep Learning Boom (2010s\u2013Present) The field\u2019s third surge harnessed large datasets and GPU acceleration to train deep neural networks\u2014models with many layers that learn hierarchical features automatically. o   Key Breakthrough \u2013 AlexNet (2012): \uf0a7   An eight layer convolutional neural network (CNN) that halved error rates on the ImageNet challenge (1.2\u202fmillion labeled images, 1,000 categories). \uf0a7   Employed ReLU activations, dropout regularization, and GPU based training. o   Why Deep Learning Emerged:</li> <li>Data: Massive labeled datasets (images, text, speech).</li> <li>Compute: GPUs excel at parallel matrix operations critical for neural nets.</li> <li>Algorithms: Innovations like batch normalization, architectural search, and optimized backpropagation. o   Transformative Applications: \uf0a7   Computer Vision: Object detection (self driving cars), medical imaging (tumor detection). \uf0a7   Natural Language Processing: Language translation, text generation (GPT style models). \uf0a7   Speech &amp; Audio: Voice assistants, real time translation. o   Why It Matters for You: \uf0a7   Modern AI frameworks (TensorFlow, PyTorch) are built around neural networks. \uf0a7   This era explains why subsequent terms focus on coding deep models and leveraging pretrained architectures for rapid deployment.</li> </ol> <p>C. Probability Basics Excerpt (Definition Box): Probability Theory \u2013 \u201cThe mathematical framework for quantifying uncertainty and modeling random phenomena.\u201d C1. Introduction 1.  What Is Chance? o   Everyday Analogy: Flipping a coin. You know there are two sides\u2014heads or tails\u2014but you can\u2019t predict which will land face up. o   Key Idea: Probability measures how likely something is to happen, on a scale from 0 (impossible) to 1 (certain). \uf0a7   Example: A fair coin has probability 0.5 of landing heads. 2.  Simple Data &amp; Averages o   Real World Example: Your test scores this week: 80%, 90%, 70%, 100%, 60%. o   Mean (Average): Add them up and divide by the number of tests: 80+90+70+100+605=80%. \\frac{80 + 90 + 70 + 100 + 60}{5} = 80\\%.580+90+70+100+60=80%.  o   Why It Matters: The mean gives a sense of \u201ctypical\u201d performance. 3.  Measuring Spread (Variance) o   Analogy: If all scores are close to 80% (say 75%,\u202f80%,\u202f85%), that\u2019s low spread; if they vary widely (60%,\u202f100%,\u202f70%), that\u2019s high spread. o   Step by Step (Scores Example): 1.  Compute each score\u2019s difference from the mean (80): e.g., 60\u201380 = \u201320. 2.  Square these differences to make them positive: (\u201320)\u00b2 = 400. 3.  Average the squared differences: if squares are [400,100,100,400,400], mean = 280. 4.  That average (280) is the variance; its square root (\u224816.7) is the standard deviation. o   Real World Uses: \uf0a7   Weather Forecasts: \u201cThere\u2019s a 30% chance of rain\u201d guides umbrella choices. \uf0a7   Quality Control: A factory measures weight of cereal boxes; variance tells if the filling machine is consistent. C2. Formal Definitions &amp; Deep Dive 1.  Understanding Random Variables A random variable XXX formalizes outcomes of random processes by assigning numeric values. o   Discrete RV: Takes countable values (e.g., die rolls, number of returned orders). \uf0a7   Example: Rolling a six sided die \u2192 X\u2208{1,2,3,4,5,6}X \\in {1,2,3,4,5,6}X\u2208{1,2,3,4,5,6} with P(X=k)=16P(X=k)=\\tfrac{1}{6}P(X=k)=61. o   Continuous RV: Takes any value in a continuum (e.g., time between machine failures). \uf0a7   Example: Time (in minutes) between software crashes might follow an exponential distribution: f(t)=\u03bbe\u2212\u03bbt,t\u22650. f(t)=\\lambda e^{-\\lambda t},\\quad t\\ge0.f(t)=\u03bbe\u2212\u03bbt,t\u22650.  2.  Expectation (Mean) The expectation E[X]E[X]E[X] is the long run average if the experiment repeats infinitely. o   Formula (Discrete): E[X]=\u2211ixi\u2009P(X=xi). E[X] = \\sum_{i} x_i\\,P(X=x_i).E[X]=i\u2211xiP(X=xi).  o   Formula (Continuous): E[X]=\u222b\u2212\u221e\u221ex\u2009f(x)\u2009dx. E[X] = \\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx.E[X]=\u222b\u2212\u221e\u221exf(x)dx.  o   Worked Example (Die): E[X]=1+2+3+4+5+66=3.5. E[X] = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5.E[X]=61+2+3+4+5+6=3.5.  o   Relevance: Loss functions like mean squared error minimize expected error; understanding expectation clarifies why we average squared deviations. 3.  Variance &amp; Standard Deviation o   Variance: Var(X)=E[(X\u2212E[X])2]. \\mathrm{Var}(X) = E\\bigl[(X - E[X])^2\\bigr].Var(X)=E[(X\u2212E[X])2].  o   Standard Deviation: \u03c3=Var(X). \\sigma = \\sqrt{\\mathrm{Var}(X)}.\u03c3=Var(X).  o   Worked Example (Die): Var(X)=(1\u22123.5)2+\u22ef+(6\u22123.5)26=17.56\u22482.92,\u03c3\u22481.71. \\mathrm{Var}(X) = \\frac{(1-3.5)^2 + \\dots + (6-3.5)^2}{6} = \\frac{17.5}{6} \\approx 2.92,\\quad \\sigma \\approx 1.71.Var(X)=6(1\u22123.5)2+\u22ef+(6\u22123.5)2=617.5\u22482.92,\u03c3\u22481.71.  o   Relevance: Guides feature scaling, sets confidence intervals, and underpins uncertainty quantification in finance or anomaly detection. 4.  Why These Concepts Matter in AI o   Model Training: Loss functions (e.g., MSE) rely on expectation of squared errors. o   Uncertainty Quantification: Variance informs risk metrics (VaR, confidence intervals). o   Feature Engineering: Distribution shapes dictate transformations (e.g., log scaling skewed data).</p> <p>D. Linear Algebra Basics Excerpt (Definition Box): Linear Algebra \u2013 \u201cThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.\u201d D1. Introduction 1.  Vectors as Lists o   Analogy: A grocery list: [2\u202fbananas,\u202f1\u202floaf\u202fbread,\u202f500\u202fg\u202fcheese]. o   Key Idea: A vector is just a list of numbers representing \u201cfeatures.\u201d 2.  Matrices as Tables o   Analogy: A seating chart in class: rows are table numbers, columns are seat positions. text CopyEdit |    | S1 | S2 | S3 | |----|----|----|----| | T1 | A  | B  | C  | | T2 | D  | E  | F  | o   Key Idea: A matrix is multiple vectors \u201cstacked\u201d into rows or columns. 3.  Dot Product Intuition o   Example (Bill Splitting): You and a friend order appetizers ([3,\u202f2] plates) and drinks ([1,\u202f2] each). To compute total cost if plates = $5, drinks = $2: [3,2]\u22c5[5,2]=3\u00d75+2\u00d72=15+4=$19. [3,2]\\cdot[5,2] = 3\\times5 + 2\\times2 = 15 + 4 = \\$19.[3,2]\u22c5[5,2]=3\u00d75+2\u00d72=15+4=$19.  o   Why It Matters: Combines quantities and prices; same math as a regression prediction. 4.  Real World Matrix Use o   Recipe scaling: A 4 serving recipe\u2019s ingredients in a matrix, multiply by 1.5 to get 6 servings. o   School timetable: Days\u202f\u00d7\u202fhours grid for scheduling classes. D2. Formal Definitions &amp; Deep Dive 1.  Vectors &amp; Their Interpretation A vector x\u2208Rn\\mathbf{x}\\in\\mathbb{R}^nx\u2208Rn is an ordered list of nnn numbers representing features or data points. o   Example: x=[age,\u2009monthly_spend,\u2009num_orders]=[45,\u2009320.5,\u200912]. \\mathbf{x} = [\\text{age},\\,\\text{monthly_spend},\\,\\text{num_orders}] = [45,\\,320.5,\\,12].x=[age,monthly_spend,num_orders]=[45,320.5,12].  2.  Matrices &amp; Batch Operations A matrix X\u2208Rm\u00d7n\\mathbf{X}\\in\\mathbb{R}^{m\\times n}X\u2208Rm\u00d7n stacks mmm row vectors of dimension nnn. o   Example: X=[45320.51223150.05\u22ee\u22ee\u22ee]. \\mathbf{X} = \\begin{bmatrix} 45 &amp; 320.5 &amp; 12 \\ 23 &amp; 150.0 &amp; 5 \\ \\vdots &amp; \\vdots &amp; \\vdots \\end{bmatrix}.X=4523\u22ee320.5150.0\u22ee125\u22ee.  3.  Dot Product &amp; Linear Transformations o   Dot Product: a\u22c5b=\u2211i=1nai\u2009bi. \\mathbf{a}\\cdot\\mathbf{b} = \\sum_{i=1}^n a_i\\,b_i.a\u22c5b=i=1\u2211naibi.  Example: [1,2,3]\u22c5[4,5,6]=32.[1,2,3]\\cdot[4,5,6] = 32.[1,2,3]\u22c5[4,5,6]=32. o   Use in AI: \uf0a7   Regression: y^=w\u22c5x+b\\hat y = \\mathbf{w}\\cdot\\mathbf{x} + by^=w\u22c5x+b. \uf0a7   Neural Nets: Each neuron computes z=w\u22c5x+bz=\\mathbf{w}\\cdot\\mathbf{x}+bz=w\u22c5x+b, then applies an activation. 4.  Matrix Multiplication C=A\u00d7B,Cij=\u2211k=1nAik\u2009Bkj. \\mathbf{C} = \\mathbf{A}\\times\\mathbf{B},\\quad C_{ij} = \\sum_{k=1}^n A_{ik}\\,B_{kj}.C=A\u00d7B,Cij=k=1\u2211nAikBkj.  Example: Transforming feature spaces or chaining layers in a deep network. 5.  Relevance for AI Practitioners o   Batch Processing: GPUs and NumPy rely on vectorized matrix operations. o   Model Introspection: Weight matrices and activation maps in CNNs are built on these operations. o   Dimensionality Reduction: PCA uses eigenvectors/eigenvalues of covariance matrices to compress data.</p> <ol> <li>Tools Installation &amp; Setup Windows &amp; Mac A. Install Python &amp; Anaconda</li> <li>Navigate to\u202fhttps://www.anaconda.com/products/distribution</li> <li>Download the Python\u202f3.x installer for your OS.</li> <li>Run the installer, accept defaults.</li> <li>Open Anaconda Navigator from your Start menu (Windows) or Applications folder (Mac). B. Launch Jupyter Notebook</li> <li>In Anaconda Navigator, click Launch under Jupyter Notebook.</li> <li>A browser window opens showing your file system.</li> <li>Click New \u2192 Python\u202f3.</li> <li>Rename the notebook to Week1_AI_Math.ipynb. C. Install &amp; Import NumPy &amp; pandas</li> <li>In a notebook cell, run: bash CopyEdit !conda install numpy pandas -y</li> <li>In the next cell, import the libraries: python CopyEdit import numpy as np import pandas as pd</li> </ol> <ol> <li>Step by Step Exercises Exercise 1: Die Roll Simulation &amp; Statistics (Template)</li> <li>Exercise Overview &amp; Purpose \u2022   What we\u2019re doing: Simulate 1,000 rolls of a fair six sided die in Python to compute the empirical mean and variance. \u2022   Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</li> <li>Concept Reinforcement \u2022   Probability &amp; Random Variables \u2022   Expectation &amp; Variance \u2022   Sampling Variability</li> <li>Real World Relevance \u2022   Quality Control (defect rate simulation) \u2022   Risk Modeling (Monte Carlo portfolio variance) \u2022   Randomized Algorithms &amp; Game Balancing</li> <li>Step by Step Instructions python CopyEdit import numpy as np</li> </ol>"},{"location":"week-1/#simulate-1000-die-rolls","title":"Simulate 1,000 die rolls","text":"<p>rolls = np.random.randint(1, 7, size=1000)</p>"},{"location":"week-1/#compute-statistics","title":"Compute statistics","text":"<p>mean_rolls = rolls.mean() var_rolls = rolls.var()</p> <p>print(\"Simulated Mean:    \", mean_rolls) print(\"Simulated Variance:\", var_rolls) \u2022   Notes: o   np.random.randint(1, 7, size=1000): integers 1\u20136 o   .mean(), .var(): compute empirical mean &amp; variance 5. Expected Outcomes &amp; Interpretation \u2022   Mean \u2248\u202f3.5, Variance \u2248\u202f2.92 (\u00b1 sampling noise) \u2022   Larger samples converge closer to theory 6. Extensions &amp; Variations \u2022   Vary sample size (100, 10,000) \u2022   Simulate weighted/unfair die \u2022   Plot histogram with matplotlib 7. Additional Notes &amp; Tips \u2022   Use np.random.seed(42) for reproducibility \u2022   Avoid Python loops; prefer NumPy vectorization Exercise\u202f2: Coin Flip Probability Estimation 1. Overview &amp; Purpose Simulate 10,000 coin flips to estimate the probability of heads and tails. 2. Concept Reinforcement \u2022   Discrete random variables \u2022   Empirical vs. theoretical probability 3. Real World Relevance \u2022   A/B testing conversion rates (success/failure) \u2022   Clinical trial outcomes 4. Step by Step Instructions python CopyEdit import numpy as np</p> <p>np.random.seed(0) flips = np.random.choice(['H','T'], size=10000) p_heads = np.mean(flips == 'H') p_tails = np.mean(flips == 'T') print(f\"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\") 5. Expected Outcomes &amp; Interpretation ~0.5 each, with fluctuations ~\u00b10.01. 6. Extensions &amp; Variations \u2022   Weighted coin (p=['H':0.3,'T':0.7]) \u2022   Plot bar chart of counts 7. Additional Notes &amp; Tips Use np.random.seed(\u2026) for reproducibility.</p> <p>Exercise\u202f3: Histogram of Die Rolls 1. Overview &amp; Purpose Visualize the distribution of the 1,000 die rolls from Exercise\u202f1. 2. Concept Reinforcement \u2022   Frequency vs. probability \u2022   Data visualization basics 3. Real World Relevance \u2022   Sales distribution by category \u2022   Error rates by batch 4. Step by Step Instructions python CopyEdit import matplotlib.pyplot as plt</p> <p>plt.hist(rolls, bins=range(1,8), align='left', rwidth=0.8) plt.xlabel('Die Face') plt.ylabel('Frequency') plt.title('Histogram of 1,000 Die Rolls') plt.show() 5. Expected Outcomes &amp; Interpretation Bars roughly equal height for faces\u202f1\u20136. 6. Extensions &amp; Variations \u2022   Normalized histogram (density=True) \u2022   Overlay theoretical PMF 7. Additional Notes &amp; Tips Ensure bins=range(1,8) to center on integer faces.</p> <p>Exercise\u202f4: Exponential Distribution Simulation 1. Overview &amp; Purpose Simulate 5,000 samples from an exponential distribution (mean\u202f=\u202f2) and compute mean/variance. 2. Concept Reinforcement \u2022   Continuous random variables \u2022   Relationship between distribution parameters and statistics 3. Real World Relevance \u2022   Time between machine failures \u2022   Call-center interarrival times 4. Step by Step Instructions python CopyEdit samples = np.random.exponential(scale=2, size=5000) print(\"Empirical Mean:\", samples.mean()) print(\"Empirical Variance:\", samples.var()) 5. Expected Outcomes &amp; Interpretation Mean \u2248\u202f2; variance \u2248\u202f4. 6. Extensions &amp; Variations \u2022   Change scale (mean) parameter \u2022   Plot histogram + theoretical PDF 7. Additional Notes &amp; Tips Use np.histogram or matplotlib for PDF overlay.</p> <p>Exercise\u202f5: Normal Distribution Sampling 1. Overview &amp; Purpose Draw 10,000 samples from a standard normal (mean\u202f0, \u03c3\u202f=\u202f1) and verify statistics. 2. Concept Reinforcement \u2022   Properties of Gaussian distribution \u2022   Central Limit Theorem preview 3. Real World Relevance \u2022   Measurement errors \u2022   Standardized test score modeling 4. Step by Step Instructions python CopyEdit normals = np.random.randn(10000) print(\"Mean:\", normals.mean()) print(\"Variance:\", normals.var()) 5. Expected Outcomes &amp; Interpretation Mean \u2248\u202f0, variance \u2248\u202f1. 6. Extensions &amp; Variations \u2022   Use np.random.normal(loc, scale, size) \u2022   QQ plot vs. theoretical normal 7. Additional Notes &amp; Tips Matplotlib\u2019s plt.hist(..., density=True) for PDF shape.</p> <p>Exercise\u202f6: Sampling Distribution of the Mean 1. Overview &amp; Purpose Simulate 1,000 experiments, each of 100 die rolls, record sample means, and examine their distribution. 2. Concept Reinforcement \u2022   Law of Large Numbers \u2022   Sampling variability reduction 3. Real World Relevance \u2022   Polling averages \u2022   Quality metrics over batches 4. Step by Step Instructions python CopyEdit means = [np.random.randint(1,7,100).mean() for _ in range(1000)] plt.hist(means, bins=20) plt.title('Sampling Distribution of Die Roll Means') plt.show() 5. Expected Outcomes &amp; Interpretation Histogram approximates normal around 3.5 with narrower spread. 6. Extensions &amp; Variations \u2022   Vary experiment size (n=10, n=1000) \u2022   Compute standard error (\u03c3/\u221an) 7. Additional Notes &amp; Tips List comprehensions vs. loops for clarity.</p> <p>Exercise\u202f7: Weighted Dice Simulation 1. Overview &amp; Purpose Simulate 1,000 rolls of a biased die with P(6)=0.5, others equal. 2. Concept Reinforcement \u2022   Custom discrete distributions \u2022   Impact of bias on mean/variance 3. Real World Relevance \u2022   Biased processes in manufacturing \u2022   Skewed customer behavior models 4. Step by Step Instructions python CopyEdit faces = [1,2,3,4,5,6] probs = [0.1]*5 + [0.5] rolls_biased = np.random.choice(faces, size=1000, p=probs) print(\"Mean:\", rolls_biased.mean(), \"Variance:\", rolls_biased.var()) 5. Expected Outcomes &amp; Interpretation Mean &gt;\u202f3.5, variance different from fair die. 6. Extensions &amp; Variations \u2022   Tune probs for different biases \u2022   Compare histograms side by side 7. Additional Notes &amp; Tips Sum of probs must equal 1.</p> <p>Exercise\u202f8: Vector Addition &amp; Scaling 1. Overview &amp; Purpose Demonstrate vector addition and scalar multiplication with feature vectors. 2. Concept Reinforcement \u2022   Vector space operations \u2022   Geometric interpretation 3. Real World Relevance \u2022   Combining feature influences \u2022   Scaling normalized data 4. Step by Step Instructions python CopyEdit v1 = np.array([2, 4, 6]) v2 = np.array([1, 3, 5]) sum_v = v1 + v2              # vector addition scaled_v = 0.5 * v1          # scalar multiplication print(\"Sum:\", sum_v) print(\"Scaled:\", scaled_v) 5. Expected Outcomes &amp; Interpretation Sum = [3,7,11]; scaled = [1,2,3]. 6. Extensions &amp; Variations \u2022   Compute dot product of sum_v and v2 \u2022   Visualize vectors in 2D/3D 7. Additional Notes &amp; Tips Ensure consistent dimensions.</p> <p>Exercise\u202f9: Matrix Multiplication Demonstration 1. Overview &amp; Purpose Multiply a 2\u00d73 matrix by a 3\u00d72 matrix to reinforce matrix multiplication rules. 2. Concept Reinforcement \u2022   Shape compatibility \u2022   Summation over inner index 3. Real World Relevance \u2022   Transforming feature spaces \u2022   Composition of network layers 4. Step by Step Instructions python CopyEdit A = np.array([[1,2,3],[4,5,6]]) B = np.array([[7,8],[9,10],[11,12]]) C = A.dot(B) print(\"Result:\\n\", C) 5. Expected Outcomes &amp; Interpretation C = [[58,64],[139,154]]. 6. Extensions &amp; Variations \u2022   Reverse multiplication to show error \u2022   Use @ operator in Python 3.5+ 7. Additional Notes &amp; Tips Check shapes via A.shape and B.shape.</p> <p>Exercise\u202f10: PCA on Toy Dataset 1. Overview &amp; Purpose Perform PCA on a small 2 D dataset to reduce to 1 D and visualize variance capture. 2. Concept Reinforcement \u2022   Eigenvectors/eigenvalues \u2022   Dimensionality reduction 3. Real World Relevance \u2022   Compressing image data \u2022   Feature extraction for clustering 4. Step by Step Instructions python CopyEdit from sklearn.decomposition import PCA</p> <p>X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0]]) pca = PCA(n_components=1) X_pca = pca.fit_transform(X) print(\"Explained variance ratio:\", pca.explained_variance_ratio_) print(\"Projected data:\\n\", X_pca) 5. Expected Outcomes &amp; Interpretation Most variance captured in first component (\u224898%). 6. Extensions &amp; Variations \u2022   Plot original vs. projected points \u2022   Try n_components=2 7. Additional Notes &amp; Tips Requires scikit-learn installation.</p> <ol> <li>Summary of Week\u202f1 Throughout this first week, you have: \u2022   Traced AI History: From the Dartmouth Workshop to Expert Systems (MYCIN) and the Deep Learning revolution (AlexNet). \u2022   Built Probability Skills: Learned random variables, expectation, variance\u2014both by hand and in Python (die rolls, coin flips, exponential and normal sampling). \u2022   Visualized Data: Plotted histograms, demonstrated Central Limit Theorem. \u2022   Handled Bias: Modeled a weighted die to see effects on distribution. \u2022   Applied Linear Algebra: Performed vector ops, matrix multiplication, and PCA for dimensionality reduction. These foundational concepts and hands on exercises prepare you for more advanced AI and ML topics.</li> </ol> <ol> <li>Additional Resources Probability Fundamentals: Khan Academy \u201cIntroduction to Probability\u201d \u2022   Linear Algebra Visualizations: 3Blue1Brown \u201cEssence of Linear Algebra\u201d series \u2022   Python Tutorials: Official Python documentation at\u202fpython.org</li> </ol>"},{"location":"week-2/","title":"Week 2","text":"<p>testing the editing of this page 1x </p>"}]}