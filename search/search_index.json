{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI University","text":"<p>Welcome to the AI University Curriculum. Use the left menu to pick a week.</p>"},{"location":"cheat-sheet/","title":"Quick Reminder Cheat Sheets","text":""},{"location":"cheat-sheet/#1-everyday-workflow-edit-preview-commit-deploy","title":"1. Everyday Workflow (Edit \u2192 Preview \u2192 Commit \u2192 Deploy)","text":"<p>Local preview ```bash python -m mkdocs serve</p>"},{"location":"week-1/","title":"Week 1","text":"<p>Week 1: History of AI &amp; Math Foundations 1. Lesson Overview Learning Objectives By the end of this lesson, you will be able to:</p> <p>Describe three pivotal AI milestones and their lasting impact.</p> <p>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</p> <p>Understand probability fundamentals\u2014random variables, expectation, variance\u2014and compute them by hand and in Python.</p> <p>Grasp key linear algebra concepts\u2014vectors, matrices, dot products, matrix multiplication\u2014and see how they underpin AI algorithms.</p> <p>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</p> <ol> <li> <p>Core Definitions Term    Definition &amp; Example Artificial Intelligence (AI)    \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956 Example: A chatbot that interprets questions and crafts human\u2011like responses. Machine Learning (ML)   Algorithms that improve performance on tasks by learning from data rather than explicit programming. Example: A regression model that learns to predict steel prices from historical sales. Data Science    Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data. Example: Cleaning and visualizing e\u2011commerce logs to uncover purchasing trends.</p> </li> <li> <p>Concept Sections A. AI Milestones Excerpt (Definition Box): Artificial Intelligence (AI) \u2013 \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956</p> </li> <li> <p>The Dartmouth Workshop (1956) In the summer of 1956, John\u202fMcCarthy, Marvin\u202fMinsky, Nathaniel\u202fRochester, and Claude\u202fShannon convened at Dartmouth College to explore a bold question: \u201cCan machines be made to simulate human intelligence?\u201d They coined the term \u201cArtificial Intelligence\u201d and launched a two\u2011month study to investigate how machines might \u201clearn from experience,\u201d \u201cmake abstractions,\u201d and \u201cuse language.\u201d</p> </li> </ol> <p>Context &amp; Significance: Before Dartmouth, computers were viewed largely as number crunchers. This workshop reframed them as potential thinking machines, seeding optimism that a small team could tackle \u201cevery aspect of learning or any other feature of intelligence.\u201d</p> <p>First Programs:</p> <p>Logic Theorist (1955): Developed by Newell &amp; Simon, it proved theorems in symbolic logic\u2014demonstrating that \u201cthinking\u201d tasks could be mechanized.</p> <p>General Problem Solver (1957): An early attempt at a universal reasoning engine.</p> <p>Why It Matters Today:</p> <p>Cycle of Hype &amp; AI Winters: The booms and busts following Dartmouth teach us to balance ambition with realism when evaluating modern AI breakthroughs.</p> <p>Legacy in Modern Research: Symbolic reasoning and search algorithms from this era underpin today\u2019s knowledge graphs and constraint solving systems.</p> <ol> <li>Expert Systems Era (1970s\u20131980s) As symbolic AI matured, expert systems emerged\u2014rule\u2011based programs encoding human expertise as \u201cif\u2013then\u201d statements.</li> </ol> <p>Core Idea: Encode domain knowledge in production rules:</p> <p>java Copy Edit IF symptom = fever AND symptom = rash THEN suggest = measles Notable Example \u2013 MYCIN (1972\u20131980):</p> <p>Built at Stanford, MYCIN contained ~600 rules for diagnosing bacterial infections and recommending antibiotics.</p> <p>It queried patient data (age, symptoms), applied its rule base, and in blind tests matched or outperformed human experts.</p> <p>Strengths &amp; Limitations:</p> <p>Strength: Transparent logic\u2014every recommendation traces back to specific rules.</p> <p>Limitation: Required hand\u2011crafting thousands of rules and handled uncertainty poorly (no probabilistic reasoning).</p> <p>Modern Relevance:</p> <p>Rule\u2011based approaches inform decision support in finance and healthcare.</p> <p>Today\u2019s hybrid systems combine rules with statistical ML (e.g., regulatory checks plus model\u2011based scoring).</p> <ol> <li>Deep Learning Boom (2010s\u2013Present) The field\u2019s third surge harnessed large datasets and GPU acceleration to train deep neural networks\u2014models with many layers that learn hierarchical features automatically.</li> </ol> <p>Key Breakthrough \u2013 AlexNet (2012):</p> <p>An eight\u2011layer convolutional neural network (CNN) that halved error rates on the ImageNet challenge (1.2\u202fmillion labeled images, 1,000 categories).</p> <p>Employed ReLU activations, dropout regularization, and GPU\u2011based training.</p> <p>Why Deep Learning Emerged:</p> <p>Data: Massive labeled datasets (images, text, speech).</p> <p>Compute: GPUs excel at parallel matrix operations critical for neural nets.</p> <p>Algorithms: Innovations like batch normalization, architectural search, and optimized backpropagation.</p> <p>Transformative Applications:</p> <p>Computer Vision: Object detection (self\u2011driving cars), medical imaging (tumor detection).</p> <p>Natural Language Processing: Language translation, text generation (GPT\u2011style models).</p> <p>Speech &amp; Audio: Voice assistants, real\u2011time translation.</p> <p>Why It Matters for You:</p> <p>Modern AI frameworks (TensorFlow, PyTorch) are built around neural networks.</p> <p>This era explains why subsequent terms focus on coding deep models and leveraging pretrained architectures for rapid deployment.</p> <p>C. Probability Basics Excerpt (Definition Box): Probability Theory \u2013 \u201cThe mathematical framework for quantifying uncertainty and modeling random phenomena.\u201d</p> <p>C1. Introduction What Is Chance?</p> <p>Everyday Analogy: Flipping a coin. You know there are two sides\u2014heads or tails\u2014but you can\u2019t predict which will land face up.</p> <p>Key Idea: Probability measures how likely something is to happen, on a scale from 0 (impossible) to 1 (certain).</p> <p>Example: A fair coin has probability 0.5 of landing heads.</p> <p>Simple Data &amp; Averages</p> <p>Real World Example: Your test scores this week: 80%, 90%, 70%, 100%, 60%.</p> <p>Mean (Average): Add them up and divide by the number of tests: ( 80 + 90 + 70 + 100 + 60 ) / 5 = 80 % (80+90+70+100+60)/5=80%</p> <p>Why It Matters: The mean gives a sense of \u201ctypical\u201d performance.</p> <p>Measuring Spread (Variance)</p> <p>Analogy: If all scores are close to 80% (say 75%,\u202f80%,\u202f85%), that\u2019s low spread; if they vary widely (60%,\u202f100%,\u202f70%), that\u2019s high spread.</p> <p>Step by Step (Scores Example):</p> <p>Compute each score\u2019s difference from the mean (80): e.g., 60\u201380 = \u201320.</p> <p>Square these differences to make them positive: (\u201320)\u00b2 = 400.</p> <p>Average the squared differences: if squares are [400,100,100,400,400], mean = 280.</p> <p>That average (280) is the variance; its square root (\u224816.7) is the standard deviation.</p> <p>Real World Uses:</p> <p>Weather Forecasts: \u201cThere\u2019s a 30% chance of rain\u201d guides umbrella choices.</p> <p>Quality Control: A factory measures weight of cereal boxes; variance tells if the filling machine is consistent.</p> <p>C2. Formal Definitions &amp; Deep Dive Understanding Random Variables A random variable  \ud835\udc4b X formalizes outcomes of random processes by assigning numeric values.</p> <p>Discrete RV: Takes countable values (e.g., die rolls, number of returned orders).</p> <p>Example: Rolling a six\u2011sided die \u2192  \ud835\udc4b \u2208 { 1 , 2 , 3 , 4 , 5 , 6 } X\u2208{1,2,3,4,5,6} with  \ud835\udc43 ( \ud835\udc4b = \ud835\udc58 ) = 1 6 P(X=k)=  6 1 \u200b  .</p> <p>Continuous RV: Takes any value in a continuum (e.g., time between machine failures).</p> <p>Example: Time (in minutes) between software crashes might follow an exponential distribution: \ud835\udc53 ( \ud835\udc61 ) = \ud835\udf06 \ud835\udc52 \u2212 \ud835\udf06 \ud835\udc61 , \u00a0 \ud835\udc61 \u2265 0 f(t)=\u03bbe  \u2212\u03bbt  ,\u00a0t\u22650.</p> <p>Expectation (Mean) The expectation  \ud835\udc38 [ \ud835\udc4b ] E[X] is the long\u2011run average if the experiment repeats infinitely.</p> <p>Formula (Discrete): \ud835\udc38 [ \ud835\udc4b ] = \u2211 \ud835\udc56 \ud835\udc65 \ud835\udc56 \u2009 \ud835\udc43 ( \ud835\udc4b = \ud835\udc65 \ud835\udc56 ) E[X]=\u2211  i \u200b  x  i \u200b  P(X=x  i \u200b  )</p> <p>Formula (Continuous): \ud835\udc38 [ \ud835\udc4b ] = \u222b \u2212 \u221e \u221e \ud835\udc65 \u2009 \ud835\udc53 ( \ud835\udc65 ) \u2009 \ud835\udc51 \ud835\udc65 E[X]=\u222b  \u2212\u221e \u221e \u200b  xf(x)dx</p> <p>Worked Example (Die): \ud835\udc38 [ \ud835\udc4b ] = 1 + 2 + 3 + 4 + 5 + 6 6 = 3.5 E[X]=  6 1+2+3+4+5+6 \u200b  =3.5</p> <p>Relevance: Loss functions like mean squared error minimize expected error; understanding expectation clarifies why we average squared deviations.</p> <p>Variance &amp; Standard Deviation</p> <p>Variance: V a r ( \ud835\udc4b ) = \ud835\udc38 [ ( \ud835\udc4b \u2212 \ud835\udc38 [ \ud835\udc4b ] ) 2 ] Var(X)=E[(X\u2212E[X])  2  ]</p> <p>Standard Deviation: \ud835\udf0e = V a r ( \ud835\udc4b ) \u03c3=  Var(X) \u200b</p> <p>Worked Example (Die): V a r ( \ud835\udc4b ) = ( 1 \u2212 3.5 ) 2 + \u22ef + ( 6 \u2212 3.5 ) 2 6 = 17.5 6 \u2248 2.92 , \u00a0 \ud835\udf0e \u2248 1.71 Var(X)=  6 (1\u22123.5)  2  +\u22ef+(6\u22123.5)  2</p> <p>\u200b  =  6 17.5 \u200b  \u22482.92,\u00a0\u03c3\u22481.71</p> <p>Relevance: Guides feature scaling, sets confidence intervals, and underpins uncertainty quantification in finance or anomaly detection.</p> <p>Why These Concepts Matter in AI</p> <p>Model Training: Loss functions (e.g., MSE) rely on expectation of squared errors.</p> <p>Uncertainty Quantification: Variance informs risk metrics (VaR, confidence intervals).</p> <p>Feature Engineering: Distribution shapes dictate transformations (e.g., log scaling skewed data).</p> <p>D. Linear Algebra Basics Excerpt (Definition Box): Linear Algebra \u2013 \u201cThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.\u201d</p> <p>D1. Introduction Vectors as Lists</p> <p>Analogy: A grocery list: [2 bananas, 1 loaf bread, 500 g cheese].</p> <p>Key Idea: A vector is just a list of numbers representing \u201cfeatures.\u201d</p> <p>Matrices as Tables</p> <p>Analogy: A seating chart in class: rows are table numbers, columns are seat positions.</p> <p>mathematica Copy Edit |    | S1 | S2 | S3 | |----|----|----|----| | T1 | A  | B  | C  | | T2 | D  | E  | F  | Key Idea: A matrix is multiple vectors \u201cstacked\u201d into rows or columns.</p> <p>Dot Product Intuition</p> <p>Example (Bill Splitting): You and a friend order appetizers [3, 2] plates and drinks [1, 2] each. To compute total cost if plates = $5, drinks = $2: [ 3 , 2 ] \u22c5 [ 5 , 2 ] = 3 \u00d7 5 + 2 \u00d7 2 = 15 + 4 = $ 19 [3,2]\u22c5[5,2]=3\u00d75+2\u00d72=15+4=$19</p> <p>Why It Matters: Combines quantities and prices; same math as a regression prediction.</p> <p>Real World Matrix Use</p> <p>Recipe scaling: A 4\u2011serving recipe\u2019s ingredients in a matrix, multiply by 1.5 to get 6 servings.</p> <p>School timetable: Days\u202f\u00d7\u202fhours grid for scheduling classes.</p> <p>D2. Formal Definitions &amp; Deep Dive Vectors &amp; Their Interpretation A vector  \ud835\udc65 \u2208 \ud835\udc45 \ud835\udc5b x\u2208R  n   is an ordered list of  \ud835\udc5b n numbers representing features or data points.</p> <p>Example: \ud835\udc65 = [ age , monthly_spend , num_orders ] = [ 45 , 320.5 , 12 ] x=[age,monthly_spend,num_orders]=[45,320.5,12]</p> <p>Matrices &amp; Batch Operations A matrix  \ud835\udc4b \u2208 \ud835\udc45 \ud835\udc5a \u00d7 \ud835\udc5b X\u2208R  m\u00d7n   stacks  \ud835\udc5a m row vectors of dimension  \ud835\udc5b n.</p> <p>Example:</p>"},{"location":"week-1/#x","title":"\ud835\udc4b","text":"<p>[ 45 320.5 12 23 150.0 5 \u22ee \u22ee \u22ee ] X=  \u200b</p> <p>45 23 \u22ee \u200b</p> <p>320.5 150.0 \u22ee \u200b</p> <p>12 5 \u22ee \u200b</p> <p>\u200b</p> <p>Dot Product &amp; Linear Transformations</p> <p>Dot Product: \ud835\udc4e \u22c5 \ud835\udc4f = \u2211 \ud835\udc56 = 1 \ud835\udc5b \ud835\udc4e \ud835\udc56 \u2009 \ud835\udc4f \ud835\udc56 a\u22c5b=\u2211  i=1 n \u200b  a  i \u200b  b  i \u200b</p> <p>Example: [1,2,3] \u22c5 [4,5,6] = 32</p> <p>Use in AI:</p> <p>Regression:  \ud835\udc66 ^ = \ud835\udc64 \u22c5 \ud835\udc65 + \ud835\udc4f y ^ \u200b  =w\u22c5x+b</p> <p>Neural Nets: Each neuron computes  \ud835\udc67 = \ud835\udc64 \u22c5 \ud835\udc65 + \ud835\udc4f z=w\u22c5x+b, then applies an activation.</p> <p>Matrix Multiplication \ud835\udc36 = \ud835\udc34 \u00d7 \ud835\udc35 , \ud835\udc36 \ud835\udc56 \ud835\udc57 = \u2211 \ud835\udc58 = 1 \ud835\udc5b \ud835\udc34 \ud835\udc56 \ud835\udc58 \ud835\udc35 \ud835\udc58 \ud835\udc57 C=A\u00d7B,C  ij \u200b  =\u2211  k=1 n \u200b  A  ik \u200b  B  kj \u200b</p> <p>Example: Transforming feature spaces or chaining layers in a deep network.</p> <p>Relevance for AI Practitioners</p> <p>Batch Processing: GPUs and NumPy rely on vectorized matrix operations.</p> <p>Model Introspection: Weight matrices and activation maps in CNNs are built on these operations.</p> <p>Dimensionality Reduction: PCA uses eigenvectors/eigenvalues of covariance matrices to compress data.</p> <ol> <li>Tools Installation &amp; Setup Windows &amp; Mac</li> </ol> <p>A. Install Python &amp; Anaconda Navigate to: https://www.anaconda.com/products/distribution</p> <p>Download the Python\u202f3.x installer for your OS.</p> <p>Run the installer, accept defaults.</p> <p>Open Anaconda Navigator from your Start menu (Windows) or Applications folder (Mac).</p> <p>B. Launch Jupyter Notebook In Anaconda Navigator, click Launch under Jupyter Notebook.</p> <p>A browser window opens showing your file system.</p> <p>Click New \u2192 Python 3.</p> <p>Rename the notebook to Week1_AI_Math.ipynb.</p> <p>C. Install &amp; Import NumPy &amp; pandas In a notebook cell, run:</p> <p>bash Copy Edit !conda install numpy pandas -y Then, in the next cell:</p> <p>python Copy Edit import numpy as np import pandas as pd 5. Step-by-Step Exercises Exercise 1: Die Roll Simulation &amp; Statistics (Template) Exercise Overview &amp; Purpose</p> <p>What we\u2019re doing: Simulate 1,000 rolls of a fair six\u2011sided die in Python to compute the empirical mean and variance.</p> <p>Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p> <p>Concept Reinforcement</p> <p>Probability &amp; Random Variables</p> <p>Expectation &amp; Variance</p> <p>Sampling Variability</p> <p>Real World Relevance</p> <p>Quality Control (defect rate simulation)</p> <p>Risk Modeling (Monte Carlo portfolio variance)</p> <p>Randomized Algorithms &amp; Game Balancing</p> <p>Step by Step Instructions</p> <p>python Copy Edit import numpy as np</p>"},{"location":"week-1/#simulate-1000-die-rolls","title":"Simulate 1,000 die rolls","text":"<p>rolls = np.random.randint(1, 7, size=1000)</p>"},{"location":"week-1/#compute-statistics","title":"Compute statistics","text":"<p>mean_rolls = rolls.mean() var_rolls  = rolls.var()</p> <p>print(\"Simulated Mean:    \", mean_rolls) print(\"Simulated Variance:\", var_rolls) Notes:</p> <p>np.random.randint(1, 7, size=1000): integers 1\u20136</p> <p>.mean(), .var(): compute empirical mean &amp; variance</p> <p>Expected Outcomes &amp; Interpretation</p> <p>Mean \u2248\u202f3.5, Variance \u2248\u202f2.92 (\u00b1 sampling noise)</p> <p>Larger samples converge closer to theory</p> <p>Extensions &amp; Variations</p> <p>Vary sample size (100, 10,000)</p> <p>Simulate weighted/unfair die</p> <p>Plot histogram with matplotlib</p> <p>Additional Notes &amp; Tips</p> <p>Use np.random.seed(42) for reproducibility</p> <p>Avoid Python loops; prefer NumPy vectorization</p> <p>Exercise\u202f2: Coin Flip Probability Estimation Overview &amp; Purpose Simulate 10,000 coin flips to estimate the probability of heads and tails.</p> <p>Concept Reinforcement</p> <p>Discrete random variables</p> <p>Empirical vs. theoretical probability</p> <p>Real World Relevance</p> <p>A/B testing conversion rates (success/failure)</p> <p>Clinical trial outcomes</p> <p>Step by Step Instructions</p> <p>python Copy Edit import numpy as np</p> <p>np.random.seed(0) flips = np.random.choice(['H','T'], size=10000) p_heads = np.mean(flips == 'H') p_tails = np.mean(flips == 'T') print(f\"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\") Expected Outcomes &amp; Interpretation ~0.5 each, with fluctuations ~\u00b10.01.</p> <p>Extensions &amp; Variations</p> <p>Weighted coin (p=['H':0.3,'T':0.7])</p> <p>Plot bar chart of counts</p> <p>Additional Notes &amp; Tips Use np.random.seed(\u2026) for reproducibility.</p> <p>Exercise\u202f3: Histogram of Die Rolls Overview &amp; Purpose Visualize the distribution of the 1,000 die rolls from Exercise\u202f1.</p> <p>Concept Reinforcement</p> <p>Frequency vs. probability</p> <p>Data visualization basics</p> <p>Real World Relevance</p> <p>Sales distribution by category</p> <p>Error rates by batch</p> <p>Step by Step Instructions</p> <p>python Copy Edit import matplotlib.pyplot as plt</p> <p>plt.hist(rolls, bins=range(1,8), align='left', rwidth=0.8) plt.xlabel('Die Face') plt.ylabel('Frequency') plt.title('Histogram of 1,000 Die Rolls') plt.show() Expected Outcomes &amp; Interpretation Bars roughly equal height for faces\u202f1\u20136.</p> <p>Extensions &amp; Variations</p> <p>Normalized histogram (density=True)</p> <p>Overlay theoretical PMF</p> <p>Additional Notes &amp; Tips Ensure bins=range(1,8) to center on integer faces.</p> <p>Exercise\u202f4: Exponential Distribution Simulation Overview &amp; Purpose Simulate 5,000 samples from an exponential distribution (mean\u202f=\u202f2) and compute mean/variance.</p> <p>Concept Reinforcement</p> <p>Continuous random variables</p> <p>Relationship between distribution parameters and statistics</p> <p>Real World Relevance</p> <p>Time between machine failures</p> <p>Call-center interarrival times</p> <p>Step by Step Instructions</p> <p>python Copy Edit samples = np.random.exponential(scale=2, size=5000) print(\"Empirical Mean:\", samples.mean()) print(\"Empirical Variance:\", samples.var()) Expected Outcomes &amp; Interpretation Mean \u2248\u202f2; variance \u2248\u202f4.</p> <p>Extensions &amp; Variations</p> <p>Change scale (mean) parameter</p> <p>Plot histogram + theoretical PDF</p> <p>Additional Notes &amp; Tips Use np.histogram or matplotlib for PDF overlay.</p> <p>Exercise\u202f5: Normal Distribution Sampling Overview &amp; Purpose Draw 10,000 samples from a standard normal (mean\u202f0, \u03c3\u202f=\u202f1) and verify statistics.</p> <p>Concept Reinforcement</p> <p>Properties of Gaussian distribution</p> <p>Central Limit Theorem preview</p> <p>Real World Relevance</p> <p>Measurement errors</p> <p>Standardized test score modeling</p> <p>Step by Step Instructions</p> <p>python Copy Edit normals = np.random.randn(10000) print(\"Mean:\", normals.mean()) print(\"Variance:\", normals.var()) Expected Outcomes &amp; Interpretation Mean \u2248\u202f0, variance \u2248\u202f1.</p> <p>Extensions &amp; Variations</p> <p>Use np.random.normal(loc, scale, size)</p> <p>QQ plot vs. theoretical normal</p> <p>Additional Notes &amp; Tips Matplotlib\u2019s plt.hist(..., density=True) for PDF shape.</p> <p>Exercise\u202f6: Sampling Distribution of the Mean Overview &amp; Purpose Simulate 1,000 experiments, each of 100 die rolls, record sample means, and examine their distribution.</p> <p>Concept Reinforcement</p> <p>Law of Large Numbers</p> <p>Sampling variability reduction</p> <p>Real World Relevance</p> <p>Polling averages</p> <p>Quality metrics over batches</p> <p>Step by Step Instructions</p> <p>python Copy Edit means = [np.random.randint(1,7,100).mean() for _ in range(1000)] plt.hist(means, bins=20) plt.title('Sampling Distribution of Die Roll Means') plt.show() Expected Outcomes &amp; Interpretation Histogram approximates normal around 3.5 with narrower spread.</p> <p>Extensions &amp; Variations</p> <p>Vary experiment size (n=10, n=1000)</p> <p>Compute standard error (\u03c3/\u221an)</p> <p>Additional Notes &amp; Tips List comprehensions vs. loops for clarity.</p> <p>Exercise\u202f7: Weighted Dice Simulation Overview &amp; Purpose Simulate 1,000 rolls of a biased die with  \ud835\udc43 ( 6 ) = 0.5 P(6)=0.5, others equal.</p> <p>Concept Reinforcement</p> <p>Custom discrete distributions</p> <p>Impact of bias on mean/variance</p> <p>Real World Relevance</p> <p>Biased processes in manufacturing</p> <p>Skewed customer behavior models</p> <p>Step by Step Instructions</p> <p>python Copy Edit faces = [1,2,3,4,5,6] probs = [0.1]*5 + [0.5] rolls_biased = np.random.choice(faces, size=1000, p=probs) print(\"Mean:\", rolls_biased.mean(), \"Variance:\", rolls_biased.var()) Expected Outcomes &amp; Interpretation Mean &gt;\u202f3.5, variance different from fair die.</p> <p>Extensions &amp; Variations</p> <p>Tune probs for different biases</p> <p>Compare histograms side by side</p> <p>Additional Notes &amp; Tips Sum of probs must equal 1.</p> <p>Exercise\u202f8: Vector Addition &amp; Scaling Overview &amp; Purpose Demonstrate vector addition and scalar multiplication with feature vectors.</p> <p>Concept Reinforcement</p> <p>Vector space operations</p> <p>Geometric interpretation</p> <p>Real World Relevance</p> <p>Combining feature influences</p> <p>Scaling normalized data</p> <p>Step by Step Instructions</p> <p>python Copy Edit v1 = np.array([2, 4, 6]) v2 = np.array([1, 3, 5]) sum_v   = v1 + v2      # vector addition scaled_v = 0.5 * v1    # scalar multiplication print(\"Sum:\", sum_v) print(\"Scaled:\", scaled_v) Expected Outcomes &amp; Interpretation Sum = [3, 7, 11]; scaled = [1, 2, 3].</p> <p>Extensions &amp; Variations</p> <p>Compute dot product of sum_v and v2</p> <p>Visualize vectors in 2D/3D</p> <p>Additional Notes &amp; Tips Ensure consistent dimensions.</p> <p>Exercise\u202f9: Matrix Multiplication Demonstration Overview &amp; Purpose Multiply a 2\u00d73 matrix by a 3\u00d72 matrix to reinforce matrix multiplication rules.</p> <p>Concept Reinforcement</p> <p>Shape compatibility</p> <p>Summation over inner index</p> <p>Real World Relevance</p> <p>Transforming feature spaces</p> <p>Composition of network layers</p> <p>Step by Step Instructions</p> <p>python Copy Edit A = np.array([[1,2,3],[4,5,6]]) B = np.array([[7,8],[9,10],[11,12]]) C = A.dot(B) print(\"Result:\\n\", C) Expected Outcomes &amp; Interpretation C = [[58, 64], [139, 154]].</p> <p>Extensions &amp; Variations</p> <p>Reverse multiplication to show error</p> <p>Use @ operator in Python 3.5+</p> <p>Additional Notes &amp; Tips Check shapes via A.shape and B.shape.</p> <p>Exercise\u202f10: PCA on Toy Dataset Overview &amp; Purpose Perform PCA on a small 2\u2011D dataset to reduce to 1\u2011D and visualize variance capture.</p> <p>Concept Reinforcement</p> <p>Eigenvectors/eigenvalues</p> <p>Dimensionality reduction</p> <p>Real World Relevance</p> <p>Compressing image data</p> <p>Feature extraction for clustering</p> <p>Step by Step Instructions</p> <p>python Copy Edit from sklearn.decomposition import PCA import numpy as np</p> <p>X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0]]) pca = PCA(n_components=1) X_pca = pca.fit_transform(X) print(\"Explained variance ratio:\", pca.explained_variance_ratio_) print(\"Projected data:\\n\", X_pca) Expected Outcomes &amp; Interpretation Most variance captured in first component (\u224898%).</p> <p>Extensions &amp; Variations</p> <p>Plot original vs. projected points</p> <p>Try n_components=2</p> <p>Additional Notes &amp; Tips Requires scikit\u2011learn installation.</p> <ol> <li>Summary of Week\u202f1 Throughout this first week, you have:</li> </ol> <p>Traced AI History: From the Dartmouth Workshop to Expert Systems (MYCIN) and the Deep Learning revolution (AlexNet).</p> <p>Built Probability Skills: Learned random variables, expectation, variance\u2014both by hand and in Python (die rolls, coin flips, exponential and normal sampling).</p> <p>Visualized Data: Plotted histograms, demonstrated Central Limit Theorem.</p> <p>Handled Bias: Modeled a weighted die to see effects on distribution.</p> <p>Applied Linear Algebra: Performed vector ops, matrix multiplication, and PCA for dimensionality reduction.</p> <p>These foundational concepts and hands\u2011on exercises prepare you for more advanced AI and ML topics.</p> <ol> <li>Additional Resources Probability Fundamentals: Khan Academy \u201cIntroduction to Probability\u201d</li> </ol> <p>Linear Algebra Visualizations: 3Blue1Brown \u201cEssence of Linear Algebra\u201d series</p> <p>Python Tutorials: Official Python documentation at\u202fpython.org</p>"},{"location":"week-2/","title":"Week 2","text":"<p>testing the editing of this page 1x </p>"}]}