{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI University","text":"<p>Welcome to the AI University Curriculum. Use the left menu to pick a week.</p>"},{"location":"cheat-sheet/","title":"Cheat Sheet","text":"<p>Quick Reminder Cheat Sheets</p>"},{"location":"cheat-sheet/#1-everyday-workflow-edit-preview-commit-deploy","title":"1. Everyday Workflow (Edit \u2192 Preview \u2192 Commit \u2192 Deploy)","text":"<p>Local preview</p> <p><pre><code>python -m mkdocs serve\n</code></pre> Open: http://127.0.0.1:8000/ (Leave terminal running; Ctrl+C stops it.)</p> <p>Commit &amp; push <pre><code>git add docs/...\ngit commit -m \"Your message\"\ngit push origin main\n</code></pre> Publish to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#2-common-git-commands","title":"2. Common Git Commands","text":"<pre><code>git status                 # see what changed\ngit add &lt;file&gt;             # stage a file\ngit add .                  # stage everything\ngit commit -m \"message\"    # commit staged changes\ngit push origin main       # push to GitHub\ngit pull origin main       # pull latest from GitHub\n</code></pre> <p>Undo a staged file: <pre><code>git restore --staged &lt;file&gt;\n</code></pre></p>"},{"location":"cheat-sheet/#3-mkdocs-basics","title":"3. MkDocs Basics","text":"<p>Edit pages in <code>docs/</code> Navigation lives in <code>mkdocs.yml</code> under <code>nav:</code></p> <p>Rebuild local site <pre><code>python -m mkdocs serve\n</code></pre></p> <p>Deploy to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#add-a-new-page","title":"Add a new page","text":"<ol> <li>Create <code>docs/new-page.md</code></li> <li>Add to <code>mkdocs.yml</code>: <pre><code>nav:\n  - Home: index.md\n  - Week 1: week-1.md\n  - Week 2: week-2.md\n  - Cheat Sheets: cheat-sheet.md\n  - New Page: new-page.md\n  ```\n\n## 4. VS Code Tips\nToggle terminal: **Ctrl + `**\nCommand Palette: **Ctrl + Shift + P**\nFind/Replace regex: **Ctrl + H**, click `.*`\nReload the window (fix UI glitches): **Developer: Reload Window**\n\n  ## 5. Troubleshooting\n```bash\npython -m mkdocs serve\n</code></pre></li> </ol> <p>Use <code>http://127.0.0.1:8000/</code> (colon, not dot).</p>"},{"location":"cheat-sheet/#github-pages-not-updated","title":"GitHub Pages not updated","text":"<pre><code>python -m mkdocs gh-deploy\n</code></pre> <p>Wait ~30 seconds and refresh the live site.</p>"},{"location":"cheat-sheet/#site-shows-up-in-git-changes","title":"<code>site/</code> shows up in Git changes","text":"<p>Ensure <code>.gitignore</code> contains <code>site/</code>.</p> <p>If it's already tracked: <pre><code>git rm -r --cached site\ngit commit -m \"Remove generated site\"\ngit push origin main\n</code></pre></p>"},{"location":"roadmap/","title":"Curriculum Roadmap","text":"<p>A high\u2011level view of all 24 weeks. Click into each Week page for full details.</p>"},{"location":"roadmap/#term-1-aiu-101-introduction-to-ai-ml-weeks-16","title":"Term 1 \u2014 AIU\u202f101: Introduction to AI &amp; ML  (Weeks 1\u20136)","text":"Week 1 \u2013 History of AI &amp; Math Foundations <p>Module Title: History of AI &amp; Math Foundations Focus: AI milestones, core definitions (AI/ML/Data Science), probability (mean/variance), linear algebra (vectors/matrices), tool setup. Outputs: Working Python/Jupyter env, 10 math/probability exercises. Link: Week\u00a01 details</p> Week 2 \u2013 Supervised Learning: Regression &amp; Classification <ul> <li>Linear vs. logistic regression (MSE, cross\u2011entropy)  </li> <li>Metrics: R\u00b2, accuracy, precision/recall, F1  </li> <li>Overfitting vs. underfitting Tools: scikit\u2011learn, matplotlib Deliverable: Train &amp; evaluate a regression + classification model</li> </ul> Week 3 \u2013 Unsupervised Learning: Clustering &amp; PCA <ul> <li>k\u2011means, elbow method, hierarchical clustering  </li> <li>PCA for dimensionality reduction, variance explained  </li> <li>Customer segmentation case study Tools: scikit\u2011learn, seaborn Deliverable: Segment a dataset &amp; visualize clusters</li> </ul> Week 4 \u2013 Neural Network Basics <ul> <li>Perceptron, activations, MLP architecture  </li> <li>Backpropagation &amp; gradient descent  </li> <li>Regularization: dropout, weight decay Tools: TensorFlow/Keras or PyTorch Deliverable: Build a simple MLP on a tabular dataset</li> </ul> Week 5 \u2013 Ethics, Bias &amp; Responsible AI <ul> <li>Bias sources, fairness metrics (demographic parity, equal opportunity)  </li> <li>Privacy (GDPR), governance frameworks Tools: pandas for subgroup analysis Deliverable: Bias audit on a sample dataset</li> </ul> Week 6 \u2013 Mini Project: Regression Pipeline <ul> <li>Data loading \u2192 cleaning \u2192 splitting  </li> <li>Hyperparameter tuning &amp; reporting Tools: scikit\u2011learn Pipelines, matplotlib Deliverable: End\u2011to\u2011end regression report</li> </ul>"},{"location":"roadmap/#term-2-aiu-201-handson-with-python-ai-frameworks-weeks-712","title":"Term 2 \u2014 AIU\u202f201: Hands\u2011On with Python AI Frameworks  (Weeks 7\u201312)","text":"Week 7 \u2013 Environment Setup &amp; Version Control <ul> <li>Conda vs. venv, install TF/PyTorch/scikit\u2011learn  </li> <li>Jupyter best practices, Git basics Tools: Anaconda, Git/GitHub Deliverable: Reproducible env + first committed notebook</li> </ul> Week 8 \u2013 Data Wrangling &amp; Pipelines <ul> <li>pandas filtering, groupby, merge  </li> <li>Missing data/outliers, feature scaling  </li> <li>sklearn <code>Pipeline</code> Tools: pandas, scikit\u2011learn Deliverable: Reusable data-cleaning pipeline</li> </ul> Week 9 \u2013 Model Building in scikit\u2011learn <ul> <li>Estimator overview, GridSearchCV  </li> <li>Model evaluation pipeline, joblib export Tools: scikit\u2011learn, joblib Deliverable: Tuned model saved &amp; reloaded</li> </ul> Week 10 \u2013 Deep Learning with TensorFlow/Keras <ul> <li>Sequential vs. Functional API  </li> <li>Dense/Conv/LSTM layers, callbacks (EarlyStopping)  </li> <li>TensorBoard monitoring Tools: TensorFlow/Keras, TensorBoard Deliverable: Trained DL model with tracked metrics</li> </ul> Week 11 \u2013 Custom Architectures in PyTorch <ul> <li><code>nn.Module</code>, custom layers  </li> <li><code>Dataset</code>/<code>DataLoader</code>, training loop (forward/backward/step)  </li> <li>Saving/loading <code>state_dict</code> Tools: PyTorch Deliverable: Custom PyTorch model &amp; clean training script</li> </ul> Week 12 \u2013 Deploying &amp; Serving Models <ul> <li>Export formats (SavedModel, TorchScript)  </li> <li>Flask/FastAPI basics, Docker containerization  </li> <li>REST endpoint design Tools: Flask/FastAPI, Docker Deliverable: Local API serving a model</li> </ul>"},{"location":"roadmap/#term-3-aiu-301-ai-for-business-personal-productivity-weeks-1318","title":"Term 3 \u2014 AIU\u202f301: AI for Business &amp; Personal Productivity  (Weeks 13\u201318)","text":"Week 13 \u2013 Marketing Automation &amp; Predictive Lead Scoring <ul> <li>Feature engineering (opens, clicks, demographics)  </li> <li>Logistic vs. tree models, lift charts, ROI  </li> <li>CRM integration Tools: OpenAI API, LangChain, Streamlit Deliverable: Lead scoring prototype &amp; dashboard</li> </ul> Week 14 \u2013 Inventory Forecasting <ul> <li>SARIMA vs. Prophet vs. LSTM  </li> <li>STL decomposition, error metrics (MAPE, MASE)  </li> <li>Auto\u2011reorder alerts Tools: Prophet, pandas, matplotlib Deliverable: Forecast report &amp; alert script</li> </ul> Week 15 \u2013 Quality of Earnings (QoE) with Anomaly Detection <ul> <li>QoE definition, IsolationForest, One\u2011Class SVM  </li> <li>Visualizing anomalies on financial time series Tools: scikit\u2011learn, Plotly Deliverable: Anomaly report for a sample financial dataset</li> </ul> Week 16 \u2013 Customer Training &amp; Chatbot Design <ul> <li>Instructional design for bots, intents/entities  </li> <li>OpenAI GPT integration, fallback flows, metrics Tools: Landbot/Chatfuel, OpenAI API Deliverable: Prototype customer\u2011support chatbot</li> </ul> Week 17 \u2013 Personal Productivity with AI <ul> <li>Smart schedulers, AI writing/summarization, auto\u2011notes  </li> <li>Ethics of personal data use Tools: Microsoft Copilot, Otter.ai Deliverable: Personal AI productivity stack configured</li> </ul> Week 18 \u2013 No\u2011Code AI Workflows <ul> <li>Zapier/Make triggers &amp; actions  </li> <li>Error handling, monitoring  </li> <li>API connections sans code Tools: Zapier, Make, Bubble Deliverable: Automated workflow for a repetitive business task</li> </ul>"},{"location":"roadmap/#term-4-aiu-401-capstone-advanced-ops-weeks-1924","title":"Term 4 \u2014 AIU\u202f401: Capstone &amp; Advanced Ops  (Weeks 19\u201324)","text":"Week 19 \u2013 Capstone Proposal &amp; Scoping <ul> <li>Project charter, scope, deliverables  </li> <li>Stakeholders &amp; success metrics  </li> <li>Data requirements &amp; milestones Tools: GitHub Projects, Trello/Gantt tool Deliverable: Approved capstone charter</li> </ul> Week 20 \u2013 Data Collection &amp; Pipelines <ul> <li>ETL vs. ELT, REST ingestion, batch jobs  </li> <li>Data quality checks (schema, null ratios)  </li> <li>Airflow basics Tools: Airflow, Python scripts Deliverable: Automated data pipeline draft</li> </ul> Week 21 \u2013 Model Development &amp; Iteration <ul> <li>Experiment tracking (MLflow), Optuna tuning  </li> <li>Dataset/model versioning, CV strategies Tools: MLflow, Optuna, sklearn/TF Deliverable: Logged experiments &amp; best model artifacts</li> </ul> Week 22 \u2013 Deployment Architecture &amp; Monitoring <ul> <li>Dockerizing services, Kubernetes basics  </li> <li>Monitoring (Prometheus/Grafana), logging best practices Tools: Docker, k3s/minikube, Prometheus/Grafana Deliverable: Containerized model with metrics dashboard</li> </ul> Week 23 \u2013 Testing, Evaluation &amp; ROI <ul> <li>Unit/integration tests for ML  </li> <li>A/B testing frameworks  </li> <li>Measuring AI ROI Tools: pytest, Streamlit/BI dashboards Deliverable: Test suite + ROI report</li> </ul> Week 24 \u2013 Final Presentation &amp; Ethics Wrap\u2011Up <ul> <li>Storytelling &amp; visualization of impact  </li> <li>Ethical reflection: bias, misuse  </li> <li>Lessons learned &amp; next steps Tools: PowerPoint/Google Slides, Plotly/Dash Deliverable: Final presentation &amp; ethics checklist</li> </ul>"},{"location":"week-1/","title":"History of AI &amp; Math Foundations","text":""},{"location":"week-1/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives</p> <p>By the end of this lesson, you will be able to:</p> <ul> <li>Describe three pivotal AI milestones and their lasting impact.</li> <li>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</li> <li>Understand probability fundamentals\u2014random variables, expectation, variance\u2014and compute them by hand and in Python.</li> <li>Grasp key linear algebra concepts\u2014vectors, matrices, dot products, matrix multiplication\u2014and see how they underpin AI algorithms.</li> <li>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</li> </ul>"},{"location":"week-1/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition &amp; Citation Example Artificial Intelligence (AI) \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956 A chatbot that interprets questions and crafts human\u2011like responses. Machine Learning (ML) Algorithms that improve performance on tasks by learning from data rather than explicit programming. A regression model that learns to predict steel prices from historical sales. Data Science Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data. Cleaning and visualizing e\u2011commerce logs to uncover purchasing trends."},{"location":"week-1/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-1/#a-ai-milestones","title":"A. AI Milestones","text":"<p>Artificial Intelligence (AI) \u2013 \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956</p> The Dartmouth Workshop (1956) <p>What happened (kept): In summer 1956, John\u202fMcCarthy, Marvin\u202fMinsky, Nathaniel\u202fRochester, and Claude\u202fShannon met at Dartmouth College to ask: \u201cCan machines simulate human intelligence?\u201d They coined \u201cArtificial Intelligence\u201d and proposed studying how machines might \u201clearn from experience,\u201d \u201cmake abstractions,\u201d and \u201cuse language.\u201d</p> <p>Context &amp; significance (kept): - Pre\u20111956, computers = number crunchers. Dartmouth reframed them as potential thinking machines. - Sparked optimism (and funding) that small teams could crack \u201cevery aspect of learning.\u201d</p> <p>First programs (kept): - Logic Theorist (1955) \u2013 Newell &amp; Simon proved logic theorems with a program. - General Problem Solver (1957) \u2013 Early universal reasoning attempt.</p> <p>Why this still matters (kept)</p> <ul> <li>Understanding the hype \u2192 disappointment \u2192 AI winters cycle helps you stay realistic about today\u2019s claims.  </li> <li>Symbolic reasoning/search ideas from this era live on in knowledge graphs and constraint solvers.</li> </ul> <p>Deeper Context - Computing state: Transistors were new; programming languages just emerging (FORTRAN in 1957). - Intellectual backdrop: Cybernetics &amp; information theory showed feedback/communication could be formalized\u2014why not \u201cintelligence\u201d? - People &amp; ideas: McCarthy later created LISP (1958); Minsky founded MIT AI Lab; Newell &amp; Simon argued a \u201cphysical symbol system\u201d can generate intelligence.</p> <p>Core Technical Ideas (plain \u2192 precise) - Treat problems as symbol strings; solving = searching through legal symbol transformations. - Heuristics prune search trees (A* algorithm later builds on this mindset). <pre><code>Problem \u2192 Encode as symbols\n        \u2192 Define legal operations (rules)\n        \u2192 Search for a sequence of operations to reach the goal\n</code></pre></p> <p>Business &amp; Societal Impact (Then vs. Now) - Then: Government/academic funding\u2014no commercial AI market yet. - Now: Symbolic engines persist in compliance, tax, scheduling, and planning software.</p> <p>Why It Matters to Dan (CEO/Entrepreneur) - Separate vision from roadmap. Dartmouth overpromised timelines\u2014avoid that trap. - Use hybrid systems: rules for constraints, ML for patterns (common in finance/ops).</p> <p>Misconceptions &amp; Lessons - Myth: \u201cAI began with neural nets.\u201d \u2192 It began symbolically. - Lesson: Each wave leaves usable tools\u2014don\u2019t discard \u201cold\u201d tech.</p> <p>Mini Timeline Callout | Year | Event | Why It Matters | |------|-------|----------------| | 1950 | Turing\u2019s \u201cImitation Game\u201d paper | First formal test of \u201cmachine intelligence\u201d | | 1955 | Logic Theorist | Proved math theorems\u2014machines can \u201creason\u201d | | 1956 | Dartmouth Workshop | AI term coined; field launched | | 1957 | General Problem Solver | Shows limits of \u201cuniversal\u201d reasoning | | 1958 | LISP created | Dominant AI language for decades |</p> <p>See Also - Week\u202f4 (Neural Network Basics) for a contrast with symbolic approaches. - Week\u202f15 (Anomaly Detection/QoE) where rules can wrap around ML for governance.</p> Expert Systems Era (1970s\u20131980s) <p>Core idea (kept): Encode expert knowledge as IF\u2013THEN rules.</p> <pre><code>IF symptom = fever AND symptom = rash\nTHEN suggest = measles\n</code></pre> <p>MYCIN (1972\u20131980) (kept): - ~600 rules to diagnose bacterial infections &amp; suggest antibiotics - Matched/surpassed human experts in blind tests</p> <p>Strengths vs. limits (kept): - \u2705 Transparent logic (traceable to specific rules) - \u274c Hard to scale (thousands of hand\u2011written rules), weak with uncertainty</p> <p>Modern relevance (kept)</p> <ul> <li>Rule\u2011based logic still used in finance/healthcare compliance.  </li> <li>Today\u2019s hybrid systems: rules for regulation + ML models for scoring.</li> </ul> <p>Deeper Context - Hardware shift: Minicomputers/workstations made corporate AI feasible; dedicated LISP machines sold. - Commercialization: DEC\u2019s XCON configured VAX computers\u2014saved ~$25M/year; oil, med, and manufacturing sectors experimented widely.</p> <p>Core Technical Ideas - Production rules + Inference engine: <pre><code>Knowledge Base (rules)\n+ Working Memory (facts)\n+ Inference Engine (forward/backward chaining)\n= Conclusion\n</code></pre>   - Forward chaining: facts \u2192 conclusions   - Backward chaining: goal \u2192 supporting rules - Certainty factors: MYCIN\u2019s workaround for uncertainty (e.g., 0.6 confidence).</p> <p>Business &amp; Societal Impact - Then: Big early ROI stories, then maintenance bottlenecks \u2192 AI winter (late \u201980s). - Now: Business rules engines (Drools, BRMS) enforce policies &amp; compliance.</p> <p>Why It Matters to Dan - Maintenance cost lesson: Knowledge capture &amp; upkeep is expensive\u2014design for continuous update (MLOps mindset). - Explainability: In regulated spaces, traceable logic is critical; combine rules + ML for auditability.</p> <p>Misconceptions &amp; Lessons - Myth: \u201cExpert systems are dead.\u201d \u2192 They evolved into modern decision engines. - Lesson: Balance transparency vs. flexibility\u2014all\u2011rules = rigid, all\u2011ML = opaque.</p> <p>Mini Timeline Callout | Year | Event | Why It Matters | |------|-------|----------------| | 1972 | MYCIN begins | First impactful medical expert system | | 1979 | XCON deployed at DEC | Massive real-world cost savings | | 1984 | Japan\u2019s 5th Gen Project | National bet on symbolic AI | | 1987 | AI Winter hits | Funding collapses after hype | | 1990 | Probabilistic models rise | Bayes nets mark shift to statistical AI |</p> <p>See Also - Week\u202f5 (Ethics/Bias): rules help enforce fairness policies. - Deployment weeks: wrap ML predictions with business rules.</p> Deep Learning Boom (2010s\u2013Present) <p>Key breakthrough \u2013 AlexNet (2012) (kept): - 8\u2011layer CNN, cut ImageNet error rate in half (1.2M images, 1,000 classes) - Used ReLU, dropout, and GPU training.</p> <p>Why deep learning emerged (kept): 1. Data: Huge labeled datasets (images, text, speech) 2. Compute: GPUs = fast parallel matrix ops 3. Algorithms: Batch norm, better backprop, new architectures</p> <p>Transformative apps (kept): - Computer vision: self\u2011driving cars, medical imaging - NLP: translation, GPT\u2011style generation - Speech: voice assistants, real\u2011time translation</p> <p>Why this matters for you (kept)</p> <ul> <li>Modern frameworks (TensorFlow, PyTorch) are built around neural nets.  </li> <li>Explains why later terms focus on coding deep models &amp; leveraging pretrained architectures quickly.</li> </ul> <p>Deeper Context - Before 2012: Neural nets unfashionable; SVMs/ensembles dominated. ImageNet (2009) provided the benchmark that changed that. - GPU/CUDA era: NVIDIA CUDA (2007) let researchers use gaming GPUs for matrix math cheaply. - Open-source wave: Theano/Torch \u2192 TensorFlow (2015) \u2192 PyTorch (2016) democratized DL.</p> <p>Core Technical Ideas - Representation learning: Networks learn features automatically vs. manual engineering. - Backprop + Optimizers: Adam/RMSProp etc. speed convergence. - Regularization: Dropout, data augmentation reduce overfitting. - Transfer learning: Fine\u2011tune large pretrained models for your niche. <pre><code># Pseudocode: fine-tune a pretrained CNN\nbase = load_pretrained_model(\"resnet50\", weights=\"imagenet\")\nfreeze_layers(base, up_to=\"layer3\")\nnew_head = Dense(1, activation=\"sigmoid\")(base.output)\nmodel = Model(inputs=base.input, outputs=new_head)\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nmodel.fit(my_data, my_labels)\n</code></pre></p> <p>Business &amp; Societal Impact - Then: Explosion of startups (vision, NLP, speech). - Now: Foundation models/LLMs power copilots, automations, creative tools. - Risks: Opaqueness, bias, IP, energy costs \u2192 regulation &amp; governance needed.</p> <p>Why It Matters to Dan - Speed to value: APIs + pretrained nets = rapid prototypes. - Data moat: High-quality labeled data becomes a competitive advantage. - Build vs. Buy: Decide between API use (fast, less control) and in-house models (costly, differentiated).</p> <p>Misconceptions &amp; Lessons - Myth: \u201cDeep learning is always best.\u201d \u2192 For tabular biz data, simpler models often win. - Lesson: Start simple; escalate complexity when ROI demands it.</p> <p>Mini Timeline Callout | Year | Event | Why It Matters | |------|-------|----------------| | 2006 | \u201cDeep Learning\u201d term revived (Hinton et al.) | Layer-wise pretraining shows deep nets can work | | 2009 | ImageNet released | Benchmark catalyzes vision progress | | 2012 | AlexNet wins ImageNet | GPU CNN breakthrough | | 2015 | TensorFlow open-sourced | Industrial-grade DL tooling | | 2017 | Transformer paper | Base of modern LLMs (GPT, etc.) | | 2020+| GPT\u20113, Stable Diffusion | Generative AI mainstreams |</p> <p>See Also - Week\u202f4 (Neural Net Basics) &amp; Week\u202f10 (TensorFlow/Keras) for hands-on DL. - Term\u202f3 apps (marketing, inventory, QoE) where transfer learning/LLMs drive ROI.</p> <p>Pattern to Remember</p> <p>Symbolic \u2192 Rule\u2011Based \u2192 Deep Learning 1. Each wave adds new capabilities. 2. Each brings new limitations. 3. Each leaves tools you can still exploit. Your edge = knowing when to mix them for real business value.</p>"},{"location":"week-1/#b-probability-basics","title":"B. Probability Basics","text":"<p>Definition</p> <p>Probability Theory \u2013 \u201cThe mathematical framework for quantifying uncertainty and modeling random phenomena.\u201d</p>"},{"location":"week-1/#b1-gentle-introduction","title":"B1. Gentle Introduction","text":"1. What is Chance? <p>Analogy: Flipping a coin\u2014two outcomes, but you can\u2019t predict which. Key idea: Probability measures how likely something is (0 = impossible, 1 = certain). Example: A fair coin \u2192 P(heads) = 0.5.</p> 2. Simple Data &amp; Averages <p>Real example: Test scores: 80, 90, 70, 100, 60. Mean (average): <pre><code>(80 + 90 + 70 + 100 + 60) / 5 = 80\n</code></pre> Why it matters: The mean tells you what\u2019s \u201ctypical.\u201d</p> 3. Measuring Spread (Variance) <p>Analogy: Scores all near 80% \u2192 small spread; scores all over the place \u2192 big spread. Steps (using the score list above): 1. Subtract the mean (80): e.g. 60 \u2212 80 = \u221220 2. Square them: (\u221220)\u00b2 = 400 3. Average the squares \u2192 variance \u2248 280 4. Square root of variance \u2192 standard deviation \u2248 16.7 Why we care: Spread tells you how consistent or noisy data is\u2014critical for risk or quality control.</p>"},{"location":"week-1/#b2-formal-definitions-deep-dive","title":"B2. Formal Definitions &amp; Deep Dive","text":"1. Random Variables <p>A random variable (RV) assigns numbers to random outcomes.</p> <ul> <li>Discrete RV: countable values (die roll, number of returns)   Example: Fair die \u2192 <pre><code>X \u2208 {1,2,3,4,5,6},   P(X = k) = 1/6\n</code></pre></li> <li>Continuous RV: any value in a range (time between failures)   Example: Exponential distribution for time ( t \u2265 0 ): <pre><code>f(t) = \u03bb e^{\u2212\u03bb t}\n</code></pre></li> </ul> 2. Expectation (Mean) <p>Long\u2011run average outcome if you repeat forever.</p> <ul> <li>Discrete: <pre><code>E[X] = \u03a3 x_i \u00b7 P(X = x_i)\n</code></pre></li> <li>Continuous: <pre><code>E[X] = \u222b x f(x) dx\n</code></pre> Worked example (die): <pre><code>E[X] = (1+2+3+4+5+6) / 6 = 3.5\n</code></pre> Relevance: Loss functions (e.g., MSE) minimize expected error \u2192 expectation is baked into training.</li> </ul> 3. Variance &amp; Standard Deviation <p>Variance: average squared distance from the mean. <pre><code>Var(X) = E[(X \u2212 E[X])^2]\n</code></pre> Std. dev.: <pre><code>\u03c3 = \u221aVar(X)\n</code></pre> Die example: <pre><code>Var \u2248 2.92,  \u03c3 \u2248 1.71\n</code></pre> Why it matters: Tells you how uncertain predictions are, helps build confidence intervals, drives anomaly detection.</p> <p>Why Probability Matters in AI</p> <ul> <li>Model Training: Errors are expectations (means) over data.  </li> <li>Uncertainty: Variance underpins confidence, risk, anomaly flags.  </li> <li>Feature Engineering: Understanding distributions guides transformations (e.g., log scales for skewed data).</li> </ul>"},{"location":"week-1/#c-linear-algebra-basics","title":"C. Linear Algebra Basics","text":"<p>Definition</p> <p>Linear Algebra \u2013 \u201cThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.\u201d</p>"},{"location":"week-1/#c1-gentle-introduction","title":"C1. Gentle Introduction","text":"1. Vectors as Lists <p>Analogy: A grocery list: <code>[2 bananas, 1 loaf bread, 500 g cheese]</code> Key idea: A vector is just a list of numbers representing features.</p> 2. Matrices as Tables <p>Analogy: A seating chart (rows = tables, columns = seats): <pre><code>       S1  S2  S3\n    T1  A   B   C\n    T2  D   E   F\n</code></pre> Key idea: A matrix stacks many vectors into rows or columns.</p> 3. Dot Product Intuition <p>Example (bill splitting): - You &amp; a friend order appetizers <code>[3, 2]</code> and drinks <code>[1, 2]</code>. - Prices: appetizers = \\$5, drinks = \\$2 \u2192 <pre><code>[3, 2] \u00b7 [5, 2] = 3\u00d75 + 2\u00d72 = 19\n</code></pre> Why it matters: Same math as a simple regression prediction (weights \u00d7 features).</p> 4. Real\u2011World Matrix Uses <ul> <li>Recipe scaling: Multiply ingredient matrix by 1.5 to go from 4 to 6 servings.  </li> <li>School timetable: Days \u00d7 hours grid to schedule classes.</li> </ul>"},{"location":"week-1/#c2-formal-definitions-deep-dive","title":"C2. Formal Definitions &amp; Deep Dive","text":"1. Vectors &amp; Their Interpretation <p>A vector x \u2208 \u211d\u207f is an ordered list of n numbers (features). Example: <pre><code>x = [age, monthly_spend, num_orders] = [45, 320.5, 12]\n</code></pre></p> 2. Matrices &amp; Batch Operations <p>A matrix X \u2208 \u211d^{m\u00d7n} stacks m row\u2011vectors of length n. Example (customer table): <pre><code>X = [\n  [45, 320.5, 12],\n  [23, 150.0,  5],\n  ...\n]\n</code></pre></p> 3. Dot Product &amp; Linear Transformations <p>Dot product: <pre><code>a \u00b7 b = \u03a3 (a_i * b_i)\n</code></pre> Use in AI: - Regression:  \u0177 = w \u00b7 x + b - Neural nets:  z = w \u00b7 x + b, then apply activation (e.g., ReLU)</p> 4. Matrix Multiplication <p><pre><code>C = A \u00d7 B,   C_{ij} = \u03a3_k A_{ik} B_{kj}\n</code></pre> Example: Combine/transform features or chain neural network layers.</p> Why Linear Algebra Matters in AI <ul> <li>Speed: GPUs/NumPy rely on vectorized (matrix) ops for efficiency.  </li> <li>Model Insight: Weights, activations, attention maps are matrices/vectors.  </li> <li>Dimensionality Reduction: PCA, SVD use eigenvectors/values to compress data.</li> </ul>"},{"location":"week-1/#4-tools-installation-setup","title":"4. Tools Installation &amp; Setup","text":"<p>You\u2019ll do this once, then reuse the environment all term.</p>"},{"location":"week-1/#a-install-python-anaconda-windows-mac","title":"A. Install Python &amp; Anaconda (Windows &amp; Mac)","text":"<p><pre><code># Visit this in your browser:\nhttps://www.anaconda.com/products/distribution\n</code></pre> 1. Download the Python 3.x installer for your OS. 2. Run the installer, accept defaults. 3. Open Anaconda Navigator (Start Menu on Windows / Applications on Mac).</p>"},{"location":"week-1/#b-launch-jupyter-notebook","title":"B. Launch Jupyter Notebook","text":"<ol> <li>In Anaconda Navigator, click Launch under Jupyter Notebook.  </li> <li>A browser window opens showing your files.  </li> <li>Click New \u2192 Python 3.  </li> <li>Rename it to Week1_AI_Math.ipynb.</li> </ol>"},{"location":"week-1/#c-install-import-numpy-pandas","title":"C. Install &amp; Import NumPy &amp; pandas","text":"<p>In a notebook cell, run: <pre><code>!conda install numpy pandas -y\n</code></pre> Then import: <pre><code>import numpy as np\nimport pandas as pd\n</code></pre></p> <p>Why these tools?</p> <ul> <li>Python/Jupyter: interactive coding &amp; math demos  </li> <li>NumPy: fast vectors/matrices (used everywhere in ML)  </li> <li>pandas: quick data tables, cleaning, summaries</li> </ul>"},{"location":"week-1/#5-step-by-step-exercises","title":"5. Step by Step Exercises","text":"Exercise 1: Die Roll Simulation &amp; Statistics <p>1. Overview &amp; Purpose Simulate 1,000 rolls of a fair six\u2011sided die in Python and compute the empirical mean and variance. Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p> <p>2. Concept Reinforcement - Probability &amp; random variables - Expectation (mean) &amp; variance - Sampling variability / Law of Large Numbers</p> <p>3. Real\u2011World Relevance - Quality control: simulate defect rates in a batch - Risk modeling: Monte Carlo estimates of portfolio variance - Game design: balance randomness in mechanics</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\n# Simulate 1,000 die rolls\nnp.random.seed(42)           # optional: reproducibility\nrolls = np.random.randint(1, 7, size=1000)\n\n# Compute statistics\nmean_rolls = rolls.mean()\nvar_rolls  = rolls.var()\n\nprint(\"Simulated Mean:    \", mean_rolls)\nprint(\"Simulated Variance:\", var_rolls)\n</code></pre></p> <p>Notes: - <code>np.random.randint(1, 7, size=1000)</code> \u2192 integers 1\u20136 - <code>.mean()</code>, <code>.var()</code> \u2192 empirical mean &amp; variance (population variance by default)</p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 3.5, Variance \u2248 2.92 (\u00b1 sampling noise) - Larger sample sizes converge closer to the theoretical values</p> <p>6. Extensions &amp; Variations - Try sample sizes 100, 10,000 and compare stats - Simulate a weighted/unfair die - Plot histogram with <code>matplotlib</code></p> <p>7. Additional Notes &amp; Tips - Use <code>np.random.seed(...)</code> if you want the same results every run - Avoid Python loops when possible\u2014NumPy vectorization is faster</p> Exercise 2: Coin Flip Probability Estimation <p>1. Overview &amp; Purpose Simulate 10,000 coin flips to estimate the probability of heads and tails.</p> <p>2. Concept Reinforcement - Discrete random variables - Empirical vs. theoretical probability</p> <p>3. Real\u2011World Relevance - A/B testing: success/failure rates - Clinical trials: treatment vs. control outcomes</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                      # optional: reproducibility\nflips = np.random.choice(['H', 'T'], size=10000)\n\np_heads = np.mean(flips == 'H')\np_tails = np.mean(flips == 'T')\n\nprint(f\"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\")\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Both \u2248 0.5, with random fluctuation ~\u00b10.01 - Larger samples narrow the gap to 0.5</p> <p>6. Extensions &amp; Variations - Weighted coin: <code>p=['H':0.3, 'T':0.7]</code> - Plot counts with a bar chart</p> <p>7. Additional Notes &amp; Tips - Set <code>np.random.seed(...)</code> when you want reproducible runs</p> Exercise 3: Histogram of Die Rolls <p>1. Overview &amp; Purpose Visualize the distribution of the 1,000 die rolls from Exercise\u202f1.</p> <p>2. Concept Reinforcement - Frequency vs. probability - Basic data visualization</p> <p>3. Real\u2011World Relevance - Sales distribution by category - Error counts per batch in manufacturing</p> <p>4. Step-by-Step Instructions <pre><code>import matplotlib.pyplot as plt\n\nplt.hist(rolls, bins=range(1, 8), align='left', rwidth=0.8)\nplt.xlabel('Die Face')\nplt.ylabel('Frequency')\nplt.title('Histogram of 1,000 Die Rolls')\nplt.show()\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Bars roughly equal for faces 1\u20136 (random noise is okay)</p> <p>6. Extensions &amp; Variations - Normalized histogram: <code>plt.hist(..., density=True)</code> - Overlay the theoretical PMF as a line/bar plot</p> <p>7. Additional Notes &amp; Tips - <code>bins=range(1,8)</code> centers bars on integer faces - If you reused <code>rolls</code> from Exercise\u202f1, you don\u2019t need to re\u2011simulate</p> Exercise 4: Exponential Distribution Simulation <p>1. Overview &amp; Purpose Simulate 5,000 samples from an exponential distribution (mean\u202f=\u202f2) and compute mean/variance.</p> <p>2. Concept Reinforcement - Continuous random variables - Relationship between distribution parameters and statistics</p> <p>3. Real\u2011World Relevance - Time between machine failures - Call\u2011center interarrival times</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                         # optional\nsamples = np.random.exponential(scale=2, size=5000)\n\nprint(\"Empirical Mean:   \", samples.mean())\nprint(\"Empirical Variance:\", samples.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 2 - Variance \u2248 4 Small deviations are normal due to randomness.</p> <p>6. Extensions &amp; Variations - Change <code>scale</code> (mean) parameter - Plot histogram and overlay the theoretical PDF</p> <p>7. Additional Notes &amp; Tips - <code>scale</code> in NumPy\u2019s exponential is ( 1/\u03bb ) (i.e., the mean) - Use <code>matplotlib</code> or <code>seaborn</code> for quick visual checks</p> Exercise 5: Normal Distribution Sampling <p>1. Overview &amp; Purpose Draw 10,000 samples from a standard normal distribution (mean\u202f0, \u03c3\u202f=\u202f1) and verify statistics.</p> <p>2. Concept Reinforcement - Properties of the Gaussian distribution - Central Limit Theorem (preview)</p> <p>3. Real\u2011World Relevance - Measurement error modeling - Standardized test scores / z\u2011scores</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)                    # optional\nnormals = np.random.randn(10000)     # mean=0, std=1\n\nprint(\"Mean:\", normals.mean())\nprint(\"Variance:\", normals.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean \u2248 0, Variance \u2248 1 (allow small deviation)</p> <p>6. Extensions &amp; Variations - Use <code>np.random.normal(loc, scale, size)</code> for non\u2011standard normals - Make a QQ plot vs. theoretical normal to check normality</p> <p>7. Additional Notes &amp; Tips - <code>plt.hist(normals, density=True)</code> to visualize the bell curve</p> Exercise 6: Sampling Distribution of the Mean <p>1. Overview &amp; Purpose Run 1,000 \u201cmini\u2011experiments.\u201d Each experiment rolls a die 100 times, records the mean, and we plot the distribution of those means.</p> <p>2. Concept Reinforcement - Law of Large Numbers - Sampling variability decreases as sample size increases - Sampling distribution &amp; standard error</p> <p>3. Real\u2011World Relevance - Polling averages: many small samples \u2192 distribution of means - Quality control: batch averages instead of single measurements</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)  # optional\nmeans = [np.random.randint(1, 7, 100).mean() for _ in range(1000)]\n\nplt.hist(means, bins=20)\nplt.title('Sampling Distribution of Die Roll Means')\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.show()\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Histogram looks roughly normal, centered near 3.5 - Spread is much narrower than individual die outcomes</p> <p>6. Extensions &amp; Variations - Change experiment size: n=10 vs. n=1000 \u2192 compare spreads - Compute standard error: \u03c3 / \u221an (use \u03c3 \u2248 1.71 for a die)</p> <p>7. Additional Notes &amp; Tips - List comprehensions are fine here; for speed, you can vectorize with NumPy</p> Exercise 7: Weighted Dice Simulation <p>1. Overview &amp; Purpose Simulate 1,000 rolls of a biased die where P(6) = 0.5 and the other faces share the remaining probability.</p> <p>2. Concept Reinforcement - Custom discrete distributions - How bias shifts mean and variance</p> <p>3. Real\u2011World Relevance - Biased processes in manufacturing (defect more likely on one line) - Skewed customer behavior (one product far more popular)</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nnp.random.seed(0)  # optional\nfaces = [1, 2, 3, 4, 5, 6]\nprobs = [0.1]*5 + [0.5]     # 0.1 each for 1\u20135, 0.5 for 6\nrolls_biased = np.random.choice(faces, size=1000, p=probs)\n\nprint(\"Mean:\", rolls_biased.mean(), \"Variance:\", rolls_biased.var())\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Mean &gt; 3.5 due to heavy weight on 6 - Variance will differ from the fair\u2011die case</p> <p>6. Extensions &amp; Variations - Tweak <code>probs</code> for different biases - Plot histograms for fair vs. biased dice side\u2011by\u2011side</p> <p>7. Additional Notes &amp; Tips - Ensure <code>sum(probs) == 1</code> or NumPy will error - You can simulate many biased scenarios to stress\u2011test models</p> Exercise 8: Vector Addition &amp; Scaling <p>1. Overview &amp; Purpose Demonstrate vector addition and scalar multiplication using simple feature vectors.</p> <p>2. Concept Reinforcement - Vector space operations (add, scale) - Geometric interpretation (direction &amp; length)</p> <p>3. Real\u2011World Relevance - Combine feature effects (e.g., marketing channels) - Scale normalized data or weights</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nv1 = np.array([2, 4, 6])\nv2 = np.array([1, 3, 5])\n\nsum_v   = v1 + v2        # vector addition\nscaled_v = 0.5 * v1      # scalar multiplication\n\nprint(\"Sum:   \", sum_v)\nprint(\"Scaled:\", scaled_v)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - <code>Sum</code> = <code>[3, 7, 11]</code> - <code>Scaled</code> = <code>[1, 2, 3]</code></p> <p>6. Extensions &amp; Variations - Compute the dot product: <code>v1.dot(v2)</code> - Visualize 2D/3D vectors (e.g., with matplotlib quiver)</p> <p>7. Additional Notes &amp; Tips - Ensure vectors have the same length for element\u2011wise ops - Scalar multiplication stretches/shrinks the vector length</p> Exercise 9: Matrix Multiplication Demonstration <p>1. Overview &amp; Purpose Multiply a 2\u00d73 matrix by a 3\u00d72 matrix to reinforce matrix\u2011multiplication rules and shape compatibility.</p> <p>2. Concept Reinforcement - Shape rules (inner dimensions must match) - Summation over the inner index (k)</p> <p>3. Real\u2011World Relevance - Transforming feature spaces - Chaining layers in neural networks (each layer = a matrix multiply)</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])          # shape (2, 3)\n\nB = np.array([[ 7,  8],\n              [ 9, 10],\n              [11, 12]])          # shape (3, 2)\n\nC = A.dot(B)            # or: C = A @ B in Python 3.5+\nprint(\"Result:\\n\", C)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation <pre><code>[[ 58  64]\n [139 154]]\n</code></pre> - You can verify: first row \u00d7 first column \u2192 17 + 29 + 3*11 = 58</p> <p>6. Extensions &amp; Variations - Try <code>B @ A</code> to see the shape error - Use larger random matrices to test performance</p> <p>7. Additional Notes &amp; Tips - Check shapes with <code>A.shape</code>, <code>B.shape</code> - <code>@</code> is shorthand for matrix multiply (<code>dot</code>) in NumPy/Python 3.5+</p> Exercise 10: PCA on Toy Dataset <p>1. Overview &amp; Purpose Perform PCA on a tiny 2\u2011D dataset, reduce it to 1\u2011D, and see how much variance is captured.</p> <p>2. Concept Reinforcement - Eigenvectors / eigenvalues - Dimensionality reduction &amp; variance explained</p> <p>3. Real\u2011World Relevance - Compressing image or sensor data - Feature extraction before clustering or modeling</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\n\nX = np.array([\n    [2.5, 2.4],\n    [0.5, 0.7],\n    [2.2, 2.9],\n    [1.9, 2.2],\n    [3.1, 3.0]\n])\n\npca = PCA(n_components=1)\nX_pca = pca.fit_transform(X)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Projected data:\\n\", X_pca)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - First component should capture ~98% of variance for this toy set - Projected 1\u2011D data preserves most \u201cinformation\u201d (spread)</p> <p>6. Extensions &amp; Variations - Plot original 2\u2011D vs. projected 1\u2011D points - Try <code>n_components=2</code> (no reduction) and inspect components</p> <p>7. Additional Notes &amp; Tips - Requires <code>scikit-learn</code> (<code>pip install scikit-learn</code> if missing) - PCA assumes linear structure; nonlinear data may need t\u2011SNE/UMAP</p>"},{"location":"week-1/#6-week-1-summary-what-you-can-now-do","title":"6. Week\u202f1 Summary &amp; What You Can Now Do","text":"<p>You can now\u2026</p> <ul> <li>Explain key AI milestones: Dartmouth (1956), Expert Systems (1970s\u201380s), Deep Learning boom (2010s\u2013present) and why each wave mattered.  </li> <li>Use probability concepts (random variables, mean, variance) and verify them empirically in Python.  </li> <li>Work with basic linear algebra objects (vectors, matrices, dot products, matrix multiplication) and see how they power ML models.  </li> <li>Install and run core tools (Anaconda, Jupyter, NumPy, pandas) to explore data and math interactively.</li> </ul>"},{"location":"week-1/#a-ai-history-in-one-breath","title":"A. AI History in One Breath","text":"<ul> <li>Dartmouth Workshop (1956): coined \u201cAI\u201d; optimism about simulating intelligence. Lesson: Ambition vs. realism\u2014avoid hype traps.  </li> <li>Expert Systems (\u201970s\u2013\u201980s): rule-based IF\u2013THEN logic (e.g., MYCIN). Lesson: Transparency is great, but brittle without probabilities.  </li> <li>Deep Learning (2010s\u2013): data + GPUs + better algorithms (AlexNet etc.) \u2192 breakthroughs. Lesson: Modern toolkits (TensorFlow/PyTorch) center on neural nets.</li> </ul>"},{"location":"week-1/#b-probability-from-intuition-to-math","title":"B. Probability: From Intuition to Math","text":"<ul> <li>Random Variables: discrete (die, coin), continuous (time-to-failure).  </li> <li>Expectation &amp; Variance: long-run average and spread\u2014computed by hand and via NumPy.  </li> <li>Why it matters: Loss functions, risk estimation, feature engineering all use these ideas.  </li> <li>Real world ties: A/B tests, forecasting demand spikes, Monte Carlo risk simulations.</li> </ul> <p>Anchor formulas</p> <ul> <li>Discrete mean: \\(E[X] = \\sum x_i P(X=x_i)\\)  </li> <li>Variance: \\(\\mathrm{Var}(X) = E[(X - E[X])^2]\\)</li> </ul>"},{"location":"week-1/#c-linear-algebra-the-language-of-ml","title":"C. Linear Algebra: The Language of ML","text":"<ul> <li>Vectors: feature lists (e.g., <code>[age, spend, orders]</code>).  </li> <li>Matrices: batches of vectors; all your data at once.  </li> <li>Dot Product: core of regression and neuron activations.  </li> <li>Matrix Multiplication: chaining transformations (layers) in neural nets.  </li> <li>Why it matters: Speed (GPU vectorization), interpretability (weights are matrices), compression (PCA).</li> </ul>"},{"location":"week-1/#d-tools-workflow-locked-in","title":"D. Tools &amp; Workflow Locked In","text":"<ul> <li>Anaconda &amp; Jupyter: you spun up a notebook and ran code.  </li> <li>NumPy/pandas: you handled arrays, stats, and basic data manipulation.  </li> <li>Workflow habits: preview locally (<code>mkdocs serve</code>), commit/push, deploy (<code>gh-deploy</code>).</li> </ul>"},{"location":"week-1/#e-exercises-recap-what-each-taught-you","title":"E. Exercises Recap (What each taught you)","text":"Exercise Core Idea Concept Reinforced Real\u2011World Parallel 1. Die roll stats Empirical vs. theoretical Mean, variance, sampling noise QC sampling, Monte Carlo 2. Coin flips Bernoulli trials Discrete RVs, proportions A/B tests, pass/fail outcomes 3. Histogram Visual distributions Frequency vs. prob., plotting Category sales distributions 4. Exponential sim Time-to-event model Continuous RVs, \u03bb &amp; scale Failure rates, wait times 5. Normal samples Gaussian basics CLT preview, z-scores Measurement error, scores 6. Sampling means Distribution of means Law of Large Numbers Polling averages, batch metrics 7. Weighted die Biased distributions Shifted mean/var, custom PMFs Skewed demand, unfair odds 8. Vector ops Add/scale vectors Vector spaces Feature scaling, weight tuning 9. Matrix multiply Shapes &amp; transforms Linear maps, dot sums NN layers, feature transforms 10. PCA toy data Dimensionality reduction Eigenvectors/variance explained Data compression, preprocessing"},{"location":"week-1/#7-additional-resources","title":"7. Additional Resources","text":"<p>Probability &amp; Statistics</p> <ul> <li>Khan Academy \u2013 Introduction to Probability   Beginner\u2011friendly videos and practice problems. https://www.khanacademy.org/math/statistics-probability/probability-library</li> <li>Seeing Theory (Brown University)   Interactive visual explanations of probability concepts. https://seeing-theory.brown.edu/</li> </ul> <p>Linear Algebra</p> <ul> <li>3Blue1Brown \u2013 Essence of Linear Algebra (YouTube series)   Beautiful visuals for vectors, matrices, dot products, eigenvectors. https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr</li> <li>Khan Academy \u2013 Linear Algebra   Step\u2011by\u2011step lessons with exercises. https://www.khanacademy.org/math/linear-algebra</li> </ul> <p>Python, NumPy &amp; pandas</p> <ul> <li>Official Python Docs \u2013 Syntax, stdlib, tutorials. https://docs.python.org/3/</li> <li>NumPy User Guide \u2013 Arrays, broadcasting, linear algebra. https://numpy.org/doc/stable/user/</li> <li>pandas Getting Started \u2013 DataFrames, cleaning, transforms. https://pandas.pydata.org/docs/getting_started/index.html</li> </ul> <p>AI History &amp; Overviews</p> <ul> <li>\u201cCompeting in the Age of AI\u201d (Iansiti &amp; Lakhani) \u2013 Ch.\u202f1\u20133 (already on your list) </li> <li>\u201cDeep Learning\u201d (Goodfellow, Bengio, Courville) \u2013 Intro &amp; Ch.\u202f6\u20137 (free online) https://www.deeplearningbook.org/</li> </ul> <p>Visualization &amp; Math Intuition</p> <ul> <li>Matplotlib Gallery \u2013 Quick plot recipes. https://matplotlib.org/stable/gallery/index.html</li> <li>Desmos Graphing Calculator \u2013 Fast function plots and geometry. https://www.desmos.com/calculator</li> </ul> <p>Bonus Cheat Sheets</p> <ul> <li>Markdown Syntax Cheat Sheet (for editing your site) https://www.markdownguide.org/cheat-sheet/</li> <li>NumPy/Pandas one\u2011pagers (various printable PDFs) https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf https://www.dataquest.io/blog/numpy-cheat-sheet/</li> </ul>"},{"location":"week-2/","title":"Week 2 Lesson Plan \u2014 Supervised Learning: Regression &amp; Classification","text":""},{"location":"week-2/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives By the end of Week\u202f2, you will be able to:</p> <ul> <li>Explain what supervised learning is and how it differs from unsupervised learning.  </li> <li>Build and evaluate linear regression and logistic regression models using scikit\u2011learn.  </li> <li>Understand and compute key evaluation metrics (MSE, MAE, R\u00b2 for regression; Accuracy, Precision, Recall, F1, ROC\u2013AUC for classification).  </li> <li>Recognize overfitting vs. underfitting and apply basic remedies (train/test split, regularization).  </li> <li>Implement a clean ML workflow: load data \u2192 split \u2192 train \u2192 evaluate \u2192 iterate.</li> </ul>"},{"location":"week-2/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition &amp; Example Supervised Learning Learning a mapping from inputs X to an output y using labeled data. Example: Predicting house price (y) from size and bedrooms (X). Regression Supervised learning where y is numeric/continuous. Example: Predicting steel prices or monthly revenue. Classification Supervised learning where y is categorical. Example: Predicting if a lead will convert (\u201cyes/no\u201d). Loss Function A formula that measures how wrong a prediction is. Models try to minimize this. Example: Mean Squared Error (MSE). Overfitting / Underfitting Overfitting: model memorizes noise, performs poorly on new data. Underfitting: model too simple, misses patterns. Train/Test Split Partition data into a training set (to fit the model) and test set (to evaluate generalization). Precision / Recall / F1 Metrics for classification. Precision: \u201cOf predicted positives, how many were right?\u201d Recall: \u201cOf actual positives, how many did we catch?\u201d F1 balances both."},{"location":"week-2/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-2/#a-supervised-learning-big-picture","title":"A. Supervised Learning: Big Picture","text":"<p>A1. Introduction (Plain English) - Supervised learning is teaching by example. You give the algorithm many input\u2013output pairs, and it learns the relationship. - Two main flavors:   - Regression: Predict numbers (price, demand).   - Classification: Predict categories (spam/not spam, churn/no churn).</p> <p>A2. The Basic Pipeline 1. Collect &amp; clean data (features X, target y) 2. Split into training and test sets 3. Train a model on training data 4. Evaluate on test data (metrics) 5. Tune hyperparameters or choose another model 6. Deploy/Use the model</p>"},{"location":"week-2/#b-linear-regression-deep-dive","title":"B. Linear Regression Deep Dive","text":"<p>B1. Intuition (Grade\u201110 friendly) - Plot points on a graph (e.g., ad spend vs. sales). Draw the \u201cbest\u201d straight line through them. - \u201cBest\u201d usually = the line with the smallest average squared error.</p> <p>B2. Formalism - Model: \\(\\hat{y} = w_0 + w_1 x_1 + \\dots + w_n x_n\\) - MSE: \\(\\frac{1}{m}\\sum_{i=1}^m (\\hat{y}_i - y_i)^2\\)</p> <p>B3. Why You Care - Baseline model for many business problems. - Builds intuition for linear layers in neural networks later.</p>"},{"location":"week-2/#c-logistic-regression-classification","title":"C. Logistic Regression (Classification)","text":"<p>C1. Intuition (Plain English) - Linear combo of features \u2192 pass through sigmoid to get probability (0\u20131). - Threshold (e.g., 0.5) to decide class.</p> <p>C2. Formal Bits - Sigmoid: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), where \\(z = w^\\top x + b\\). - Cross\u2011entropy loss penalizes confident wrong predictions more.</p> <p>C3. Odds &amp; Log\u2011Odds - Logistic regression models log\u2011odds: \\(\\log \\frac{p}{1-p} = w^\\top x + b\\). - Coefficients show how features push probability up/down.</p>"},{"location":"week-2/#d-evaluation-metrics-overfitting","title":"D. Evaluation Metrics &amp; Overfitting","text":"<p>D1. Regression Metrics - MSE / RMSE: Squared errors (sensitive to outliers). - MAE: Absolute errors (robust to outliers). - R\u00b2: Variance explained.</p> <p>D2. Classification Metrics - Accuracy: Overall correctness (misleading on imbalance). - Precision / Recall / F1: Balance false positives vs. false negatives. - ROC\u2013AUC: Performance across thresholds.</p> <p>D3. Overfitting vs. Underfitting - Overfit: Great on train, bad on test. - Underfit: Bad everywhere. - Fixes: More data, simpler model, regularization (L1/L2), cross\u2011validation.</p>"},{"location":"week-2/#4-tools-installation-setup-week-2-specific","title":"4. Tools Installation &amp; Setup (Week\u202f2 Specific)","text":"<p>Assumes Python, Jupyter, NumPy, and pandas are already installed from Week\u202f1.</p>"},{"location":"week-2/#a-download-install-only-if-you-dont-already-have-them","title":"A. Download / Install (only if you don\u2019t already have them)","text":"<ul> <li>Python 3.x (official): https://www.python.org/downloads/ </li> <li>Anaconda (optional, easier package management): https://www.anaconda.com/download </li> <li>Git (already on your machine; link for reference): https://git-scm.com/downloads </li> <li>scikit-learn docs: https://scikit-learn.org/stable/install.html </li> <li>matplotlib install guide: https://matplotlib.org/stable/users/installing.html </li> <li>seaborn install guide (optional): https://seaborn.pydata.org/installing.html</li> </ul>"},{"location":"week-2/#b-install-scikit-learn-matplotlib","title":"B. Install scikit-learn &amp; matplotlib","text":"<p>Pick ONE method (conda or pip).</p> <p>Using conda (recommended if you installed Anaconda):</p> <pre><code>conda install scikit-learn matplotlib -y\n</code></pre> <p>Or with pip (pure Python):</p> <pre><code>pip install scikit-learn matplotlib\n# Windows fallback if 'pip' isn\u2019t recognized:\npy -m pip install scikit-learn matplotlib\n</code></pre>"},{"location":"week-2/#c-optional-install-seaborn-for-nicer-plots","title":"C. (Optional) Install seaborn for nicer plots","text":"<p>With conda:</p> <pre><code>conda install seaborn -y\n</code></pre> <p>Or with pip:</p> <pre><code>pip install seaborn\npy -m pip install seaborn\n</code></pre>"},{"location":"week-2/#d-verify-installs-inside-pythonjupyter","title":"D. Verify installs inside Python/Jupyter","text":"<pre><code>import sklearn, matplotlib, seaborn\nprint(\"sklearn:\", sklearn.__version__)\nprint(\"matplotlib:\", matplotlib.__version__)\nprint(\"seaborn:\", seaborn.__version__)\n</code></pre>"},{"location":"week-2/#e-why-these-tools","title":"E. Why these tools?","text":"<ul> <li>scikit-learn \u2013 Core classic ML toolkit (linear/logistic regression, train/test split, GridSearchCV).  </li> <li>matplotlib \u2013 Base plotting library (residual plots, ROC curves, confusion matrices).  </li> <li>seaborn (optional) \u2013 Cleaner statistical charts (pairplots, heatmaps) built on matplotlib.  </li> <li>joblib (bundled with scikit-learn) \u2013 Quick model save/load.</li> </ul>"},{"location":"week-2/#f-troubleshooting","title":"F. Troubleshooting","text":"<pre><code># 'pip' not found\npython -m pip install --upgrade pip\n\n# 'conda' not found\n# You probably didn\u2019t install Anaconda or its PATH isn\u2019t set. Use pip instead.\n\n# Permission error on Windows\n# Open \"Anaconda Prompt\" (Start Menu) or run PowerShell as Administrator\n\n# Still stuck? Try upgrading tools\npip install --upgrade scikit-learn matplotlib seaborn\n</code></pre>"},{"location":"week-2/#5-step-by-step-exercises-10-total","title":"5 \u2014 Step-by-Step Exercises (10 total)","text":""},{"location":"week-2/#5-step-by-step-exercises","title":"5. Step by Step Exercises","text":"<p>Same template as Week\u202f1: Purpose \u2192 Concept \u2192 Real\u2011World \u2192 Steps \u2192 Expected Outcome \u2192 Extensions \u2192 Notes.</p>"},{"location":"week-2/#regression-exercises-14","title":"Regression (Exercises 1\u20134)","text":"Exercise 1: Simple Linear Regression (Synthetic Data) <p>1. Overview &amp; Purpose Fit a linear regression on generated data (<code>y = 3x + noise</code>) to see how the model recovers the relationship.</p> <p>2. Concept Reinforcement - Linear regression basics - Train/test split, MSE, R\u00b2</p> <p>3. Real\u2011World Relevance - Forecasting revenue vs. ad spend - KPI prediction</p> <p>4. Step-by-Step Instructions <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nnp.random.seed(0)\nX = np.random.rand(100, 1) * 10\ny = 3 * X.squeeze() + np.random.randn(100) * 2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmodel = LinearRegression().fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"MSE:\", mse)\nprint(\"R\u00b2 :\", r2)\nprint(\"Coef:\", model.coef_, \"Intercept:\", model.intercept_)\n</code></pre></p> <p>5. Expected Outcomes &amp; Interpretation - Slope \u2248 3; high R\u00b2.  </p> <p>6. Extensions &amp; Variations - Increase noise - Add polynomial features</p> <p>7. Notes - Always keep a test set to gauge generalization.</p> Exercise 2: Visualizing Fit &amp; Residuals <p>Purpose: See where the model makes errors. Concepts: Residual plots reveal structure; random scatter = good.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Uses X_test, y_test, y_pred, model from Exercise 1\nplt.scatter(X_test, y_test, label='Actual')\nplt.scatter(X_test, y_pred, label='Predicted')\nplt.plot(sorted(X_test[:,0]), model.predict(np.sort(X_test, axis=0)), color='red', label='Line')\nplt.legend(); plt.title(\"Actual vs. Predicted\")\nplt.show()\n\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Residual\")\nplt.title(\"Residual Plot\")\nplt.show()\n</code></pre> <p>Expected: Residuals roughly centered around 0 with no clear pattern. Extensions: Histogram of residuals; log-transform y if needed.</p> Exercise 3: Multiple Linear Regression with pandas <p>Purpose: Use several features to predict a target. Concepts: Multivariate regression, MAE.</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\ndata = {\n    'size': [1400,1600,1700,1875,1100,1550,2350,2450,1425,1700],\n    'bedrooms': [3,3,3,2,2,3,4,4,3,3],\n    'age': [20,15,18,12,30,15,7,5,24,18],\n    'price': [245000,312000,279000,308000,199000,219000,405000,324000,319000,255000]\n}\ndf = pd.DataFrame(data)\n\nX = df[['size','bedrooms','age']]\ny = df['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlr = LinearRegression().fit(X_train, y_train)\npreds = lr.predict(X_test)\nmae = mean_absolute_error(y_test, preds)\n\nprint(\"Coefficients:\", lr.coef_)\nprint(\"Intercept:\", lr.intercept_)\nprint(\"MAE:\", mae)\n</code></pre> <p>Expected: Reasonable MAE; coefficients show feature influence. Extensions: Standardize features; add interaction terms. Notes: Watch multicollinearity.</p> Exercise 4: Polynomial Regression &amp; Overfitting Demo <p>Purpose: Show how higher-degree polynomials can overfit. Concepts: Bias\u2013variance trade\u2011off.</p> <pre><code>import numpy as np, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nnp.random.seed(0)\nX = np.linspace(0, 1, 20).reshape(-1,1)\ny = np.sin(2*np.pi*X).ravel() + np.random.randn(20)*0.2\n\nfor degree in [1, 3, 9]:\n    poly = PolynomialFeatures(degree)\n    X_poly = poly.fit_transform(X)\n    model = LinearRegression().fit(X_poly, y)\n    y_pred = model.predict(X_poly)\n    mse = mean_squared_error(y, y_pred)\n    plt.scatter(X, y, label='data' if degree==1 else None)\n    plt.plot(X, y_pred, label=f'deg {degree} (MSE={mse:.2f})')\nplt.legend(); plt.show()\n</code></pre> <p>Expected: deg=1 underfits, deg=9 overfits. Extensions: Add train/test split; try Ridge/Lasso. Notes: Visual demos make concepts stick.</p>"},{"location":"week-2/#classification-exercises-57","title":"Classification (Exercises 5\u20137)","text":"Exercise 5: Logistic Regression on a Toy Dataset <p>Purpose: Fit logistic regression and read metrics. Concepts: Classification, probabilities, classification_report.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nX, y = make_classification(n_samples=500, n_features=4, n_informative=2, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nclf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\npreds = clf.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, preds))\nprint(classification_report(y_test, preds))\n</code></pre> <p>Expected: Accuracy ~0.8\u20130.9; precision/recall shown. Extensions: Change threshold; class weights. Notes: Increase <code>max_iter</code> if convergence warning.</p> Exercise 6: Confusion Matrix &amp; ROC Curve <p>Purpose: Visualize errors and threshold performance. Concepts: Confusion matrix, ROC, AUC.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n\ndisp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\nplt.title(\"Confusion Matrix\"); plt.show()\n\ny_prob = clf.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.2f})\")\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\"); plt.legend(); plt.show()\n</code></pre> <p>Expected: Curved ROC; AUC &gt; 0.5. Extensions: Precision\u2013Recall curve for imbalance. Notes: Use multiple metrics.</p> Exercise 7: Class Imbalance &amp; Threshold Tuning <p>Purpose: Show why accuracy fails with rare positives. Concepts: Threshold tuning, precision/recall trade\u2011off.</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\n\nX, y = make_classification(n_samples=1000, n_features=5, weights=[0.95, 0.05], random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nclf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\ny_probs = clf.predict_proba(X_test)[:,1]\n\nfor thresh in [0.5, 0.3, 0.1]:\n    preds = (y_probs &gt;= thresh).astype(int)\n    p, r, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary', zero_division=0)\n    print(f\"Threshold {thresh}: precision={p:.2f}, recall={r:.2f}, f1={f1:.2f}\")\n</code></pre> <p>Expected: Lower threshold \u2192 higher recall, lower precision. Extensions: Class weights, PR curves. Notes: Choose threshold by business cost.</p>"},{"location":"week-2/#workflow-tuning-exercises-810","title":"Workflow &amp; Tuning (Exercises 8\u201310)","text":"Exercise 8: Cross-Validation vs. Single Split <p>Purpose: Compare metrics stability. Concepts: KFold, variance of scores.</p> <pre><code>from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Reuse X, y from Exercise 1 or make new\nmodel = LinearRegression()\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\nscores = cross_val_score(model, X, y, scoring='r2', cv=kf)\n\nprint(\"R\u00b2 scores:\", scores)\nprint(\"Mean R\u00b2:\", np.mean(scores), \"Std:\", np.std(scores))\n</code></pre> <p>Expected: Slight variation across folds. Extensions: cross_val_predict. Notes: Vital for limited data.</p> Exercise 9: Hyperparameter Tuning with GridSearchCV <p>Purpose: Systematically find best hyperparameters. Concepts: Grid search, scoring.</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nparams = {'alpha': [0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\ngrid.fit(X, y)\n\nprint(\"Best params:\", grid.best_params_)\nprint(\"Best score (MSE):\", -grid.best_score_)\n</code></pre> <p>Expected: One alpha gives lowest MSE. Extensions: RandomizedSearchCV for speed. Notes: Don\u2019t overfit to validation by repeated peeking.</p> Exercise 10: Mini End\u2011to\u2011End Project <p>Purpose: Practice full pipeline on a real CSV. Concepts: Clean \u2192 Split \u2192 Train \u2192 Evaluate \u2192 Save.</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\ndf = pd.read_csv(\"your_data.csv\")  # replace with a real dataset\nX = df.drop(columns=['target'])\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmodel = LinearRegression().fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))\n\njoblib.dump(model, \"model.joblib\")\n</code></pre> <p>Expected: Working model + <code>model.joblib</code> saved. Extensions: Use Pipeline/ColumnTransformer. Notes: Document steps for your capstone.</p> <p>You can now\u2026</p> <ul> <li>Build baseline models for regression and classification with scikit\u2011learn.  </li> <li>Choose appropriate metrics and interpret them in context (business costs, imbalance).  </li> <li>Diagnose over/underfitting and apply simple fixes (regularization, cross\u2011validation).  </li> <li>Run a full ML workflow end\u2011to\u2011end on small datasets.</li> </ul>"},{"location":"week-2/#6-week-2-summary-what-you-can-now-do","title":"6. Week 2 Summary &amp; What You Can Now Do","text":""},{"location":"week-2/#a-core-takeaways","title":"A. Core Takeaways","text":"<ul> <li>Supervised learning = labeled data, mapping X\u2192y.  </li> <li>Linear regression \u2192 numeric predictions; logistic regression \u2192 categorical predictions via probabilities.  </li> <li>Metrics matter: pick ones aligned with the problem (e.g., F1 for imbalance).  </li> <li>Generalization beats memorization.</li> </ul>"},{"location":"week-2/#b-practical-wins","title":"B. Practical Wins","text":"<ul> <li>You used GridSearchCV, cross\u2011validation, and plotted diagnostics.  </li> <li>You can prototype business ML problems quickly.</li> </ul>"},{"location":"week-2/#c-next-week-prep","title":"C. Next Week Prep","text":"<ul> <li>Skim Week\u202f3 (k\u2011means, PCA) to see unsupervised learning.  </li> <li>Get comfortable with pandas &amp; plotting\u2014visualizations increase.</li> </ul>"},{"location":"week-2/#7-additional-resources","title":"7. Additional Resources","text":"<p>Guides &amp; Tutorials</p> <ul> <li>scikit\u2011learn User Guide \u2013 Supervised Learning https://scikit-learn.org/stable/supervised_learning.html</li> <li>StatQuest (YouTube) \u2013 Excellent clear videos on regression, classification, metrics https://www.youtube.com/user/joshstarmer</li> </ul> <p>Metrics &amp; Evaluation</p> <ul> <li>\u201cPrecision, Recall and F1 Score for Dummies\u201d (various blog guides)  </li> <li>ROC &amp; AUC explainers (blog posts, Coursera ML course notes)</li> </ul> <p>Books &amp; Chapters</p> <ul> <li>An Introduction to Statistical Learning (ISLR) \u2013 Free PDF, chapters on regression &amp; classification https://www.statlearning.com/</li> </ul> <p>Cheat Sheets</p> <ul> <li>scikit\u2011learn, matplotlib, seaborn cheat sheets (quick Google finds)</li> </ul>"},{"location":"week-3/","title":"Week 3 Lesson Plan \u2013 Unsupervised Learning: Clustering &amp; Dimensionality Reduction","text":""},{"location":"week-3/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives By the end of this week you will be able to:</p> <ul> <li>Explain the difference between supervised and unsupervised learning and when to use each.  </li> <li>Describe and implement k\u2011means and hierarchical clustering, and evaluate clusters (elbow method, silhouette score).  </li> <li>Perform Principal Component Analysis (PCA) to reduce dimensionality and visualize high\u2011dimensional data.  </li> <li>Use scikit\u2011learn to build unsupervised pipelines and interpret results for business use cases (e.g., customer segmentation).  </li> <li>Communicate findings with appropriate plots (scatter matrices, dendrograms, scree plots).</li> </ul>"},{"location":"week-3/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition Simple Example Unsupervised Learning ML that finds structure/patterns in unlabeled data. Grouping customers by behavior without knowing \u201ctrue\u201d segments. Clustering Grouping similar data points together. Clustering products by sales patterns. k\u2011means Algorithm that partitions data into k clusters by minimizing within\u2011cluster variance. Choosing 3 customer segments by spending habits. Hierarchical Clustering Builds a tree of clusters (merging or splitting). Dendrogram showing how customers merge into bigger groups. PCA (Principal Component Analysis) Linear technique that projects data into fewer dimensions while preserving as much variance as possible. Reducing 30 questionnaire answers to 3 principal factors. Silhouette Score Measure of how well a point fits in its cluster vs. others (\u22121 to 1). Average silhouette ~0.6 suggests good separation. Elbow Method Heuristic plot of inertia vs. k to pick an optimal number of clusters. \u201cElbow\u201d at k=4 means adding more clusters gives diminishing returns."},{"location":"week-3/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-3/#a-kmeans-clustering-deep-dive","title":"A. k\u2011Means Clustering (Deep Dive)","text":"<p>Intuition (Grade\u201110 friendly): Imagine you have a bunch of dots on paper. You pick k \u201ccenters\u201d and pull nearby dots toward each center. Then you move each center to the middle of its dots. Repeat until things stop moving much.</p> <p>Formal Steps: 1. Choose k (number of clusters). 2. Initialize k centroids (randomly or with k\u2011means++). 3. Assignment step: Assign each point to the closest centroid (Euclidean distance). 4. Update step: Recompute each centroid as the mean of assigned points. 5. Repeat 3\u20134 until assignments stop changing or max iterations hit.</p> <p>Cost Function (Inertia): Minimize the sum of squared distances between each point and its centroid.</p> <p>Why It Matters: Fast, simple, works well for spherical clusters. Great first pass for segmentation.</p> <p>Limitations: - You must pick k (not always obvious). - Sensitive to outliers and scaling. - Assumes roughly equal-sized, spherical clusters.</p>"},{"location":"week-3/#b-hierarchical-clustering","title":"B. Hierarchical Clustering","text":"<p>Intuition: Start with each point as its own cluster. Gradually merge the two closest clusters until only one remains. The history of merges is a dendrogram (tree diagram).</p> <p>Two main flavors: - Agglomerative (bottom\u2011up): Start with singletons, merge upwards. - Divisive (top\u2011down): Start with one big cluster, split apart.</p> <p>Linkage Criteria: Defines \u201cdistance\u201d between clusters: - Single linkage: closest pair of points. - Complete linkage: farthest pair. - Average linkage: average distance between all pairs. - Ward\u2019s method: minimizes increase in total within-cluster variance.</p> <p>Why It Matters: - No need to pre-specify k. You can \u201ccut\u201d the dendrogram later. - Reveals nested structure.</p> <p>Limitations: - Slower than k\u2011means on very large datasets. - Choice of linkage can change results noticeably.</p>"},{"location":"week-3/#c-principal-component-analysis-pca","title":"C. Principal Component Analysis (PCA)","text":"<p>Intuition (simple): If you have lots of columns (features), many may overlap. PCA finds new axes (principal components) that capture the most \u201cspread\u201d (variance). You can keep the first few and ignore the rest.</p> <p>Step-by-Step (conceptual): 1. Standardize features (mean 0, std 1). 2. Compute covariance matrix of data. 3. Find eigenvalues &amp; eigenvectors (directions of maximum variance). 4. Sort eigenvectors by eigenvalues (largest first). 5. Project data onto the top d eigenvectors to reduce dimensions.</p> <p>Key Plots: - Scree plot: eigenvalue (variance explained) vs. component index. - Explained variance ratio: how much variance each component captures.</p> <p>Why It Matters: - Visualization: reduce to 2D/3D for scatter plots. - Noise reduction &amp; faster modeling. - Feature engineering (components as new features).</p> <p>Limitations: - Linear; may not capture complex nonlinear relationships. - Components are combinations of original features\u2014sometimes hard to interpret.</p>"},{"location":"week-3/#4-tools-installation-setup-week-3-specific","title":"4. Tools Installation &amp; Setup (Week\u202f3 Specific)","text":"<p>You already installed Python, Jupyter, NumPy, pandas, scikit\u2011learn, matplotlib in Weeks\u202f1\u20132.</p>"},{"location":"week-3/#a-optional-libraries-for-this-week","title":"A. Optional Libraries for This Week","text":"<ul> <li>scipy (for hierarchical clustering &amp; dendrograms) \u2013 https://scipy.org/install/ </li> <li>seaborn (nice stats plots) \u2013 https://seaborn.pydata.org/installing.html </li> <li>yellowbrick (cluster visualization helpers) \u2013 https://www.scikit-yb.org/en/latest/install.html</li> </ul>"},{"location":"week-3/#b-install-choose-conda-or-pip","title":"B. Install (choose conda or pip)","text":"<pre><code># conda\nconda install scipy seaborn -y\nconda install -c conda-forge yellowbrick -y\n</code></pre> <pre><code># pip\npip install scipy seaborn yellowbrick\n# or\npy -m pip install scipy seaborn yellowbrick\n</code></pre>"},{"location":"week-3/#c-verify-in-pythonjupyter","title":"C. Verify in Python/Jupyter","text":"<pre><code>import scipy, seaborn, yellowbrick\nprint(\"scipy:\", scipy.__version__)\nprint(\"seaborn:\", seaborn.__version__)\nprint(\"yellowbrick:\", yellowbrick.__version__)\n</code></pre>"},{"location":"week-3/#5-step-by-step-exercises-10-total","title":"5. Step-by-Step Exercises (10 total)","text":"<p>Follow the same pattern as Week\u202f1 exercises: overview/purpose \u2192 concept reinforcement \u2192 real-world relevance \u2192 step-by-step code \u2192 expected outcomes \u2192 extensions \u2192 notes.</p>"},{"location":"week-3/#exercise-1-kmeans-on-a-toy-2d-dataset-visual","title":"Exercise 1: k\u2011Means on a Toy 2D Dataset (Visual)","text":"<p>Overview &amp; Purpose Run k\u2011means on simple 2D points and plot clusters to see the algorithm visually.</p> <p>Concept Reinforcement - k\u2011means assignment/update loop - Euclidean distance, centroids - Need for feature scaling</p> <p>Real World Relevance Customer segmentation by 2 numeric features (e.g., spend &amp; visits).</p> <p>Step-by-Step Instructions</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Create toy data\nnp.random.seed(42)\nX1 = np.random.normal([2, 2], 0.3, size=(50, 2))\nX2 = np.random.normal([6, 6], 0.4, size=(50, 2))\nX3 = np.random.normal([2, 7], 0.5, size=(50, 2))\nX = np.vstack([X1, X2, X3])\n\n# 2. Scale (optional but recommended)\nscaler = StandardScaler()\nXs = scaler.fit_transform(X)\n\n# 3. Fit KMeans\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(Xs)\n\n# 4. Plot\nplt.scatter(Xs[:,0], Xs[:,1], c=labels, cmap='viridis', s=30)\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],\n            c='red', marker='x', s=100, label='centroids')\nplt.legend(); plt.title(\"k-means Clusters (scaled data)\")\nplt.show()\n</code></pre> <p>Expected Outcomes &amp; Interpretation Three groups colored differently, centroids marked. Check if they match where you generated data.</p> <p>Extensions - Try different seeds or cluster counts. - Remove scaling and compare results.  </p> <p>Notes <code>random_state</code> for reproducibility. Scaling often critical.</p>"},{"location":"week-3/#exercise-2-elbow-method-to-choose-k","title":"Exercise 2: Elbow Method to Choose k","text":"<p>Purpose: Plot inertia vs. k.</p> <pre><code>inertias = []\nfor k in range(1, 10):\n    km = KMeans(n_clusters=k, random_state=42).fit(Xs)\n    inertias.append(km.inertia_)\n\nplt.plot(range(1,10), inertias, marker='o')\nplt.xlabel(\"k\"); plt.ylabel(\"Inertia (Sum of Squared Distances)\")\nplt.title(\"Elbow Method\")\nplt.show()\n</code></pre> <p>Expected Outcome Look for the \u201celbow\u201d (sharp bend). That k is a good starting point.</p>"},{"location":"week-3/#exercise-3-silhouette-score-evaluation","title":"Exercise 3: Silhouette Score Evaluation","text":"<p>Purpose: Quantify cluster quality.</p> <pre><code>from sklearn.metrics import silhouette_score\n\nfor k in range(2, 7):\n    km = KMeans(n_clusters=k, random_state=42).fit(Xs)\n    score = silhouette_score(Xs, km.labels_)\n    print(f\"k={k}, silhouette={score:.3f}\")\n</code></pre> <p>Outcome: Higher average silhouette indicates better separation.</p>"},{"location":"week-3/#exercise-4-hierarchical-clustering-dendrogram","title":"Exercise 4: Hierarchical Clustering &amp; Dendrogram","text":"<p>Purpose: Visualize merges with scipy.</p> <pre><code>from scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n\nZ = linkage(Xs, method='ward')  # or 'single', 'complete', 'average'\nplt.figure(figsize=(8,4))\ndendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45., leaf_font_size=10.)\nplt.title(\"Hierarchical Clustering Dendrogram (truncated)\")\nplt.xlabel(\"Cluster size\"); plt.ylabel(\"Distance\")\nplt.show()\n</code></pre> <p>Outcome: Tree plot; cut at a distance to choose clusters.</p>"},{"location":"week-3/#exercise-5-agglomerativeclustering-in-scikitlearn","title":"Exercise 5: AgglomerativeClustering in scikit\u2011learn","text":"<p>Purpose: Fit hierarchical clustering and compare to k-means.</p> <pre><code>from sklearn.cluster import AgglomerativeClustering\n\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels_agg = agg.fit_predict(Xs)\n\nplt.scatter(Xs[:,0], Xs[:,1], c=labels_agg, cmap='viridis', s=30)\nplt.title(\"Agglomerative Clustering (Ward)\")\nplt.show()\n</code></pre> <p>Outcome: Similar but not identical groupings vs. k-means.</p>"},{"location":"week-3/#exercise-6-pca-on-a-4feature-dataset","title":"Exercise 6: PCA on a 4\u2011Feature Dataset","text":"<p>Purpose: Reduce 4D to 2D and visualize.</p> <pre><code>from sklearn.decomposition import PCA\n\n# Make a small 4D toy set\nnp.random.seed(0)\nX4 = np.random.randn(200, 4) @ np.array([[1, 0.2, 0.1, 0],\n                                         [0.2, 1, 0.3, 0.1],\n                                         [0.1, 0.3, 1, 0.2],\n                                         [0, 0.1, 0.2, 1]])\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X4)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n\nplt.scatter(X_pca[:,0], X_pca[:,1], s=20)\nplt.title(\"PCA Projection (2D)\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.show()\n</code></pre> <p>Outcome: 2D scatter and variance ratios.</p>"},{"location":"week-3/#exercise-7-scree-plot-explained-variance","title":"Exercise 7: Scree Plot (Explained Variance)","text":"<pre><code>pca_full = PCA().fit(X4)\nplt.plot(np.arange(1, X4.shape[1]+1), np.cumsum(pca_full.explained_variance_ratio_), marker='o')\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.title(\"Scree / Cumulative Variance Plot\")\nplt.grid(True)\nplt.show()\n</code></pre> <p>Outcome: Choose number of components capturing, say, 90% variance.</p>"},{"location":"week-3/#exercise-8-reconstruct-data-from-first-pcs","title":"Exercise 8: Reconstruct Data from First PCs","text":"<p>Purpose: Show information loss.</p> <pre><code>k = 2  # keep first 2 components\npca_k = PCA(n_components=k).fit(X4)\nX_red = pca_k.transform(X4)\nX_recon = pca_k.inverse_transform(X_red)\n\nmse = np.mean((X4 - X_recon)**2)\nprint(\"Reconstruction MSE:\", mse)\n</code></pre> <p>Outcome: Lower MSE if more components kept.</p>"},{"location":"week-3/#exercise-9-combine-pca-kmeans-pipeline","title":"Exercise 9: Combine PCA + k\u2011Means Pipeline","text":"<pre><code>from sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"pca\", PCA(n_components=2)),\n    (\"kmeans\", KMeans(n_clusters=3, random_state=42))\n])\npipe.fit(X4)\nprint(\"Clusters:\", np.unique(pipe['kmeans'].labels_))\n</code></pre> <p>Outcome: Clean pipeline workflow (preprocessing + model).</p>"},{"location":"week-3/#exercise-10-business-miniproject-customer-segmentation","title":"Exercise 10: Business Mini\u2011Project \u2013 Customer Segmentation","text":"<p>Purpose: Apply to a CSV of customer metrics (spend, visits, tenure, channel usage).</p> <ol> <li>Load &amp; clean data. </li> <li>Scale numeric features. </li> <li>Try k from 2\u20138, elbow + silhouette. </li> <li>Pick best k, interpret clusters (means of features). </li> <li>Visualize with PCA 2D scatter colored by cluster. </li> <li>Write 3\u20134 bullet \u201cpersonas\u201d describing each segment.</li> </ol> <p>Skeleton:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n\ndf = pd.read_csv(\"customers.csv\")  # replace with your file\nfeatures = [\"annual_spend\", \"visits\", \"tenure_months\", \"returns\"]\nX = df[features].dropna()\n\nX_scaled = StandardScaler().fit_transform(X)\n\nscores = {}\nfor k in range(2,9):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n    scores[k] = silhouette_score(X_scaled, km.labels_)\nprint(scores)\n\nbest_k = max(scores, key=scores.get)\nkm = KMeans(n_clusters=best_k, random_state=42).fit(X_scaled)\ndf[\"segment\"] = km.labels_\n\n# PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:,0], X_pca[:,1], c=km.labels_, cmap='tab10', s=20)\nplt.title(f\"PCA projection colored by k-means (k={best_k})\")\nplt.show()\n\nprint(df.groupby(\"segment\")[features].mean())\n</code></pre> <p>Outcome: Table of segment means + plots.</p> <p>Extensions - Try hierarchical clustering on same data. - Use different features or include categorical encodings.</p>"},{"location":"week-3/#6-summary-of-week-3","title":"6. Summary of Week 3","text":"<p>This week you:</p> <ul> <li>Differentiated unsupervised from supervised learning and saw where it fits (e.g., exploration, segmentation).  </li> <li>Implemented k\u2011means: understood its objective (minimizing within-cluster variance), the need for scaling, and how to pick k with elbow/silhouette.  </li> <li>Explored hierarchical clustering and read dendrograms to decide cluster cuts without predefining k.  </li> <li>Performed PCA to compress high-dimensional data, interpreted explained variance, and visualized in 2D.  </li> <li>Built pipelines combining preprocessing, dimensionality reduction, and clustering for cleaner code.  </li> <li>Applied everything to a business-like customer dataset, turning raw numbers into actionable segments (personas).</li> </ul> <p>These skills set you up for Week\u202f4\u2019s neural networks by cementing your understanding of data structure, feature scaling, and evaluation without labels.</p>"},{"location":"week-3/#7-additional-resources","title":"7. Additional Resources","text":"<ul> <li>scikit-learn Clustering Guide: https://scikit-learn.org/stable/modules/clustering.html </li> <li>scikit-learn PCA docs: https://scikit-learn.org/stable/modules/decomposition.html#pca </li> <li>\u201cA Tutorial on Clustering Algorithms\u201d (GitHub repo): https://github.com/annjose/Clustering </li> <li>StatQuest: PCA &amp; k\u2011means videos (YouTube) \u2013 clear step-by-step intuition.  </li> <li>3Blue1Brown \u2013 \u201cBut what is a neural network?\u201d (preview for Week\u202f4 thinking about high-dimensional spaces).</li> </ul>"},{"location":"week-4/","title":"Week 4 Lesson Plan \u2013 Neural Network Basics","text":""},{"location":"week-4/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives</p> <p>By the end of this week you will be able to:</p> <ul> <li>Explain how a single neuron (perceptron) works and why activation functions are needed.  </li> <li>Build and train a multilayer perceptron (MLP) for regression and classification using both NumPy (from scratch) and a high-level library (TensorFlow/Keras or PyTorch).  </li> <li>Describe and implement gradient descent and backpropagation conceptually and in code.  </li> <li>Use common regularization techniques (dropout, weight decay) and basic callbacks (EarlyStopping).  </li> <li>Visualize and debug training with learning curves and (optionally) TensorBoard.</li> </ul>"},{"location":"week-4/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition Simple Example Neuron (Perceptron) A function that computes ( z = \\mathbf{w}\\cdot\\mathbf{x} + b ) then passes it through an activation function. Predicting \u201cspam vs not spam\u201d from word counts. Activation Function Nonlinear function applied to neuron output (e.g., ReLU, sigmoid). ReLU keeps positives, zeroes negatives. Feedforward Network (MLP) Layers of neurons where outputs of one layer feed the next. Input (pixels) \u2192 Hidden \u2192 Output (digit class). Gradient Descent Iterative method to minimize loss by moving weights opposite the gradient. Adjust regression weights to reduce MSE. Backpropagation Efficient algorithm to compute gradients of all weights using the chain rule. Update all network layers after one forward pass. Loss Function Quantifies error between prediction and target. Cross-entropy for classification, MSE for regression. Regularization Techniques to prevent overfitting (dropout, L2). Randomly \u201cdrop\u201d 20% of neurons each step. Epoch / Batch / Iteration Epoch: one pass over training set. Batch: subset used for one update. Iteration: one batch update. 10,000 samples, batch size 100 \u2192 100 iterations per epoch."},{"location":"week-4/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-4/#a-the-perceptron-activation-functions","title":"A. The Perceptron &amp; Activation Functions","text":"<p>Intuition (Grade\u201110 friendly): A neuron is like a weighted \u201cdecision calculator.\u201d Each input gets multiplied by a weight, all added up, plus a bias. If the total is big enough, the neuron \u201cfires.\u201d Without a nonlinear activation, stacking many neurons is just doing one big linear equation\u2014boring! Activations add curves and bends so networks can learn complex patterns.</p> <p>Common Activations:</p> Name Formula Range Pros Cons Sigmoid ( \\sigma(z)=1/(1+e^{-z}) ) (0,1) Probabilities, smooth Vanishing gradients for large |z| Tanh ( \\tanh(z) ) (\u22121,1) Zero-centered Still can vanish ReLU ( \\max(0,z) ) [0,\u221e) Fast, simple \u201cDead\u201d neurons (negative forever) Leaky ReLU ( \\max(0.01z,z) ) (\u2212\u221e,\u221e) Fixes dead ReLU Slightly more compute <p>Why This Matters: - Picking the right activation affects learning speed &amp; accuracy. - Output layer activation ties to loss: sigmoid + BCE for binary, softmax + cross entropy for multi-class.</p>"},{"location":"week-4/#b-gradient-descent-backpropagation","title":"B. Gradient Descent &amp; Backpropagation","text":"<p>Story Version: You\u2019re on a dark hillside (loss surface) trying to find the lowest point. You feel the slope (gradient) and take a step downhill. Repeat until the ground is mostly flat.</p> <p>Formal Idea: - Loss ( L(\\mathbf{w}) ). - Update rule: ( \\mathbf{w}{t+1} = \\mathbf{w}_t - \\eta \\nabla{\\mathbf{w}} L ). - Backprop uses chain rule to compute all partial derivatives efficiently.</p> <p>Mini Example (1 neuron, MSE): 1. Forward: ( \\hat{y} = \\sigma(\\mathbf{w}\\cdot\\mathbf{x}+b) ) 2. Loss: ( L = \\frac{1}{2}(\\hat{y}-y)^2 ) 3. Compute ( \\partial L/\\partial w_i = (\\hat{y}-y)\\cdot\\sigma'(z)\\cdot x_i )</p> <p>Why It Matters: Understanding the math demystifies neural nets and helps you debug exploding/vanishing gradients.</p>"},{"location":"week-4/#c-regularization-training-tricks","title":"C. Regularization &amp; Training Tricks","text":"<ul> <li>Overfitting: Training loss \u2193 but validation loss \u2191.  </li> <li>Dropout: Randomly set some activations to 0 during training.  </li> <li>L2 Weight Decay: Add ( \\lambda |\\mathbf{w}|^2 ) to the loss to keep weights small.  </li> <li>EarlyStopping: Stop when validation doesn\u2019t improve for N epochs.  </li> <li>Learning Rate Schedules: Reduce (\\eta) over time.</li> </ul> <p>Why It Matters: Models generalize better, saving you from false confidence.</p>"},{"location":"week-4/#4-tools-installation-setup-week-4-specific","title":"4. Tools Installation &amp; Setup (Week\u202f4 Specific)","text":"<p>You already have NumPy/pandas/matplotlib/scikit-learn. Now we add a deep learning framework.</p>"},{"location":"week-4/#a-pick-one-framework-or-try-both","title":"A. Pick One Framework (or try both)","text":"<ul> <li>TensorFlow/Keras (high-level, batteries included)   Download: https://www.tensorflow.org/install </li> <li>PyTorch (popular in research, very pythonic)   Download: https://pytorch.org/get-started/locally/</li> </ul>"},{"location":"week-4/#b-install-commands","title":"B. Install Commands","text":"<p>TensorFlow (CPU-only):</p> <pre><code># conda\nconda install -c conda-forge tensorflow -y\n\n# or pip\npip install tensorflow\n</code></pre> <p>PyTorch (CPU-only example; site gives exact command for your OS/Python):</p> <pre><code># conda (example)\nconda install pytorch torchvision torchaudio cpuonly -c pytorch -y\n\n# or pip\npip install torch torchvision torchaudio\n</code></pre> <p>Optional tools</p> <pre><code>pip install tensorboard  # for TF\npip install torchinfo    # model summaries in PyTorch\n</code></pre>"},{"location":"week-4/#c-verify","title":"C. Verify","text":"<pre><code>import tensorflow as tf\nprint(\"TF:\", tf.__version__)\n\nimport torch\nprint(\"Torch:\", torch.__version__)\n</code></pre> <p>Why We Need These Tools</p> <ul> <li>Frameworks handle automatic differentiation (backprop), GPU support, layers, optimizers\u2014so you don\u2019t reinvent the wheel.  </li> <li>TensorBoard helps visualize loss curves, graphs.  </li> <li>torchinfo / model.summary() quickly show network architecture.</li> </ul>"},{"location":"week-4/#5-step-by-step-exercises-10-total","title":"5. Step-by-Step Exercises (10 total)","text":""},{"location":"week-4/#exercise-1-single-neuron-in-numpy-binary-classification","title":"Exercise 1: Single Neuron in NumPy (Binary Classification)","text":"<p>Purpose: Understand forward pass &amp; loss manually.</p> <pre><code>import numpy as np\n\n# Data: OR gate\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\ny = np.array([0,1,1,1])\n\n# Weights &amp; bias\nw = np.random.randn(2)\nb = 0.0\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n# Forward\nz = X.dot(w) + b\ny_hat = sigmoid(z)\nloss = np.mean((y_hat - y)**2)\nprint(\"Predictions:\", y_hat.round(3))\nprint(\"Loss:\", loss)\n</code></pre> <p>Extensions: Manually tweak w,b to see loss change.</p>"},{"location":"week-4/#exercise-2-gradient-descent-by-hand-numpy","title":"Exercise 2: Gradient Descent by Hand (NumPy)","text":"<pre><code>lr = 0.1\nfor epoch in range(5000):\n    z = X.dot(w) + b\n    y_hat = sigmoid(z)\n    loss = np.mean((y_hat - y)**2)\n\n    # grads\n    dL_dy = 2*(y_hat - y)/len(y)\n    dy_dz = y_hat*(1-y_hat)\n    dz_dw = X\n    dz_db = 1\n\n    grad_w = dz_dw.T.dot(dL_dy*dy_dz)\n    grad_b = np.sum(dL_dy*dy_dz*dz_db)\n\n    w -= lr*grad_w\n    b -= lr*grad_b\n\nprint(\"Final loss:\", loss)\nprint(\"w,b:\", w, b)\n</code></pre> <p>Outcome: Loss decreases; weights converge.</p>"},{"location":"week-4/#exercise-3-build-a-tiny-mlp-in-numpy-2-2-1","title":"Exercise 3: Build a Tiny MLP in NumPy (2-2-1)","text":"<pre><code>np.random.seed(0)\nW1 = np.random.randn(2,2)\nb1 = np.zeros(2)\nW2 = np.random.randn(2,1)\nb2 = np.zeros(1)\n\ndef relu(x): return np.maximum(0,x)\n\nfor epoch in range(2000):\n    # forward\n    h = relu(X.dot(W1)+b1)\n    y_hat = sigmoid(h.dot(W2)+b2)\n    loss = np.mean((y_hat - y.reshape(-1,1))**2)\n\n    # backprop\n    dL_dy = 2*(y_hat - y.reshape(-1,1))/len(y)\n    dy_dz2 = y_hat*(1-y_hat)\n    dL_dz2 = dL_dy * dy_dz2\n    dL_dW2 = h.T.dot(dL_dz2)\n    dL_db2 = dL_dz2.sum(axis=0)\n\n    dh = dL_dz2.dot(W2.T)\n    dz1 = dh*(h&gt;0)  # relu grad\n    dL_dW1 = X.T.dot(dz1)\n    dL_db1 = dz1.sum(axis=0)\n\n    # update\n    lr = 0.1\n    W1 -= lr*dL_dW1\n    b1 -= lr*dL_db1\n    W2 -= lr*dL_dW2\n    b2 -= lr*dL_db2\n\nprint(\"Final Loss:\", loss)\nprint(\"Predictions:\", (y_hat&gt;0.5).astype(int).flatten())\n</code></pre>"},{"location":"week-4/#exercise-4-mlp-with-keras-tensorflow","title":"Exercise 4: MLP with Keras (TensorFlow)","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Dense(8, activation='relu', input_shape=(2,)),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=200, verbose=0)\n\nprint(\"Final acc:\", history.history['accuracy'][-1])\n\n# Plot learning curve (optional)\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n</code></pre>"},{"location":"week-4/#exercise-5-same-network-in-pytorch","title":"Exercise 5: Same Network in PyTorch","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nX_t = torch.tensor(X, dtype=torch.float32)\ny_t = torch.tensor(y.reshape(-1,1), dtype=torch.float32)\n\nmodel = nn.Sequential(\n    nn.Linear(2,8),\n    nn.ReLU(),\n    nn.Linear(8,1),\n    nn.Sigmoid()\n)\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(500):\n    optimizer.zero_grad()\n    y_hat = model(X_t)\n    loss = criterion(y_hat, y_t)\n    loss.backward()\n    optimizer.step()\n\nprint(\"Final loss:\", loss.item())\nprint(\"Pred:\", (y_hat.detach().numpy()&gt;0.5).astype(int).flatten())\n</code></pre>"},{"location":"week-4/#exercise-6-overfitting-demo-train-vs-validation","title":"Exercise 6: Overfitting Demo (Train vs. Validation)","text":"<p>Use a small noisy dataset:</p> <pre><code>import numpy as np\nnp.random.seed(1)\nXn = np.linspace(-1,1,200).reshape(-1,1)\ny_true = Xn**3\ny_noise = y_true + 0.2*np.random.randn(*y_true.shape)\n\n# Train/val split\nX_train, X_val = Xn[:150], Xn[150:]\ny_train, y_val = y_noise[:150], y_noise[150:]\n\nimport tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(optimizer='adam', loss='mse')\nhist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                 epochs=200, verbose=0)\n\nimport matplotlib.pyplot as plt\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.legend(); plt.title(\"Overfitting Demo\")\nplt.show()\n</code></pre>"},{"location":"week-4/#exercise-7-dropout-weight-decay","title":"Exercise 7: Dropout &amp; Weight Decay","text":"<p>Compare models with and without regularization.</p> <pre><code>from tensorflow.keras import regularizers\n\nmodel_reg = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,),\n                          kernel_regularizer=regularizers.l2(1e-3)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation='relu',\n                          kernel_regularizer=regularizers.l2(1e-3)),\n    tf.keras.layers.Dense(1)\n])\n\nmodel_reg.compile(optimizer='adam', loss='mse')\nhist2 = model_reg.fit(X_train, y_train, validation_data=(X_val, y_val),\n                      epochs=200, verbose=0)\n</code></pre> <p>Plot both curves to show improvement.</p>"},{"location":"week-4/#exercise-8-visualize-with-tensorboard-optional","title":"Exercise 8: Visualize with TensorBoard (optional)","text":"<pre><code># In terminal\ntensorboard --logdir logs --port 6006\n</code></pre> <pre><code>tb_cb = tf.keras.callbacks.TensorBoard(log_dir=\"logs/run1\", histogram_freq=1)\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train, y_train, epochs=50, callbacks=[tb_cb], verbose=0)\n</code></pre> <p>Open http://127.0.0.1:6006 to view.</p>"},{"location":"week-4/#exercise-9-learning-rate-experiment","title":"Exercise 9: Learning Rate Experiment","text":"<p>Train the same model with LR = 0.1, 0.01, 0.0001 and compare speed/stability.</p> <pre><code>for lr in [0.1, 0.01, 0.0001]:\n    m = tf.keras.Sequential([layers.Dense(16, activation='relu', input_shape=(2,)),\n                             layers.Dense(1, activation='sigmoid')])\n    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n              loss='binary_crossentropy', metrics=['accuracy'])\n    h = m.fit(X, y, epochs=100, verbose=0)\n    print(lr, \"final acc:\", h.history['accuracy'][-1])\n</code></pre>"},{"location":"week-4/#exercise-10-mini-project-mnist-digit-classifier-lite-version","title":"Exercise 10: Mini Project \u2013 MNIST Digit Classifier (Lite Version)","text":"<ol> <li>Load MNIST (digits 0\u20139).  </li> <li>Normalize pixels to [0,1].  </li> <li>Build a 2\u2011hidden\u2011layer MLP.  </li> <li>Train, plot accuracy.  </li> <li>Show 10 random predictions.</li> </ol> <pre><code>import tensorflow as tf\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.reshape(-1,784)/255.0\nX_test  = X_test.reshape(-1,784)/255.0\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhist = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\nprint(\"Test acc:\", model.evaluate(X_test, y_test, verbose=0)[1])\n</code></pre>"},{"location":"week-4/#6-summary-of-week-4","title":"6. Summary of Week 4","text":"<p>This week you:</p> <ul> <li>Broke down the neuron: weighted sum + activation \u2192 building block of deep nets.  </li> <li>Learned why nonlinearity (activations) makes deep learning powerful.  </li> <li>Understood and coded gradient descent/backprop, both conceptually and with NumPy.  </li> <li>Implemented real models in TensorFlow/Keras or PyTorch, saw how frameworks automate differentiation and training loops.  </li> <li>Practiced regularization (dropout, L2) and monitored training (learning curves, TensorBoard).  </li> <li>Built a tiny digit classifier\u2014your first \u201creal\u201d deep learning model.</li> </ul> <p>These fundamentals unlock everything coming next: CNNs, RNNs, transformers, and more advanced architectures.</p>"},{"location":"week-4/#7-additional-resources","title":"7. Additional Resources","text":"<ul> <li>Goodfellow, Bengio, Courville \u2013 Deep Learning, Ch. 6\u20137 (Neural Nets &amp; Deep Feedforward).  </li> <li>TensorFlow Keras Tutorials: https://www.tensorflow.org/tutorials </li> <li>PyTorch \u201c60\u2011Minute Blitz\u201d: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html </li> <li>3Blue1Brown \u2013 \u201cWhat is backpropagation really doing?\u201d (YouTube)  </li> <li>CS231n Notes (Stanford): sections on backprop &amp; optimization.  </li> </ul>"},{"location":"week-5/","title":"Week 5 Lesson Plan \u2013 Ethics, Bias &amp; Responsible AI","text":""},{"location":"week-5/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives</p> <p>By the end of this week you will be able to:</p> <ul> <li>Identify common sources of bias in data, labeling, and algorithms.  </li> <li>Calculate and interpret key fairness metrics (e.g., demographic parity, equal opportunity).  </li> <li>Understand core privacy &amp; compliance concepts (consent, minimization, GDPR/PIPEDA basics).  </li> <li>Apply responsible AI frameworks (checklists, governance processes) to your own projects.  </li> <li>Use Python tools to measure and mitigate bias in a toy dataset.</li> </ul>"},{"location":"week-5/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition Simple Example Algorithmic Bias Systematic and repeatable errors that create unfair outcomes. Loan model approves 80% of group A but 55% of group B. Demographic Parity Selection/positive rate should be similar across groups. % approved loans for men \u2248 % for women. Equal Opportunity True positive rate should be equal across groups. Cancer detection model should catch positives equally well for all races. Fairness Metric Quantitative measure of disparity (e.g., difference in TPR). TPR_A \u2212 TPR_B \u2264 0.05 target. Data Minimization Collect only what is necessary for the task. Don\u2019t store SSNs if you only need age. Explainability Ability to interpret model decisions. SHAP values showing which features drove a prediction. Responsible AI Governance Policies, processes, and tools ensuring ethical deployment. Review board, model cards, audit logs."},{"location":"week-5/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-5/#a-where-bias-creeps-in-and-real-examples","title":"A. Where Bias Creeps In (and Real Examples)","text":"<ol> <li> <p>Data Collection Bias \u2013 Your dataset under-represents a subgroup. Example: Feedback forms mostly from tech-savvy users \u2192 product skews to their needs.</p> </li> <li> <p>Labeling Bias \u2013 Human annotators carry their own prejudices. Example: Historical hiring decisions labeled \u201cgood candidate\u201d reflect past discrimination.</p> </li> <li> <p>Measurement / Proxy Bias \u2013 Using a proxy variable that encodes sensitive info. Example: Zip code as proxy for race or income.</p> </li> <li> <p>Algorithm/Optimization Bias \u2013 Loss functions ignore fairness constraints. Example: Optimizing only for accuracy favors majority class.</p> </li> <li> <p>Deployment &amp; Feedback Loops \u2013 Model outputs change behavior, which changes inputs. Example: Police patrol more in predicted \u201chot spots\u201d \u2192 more arrests \u2192 model \u201cproves\u201d itself.</p> </li> </ol> <p>Why This Matters: You can\u2019t fix what you can\u2019t see. Knowing the phases of an ML pipeline helps you inject checks at the right places.</p>"},{"location":"week-5/#b-fairness-metrics-intuition-formula","title":"B. Fairness Metrics (Intuition \u2192 Formula)","text":"<p>1. Demographic Parity (DP): Positive prediction rate should be equal. [ \\text{DP\\ diff} = |P(\\hat{Y}=1 \\mid A=0) - P(\\hat{Y}=1 \\mid A=1)| ]</p> <p>2. Equal Opportunity (EOpp): True positive rates equal across groups. [ \\text{EOpp\\ diff} = |TPR_{A=0} - TPR_{A=1}| ]</p> <p>3. Equalized Odds: Both TPR and FPR equal across groups.</p> <p>4. Predictive Parity / Calibration: Given a score, actual outcome probability is the same for each group.</p> <p>Trade-offs: You usually can\u2019t satisfy all metrics simultaneously; pick what matches the product\u2019s ethical goals and legal context.</p>"},{"location":"week-5/#c-privacy-consent-and-regulation-high-level","title":"C. Privacy, Consent, and Regulation (High Level)","text":"<ul> <li>Privacy Principles: consent, purpose limitation, data minimization, security, accountability.  </li> <li>De-identification &amp; Anonymization: remove direct identifiers; but beware re-identification via linkage.  </li> <li>Basic Regs to Know (Canada/EU/US): </li> <li>PIPEDA (Canada) \u2013 personal info rules for private sector.  </li> <li>GDPR (EU) \u2013 rights to access, erase, data portability.  </li> <li>Sector-specific (HIPAA, COPPA, etc.) if you handle health/children\u2019s data.</li> </ul> <p>Focus for You: Build habits\u2014log data sources, justify fields collected, document retention &amp; deletion plans.</p>"},{"location":"week-5/#d-responsible-ai-frameworks-governance","title":"D. Responsible AI Frameworks &amp; Governance","text":"<ul> <li>Checklists &amp; Model Cards: Document purpose, data, limitations, bias tests, maintenance plan.  </li> <li>NIST AI RMF / OECD Principles: Risk management, transparency, accountability.  </li> <li>Human-in-the-loop: Allow overrides, appeals, escalation paths.  </li> <li>Monitoring in Prod: Drift detection, fairness checks on new data, incident response.</li> </ul> <p>Why This Matters: Stakeholders (investors, customers, regulators) increasingly demand proof you\u2019re managing AI responsibly.</p>"},{"location":"week-5/#4-tools-installation-setup-week-5-specific","title":"4. Tools Installation &amp; Setup (Week\u202f5 Specific)","text":"<p>You already have NumPy, pandas, matplotlib. This week we\u2019ll add a fairness library and a quick explainability tool.</p>"},{"location":"week-5/#a-install-fairness-explainability-packages","title":"A. Install Fairness &amp; Explainability Packages","text":"<p>Fairlearn (fairness metrics &amp; mitigation) + SHAP (explainability):</p> <pre><code># conda (recommended)\nconda install -c conda-forge fairlearn shap -y\n\n# or pip\npip install fairlearn shap\n</code></pre>"},{"location":"week-5/#5-step-by-step-exercises-10","title":"5. Step-by-Step Exercises (10)","text":""},{"location":"week-5/#exercise-1-quick-bias-scan-with-pandas","title":"Exercise 1: Quick Bias Scan with <code>pandas</code>","text":"<p>Overview &amp; Purpose Check basic demographic parity by grouping outcomes by a sensitive attribute.</p> <p>Concept Reinforcement - Demographic parity (selection rate) - <code>groupby</code> aggregation in pandas</p> <p>Real\u2011World Relevance - Loan approvals by gender/race - Hiring pass rates across departments</p> <p>Step-by-Step Instructions</p> <pre><code>import pandas as pd\n\ndata = {\n    \"score\":    [0.9, 0.2, 0.7, 0.85, 0.3, 0.6, 0.1, 0.95],\n    \"approved\": [1,   0,   1,   1,    0,   1,   0,   1   ],\n    \"gender\":   [\"M\", \"F\", \"M\", \"M\",  \"F\", \"F\", \"F\", \"M\" ]\n}\ndf = pd.DataFrame(data)\n\nrates = df.groupby(\"gender\")[\"approved\"].mean()\nprint(rates)\n\ndp_diff = abs(rates[\"M\"] - rates[\"F\"])\nprint(\"Demographic parity diff:\", dp_diff)\n</code></pre> <p>Expected Outcomes &amp; Interpretation - Two approval rates (e.g., M ~0.80, F ~0.50). - <code>dp_diff</code> shows the absolute disparity (e.g., 0.30).</p> <p>Extensions &amp; Variations - Add more groups (age buckets, regions). - Plot a bar chart of approval rates.</p> <p>Notes &amp; Tips - Ensure your sensitive attribute names are consistent (<code>\"gender\"</code>, <code>\"race\"</code>, etc.). - Use <code>.value_counts(normalize=True)</code> to get proportions quickly.</p>"},{"location":"week-5/#exercise-2-equal-opportunity-from-confusion-matrices","title":"Exercise 2: Equal Opportunity from Confusion Matrices","text":"<p>Overview &amp; Purpose Manually compute TPR (true positive rate) by group and compare.</p> <p>Concept Reinforcement - Equal opportunity metric - Confusion matrix interpretation</p> <p>Real\u2011World Relevance - Medical diagnosis rates across demographics - Fraud detection recall differences</p> <p>Step-by-Step Instructions</p> <pre><code>import numpy as np\n\n# Confusion matrices: rows = actual (0/1), cols = predicted (0/1)\n# group A (e.g., M)\ncm_A = np.array([[50, 10],\n                 [ 5, 35]])  # TN, FP / FN, TP\n\n# group B (e.g., F)\ncm_B = np.array([[48, 12],\n                 [12, 28]])\n\nTPR_A = cm_A[1,1] / (cm_A[1,0] + cm_A[1,1])\nTPR_B = cm_B[1,1] / (cm_B[1,0] + cm_B[1,1])\n\nprint(\"TPR_A:\", TPR_A, \"TPR_B:\", TPR_B, \"Diff:\", abs(TPR_A - TPR_B))\n</code></pre> <p>Expected Outcomes &amp; Interpretation - You\u2019ll see each group\u2019s TPR and the disparity (diff). - Large diffs (&gt;0.05) could flag fairness concerns.</p> <p>Extensions &amp; Variations - Add FPR comparison for Equalized Odds. - Visualize with a grouped bar chart.</p> <p>Notes &amp; Tips - Double-check TN/FP/FN/TP ordering\u2014easy to mix up!</p>"},{"location":"week-5/#exercise-3-fairness-metrics-with-fairlearnmetricframe","title":"Exercise 3: Fairness Metrics with <code>fairlearn.MetricFrame</code>","text":"<p>Overview &amp; Purpose Use Fairlearn to compute multiple metrics per group in one shot.</p> <p>Concept Reinforcement - MetricFrame abstraction - Selection rate, TPR, FPR</p> <p>Real\u2011World Relevance - Faster audits in production pipelines - Compliance reporting dashboards</p> <p>Step-by-Step Instructions</p> <pre><code>from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate\ny_true = [1,0,1,1,0,1,0,1]\ny_pred = [1,0,1,0,0,1,0,1]\ngender  = [\"M\",\"F\",\"M\",\"M\",\"F\",\"F\",\"F\",\"M\"]\n\nmf = MetricFrame(\n    metrics={\n        \"selection_rate\": selection_rate,\n        \"tpr\": true_positive_rate\n    },\n    y_true=y_true,\n    y_pred=y_pred,\n    sensitive_features=gender\n)\n\nprint(mf.by_group)\nprint(\"TPR diff:\", mf.difference(method='between_groups')[\"tpr\"])\nprint(\"Selection rate diff:\", mf.difference(method='between_groups')[\"selection_rate\"])\n</code></pre> <p>Expected Outcomes &amp; Interpretation - Table of metrics per group. - Differences help identify unfairness.</p> <p>Extensions &amp; Variations - Add more metrics: <code>false_positive_rate</code>, <code>accuracy</code>. - Try <code>mf.group_min()</code> / <code>group_max()</code> to see bounds.</p> <p>Notes &amp; Tips - Fairlearn can also handle continuous scores (probabilities).</p>"},{"location":"week-5/#exercise-4-threshold-adjustment-mitigation","title":"Exercise 4: Threshold Adjustment Mitigation","text":"<p>Overview &amp; Purpose Show how different decision thresholds per group can reduce disparity.</p> <p>Concept Reinforcement - Post-processing mitigation - Trade-off between fairness and overall accuracy</p> <p>Real\u2011World Relevance - Lending thresholds per applicant segment - Medical triage thresholds by demographic</p> <p>Step-by-Step Instructions</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate\n\n# Synthetic data with slight bias\nnp.random.seed(0)\nX = np.random.rand(400, 3)\nA = np.random.choice([\"M\",\"F\"], size=400)\ny = (X[:,0]*0.7 + (A==\"M\")*0.2 + np.random.randn(400)*0.1 &gt; 0.5).astype(int)\n\nX_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n    X, y, A, test_size=0.3, random_state=42\n)\n\nclf = LogisticRegression().fit(X_train, y_train)\nprobs = clf.predict_proba(X_test)[:,1]\n\n# Single threshold (baseline)\ny_pred_base = (probs &gt; 0.5).astype(int)\n\n# Group-specific thresholds (try to equalize selection rate)\nthr_M, thr_F = 0.55, 0.45\ny_pred_adj = np.where(A_test==\"M\", probs&gt;thr_M, probs&gt;thr_F).astype(int)\n\n# Compare\nmetrics = {\"selection_rate\": selection_rate, \"tpr\": true_positive_rate}\nmf_base = MetricFrame(metrics, y_test, y_pred_base, sensitive_features=A_test)\nmf_adj  = MetricFrame(metrics, y_test, y_pred_adj,  sensitive_features=A_test)\n\nprint(\"BASELINE\\n\", mf_base.by_group)\nprint(\"ADJUSTED\\n\", mf_adj.by_group)\n</code></pre> <p>Expected Outcomes &amp; Interpretation - Adjusted thresholds should narrow metric gaps but may lower overall accuracy. - Discuss trade-offs.</p> <p>Extensions &amp; Variations - Use Fairlearn\u2019s <code>ThresholdOptimizer</code> to find thresholds automatically. - Try optimizing for EO instead of DP.</p> <p>Notes &amp; Tips - Document any manual thresholding in model cards.</p>"},{"location":"week-5/#exercise-5-shap-for-feature-influence-proxy-detection","title":"Exercise 5: SHAP for Feature Influence (Proxy Detection)","text":"<p>Overview &amp; Purpose Use SHAP to see if a seemingly innocuous feature proxies a sensitive attribute.</p> <p>Concept Reinforcement - Explainability for fairness - Feature attribution</p> <p>Real\u2011World Relevance - Spotting \u201czip code \u2192 race\u201d proxies in credit scoring - Detecting age proxies in hiring models</p> <p>Step-by-Step Instructions</p> <pre><code>import shap\nshap.initjs()\n\nexplainer = shap.LinearExplainer(clf, X_train)\nshap_values = explainer.shap_values(X_test[:50])\n\n# Summary plot (opens in Jupyter notebook)\nshap.summary_plot(shap_values, X_test[:50], feature_names=[\"f1\",\"f2\",\"f3\"])\n</code></pre> <p>Expected Outcomes &amp; Interpretation - See which features drive predictions. - Compare distribution of feature values by group.</p> <p>Extensions &amp; Variations - Try <code>shap.dependence_plot</code> for single features. - Use <code>TreeExplainer</code> for tree models.</p> <p>Notes &amp; Tips - SHAP can be slow on big models; sample data.</p>"},{"location":"week-5/#exercise-6-red-team-prompting-for-llm-safety","title":"Exercise 6: Red-Team Prompting for LLM Safety","text":"<p>Overview &amp; Purpose Stress-test a chatbot or LLM prompt to find unethical/unsafe outputs.</p> <p>Concept Reinforcement - Safety testing / adversarial prompting - Policy compliance</p> <p>Real\u2011World Relevance - Ensure customer support bots don\u2019t give legal/medical advice without disclaimers - Prevent harmful outputs (hate speech, PII leaks)</p> <p>Step-by-Step Instructions 1. Write 5\u201310 \u201ctricky\u201d prompts (e.g., \u201cHow can I evade taxes?\u201d). 2. Record model responses. 3. Label each as Acceptable / Needs Warning / Reject. 4. Propose guardrails (prompt constraints, filters). 5. (Optional) Implement regex/keyword filters or model moderation endpoints.</p> <p>Expected Outcomes &amp; Interpretation - A list of failure modes and mitigations.</p> <p>Extensions &amp; Variations - Use OpenAI moderation API or similar tools. - Create a \u201cprompt safety checklist.\u201d</p> <p>Notes &amp; Tips - Keep logs; iterate regularly as prompts evolve.</p>"},{"location":"week-5/#exercise-7-mini-model-card","title":"Exercise 7: Mini Model Card","text":"<p>Overview &amp; Purpose Create a concise \u201cmodel card\u201d for one of your previous models (e.g., Week 4 MLP).</p> <p>Concept Reinforcement - Documentation &amp; transparency - Stakeholder communication</p> <p>Real\u2011World Relevance - Internal/external audits - Customer trust</p> <p>Step-by-Step Instructions 1. Template sections:    - Model Details: type, version, date.    - Intended Use / Out-of-Scope Use.    - Data: source, preprocessing, known issues.    - Metrics: overall + by subgroup (if available).    - Ethical Considerations &amp; Limitations.    - Maintenance / Update Plan. 2. Write 1\u20132 pages in Markdown (<code>model-card.md</code>). 3. Store in repo for traceability.</p> <p>Expected Outcomes &amp; Interpretation - Clear, repeatable doc for future models.</p> <p>Extensions &amp; Variations - Use Google\u2019s Model Card toolkit format. - Add visual metric tables.</p> <p>Notes &amp; Tips - Update the card when the model changes.</p>"},{"location":"week-5/#exercise-8-privacy-policy-mapping-checklist","title":"Exercise 8: Privacy &amp; Policy Mapping Checklist","text":"<p>Overview &amp; Purpose Map a SaaS feature\u2019s data flow to privacy and fairness controls.</p> <p>Concept Reinforcement - Data minimization - Governance alignment</p> <p>Real\u2011World Relevance - PIPEDA/GDPR readiness - Due diligence for M&amp;A</p> <p>Step-by-Step Instructions 1. List all personal data fields you collect. 2. For each: purpose, retention time, who can access. 3. Identify fairness checks pre/post deployment. 4. Note missing policies/processes. 5. Summarize in a checklist doc (<code>privacy-fairness-checklist.md</code>).</p> <p>Expected Outcomes &amp; Interpretation - Gap analysis for compliance. - Action plan items.</p> <p>Extensions &amp; Variations - Map to NIST AI RMF categories. - Add incident response flowchart.</p> <p>Notes &amp; Tips - Revisit quarterly; regulations change.</p>"},{"location":"week-5/#exercise-9-reweighting-mitigation-with-fairlearn-reductions","title":"Exercise 9: Reweighting Mitigation with Fairlearn Reductions","text":"<p>Overview &amp; Purpose Apply algorithmic mitigation (reweighting) using Fairlearn\u2019s reductions API.</p> <p>Concept Reinforcement - Pre-/in-processing mitigation - Constraint-based optimization</p> <p>Real\u2011World Relevance - Achieving fairness goals without fully redesigning model - Regulatory evidence</p> <p>Step-by-Step Instructions</p> <pre><code>from fairlearn.reductions import ExponentiatedGradient, DemographicParity\nfrom sklearn.tree import DecisionTreeClassifier\n\nestimator   = DecisionTreeClassifier(max_depth=3, random_state=0)\nconstraint  = DemographicParity()\nmitigator   = ExponentiatedGradient(estimator=estimator, constraints=constraint)\n\nmitigator.fit(X_train, y_train, sensitive_features=A_train)\ny_pred_mitigated = mitigator.predict(X_test)\n</code></pre> <p>Compute fairness/accuracy metrics before vs. after (reuse MetricFrame).</p> <p>Expected Outcomes &amp; Interpretation - Reduced DP diff, possibly lower accuracy. - Discuss acceptable trade-offs.</p> <p>Extensions &amp; Variations - Try <code>EqualizedOdds()</code> constraint. - Compare different base estimators.</p> <p>Notes &amp; Tips - Reductions can be slow; start with small datasets.</p>"},{"location":"week-5/#exercise-10-build-a-responsible-ai-checklist-for-your-company","title":"Exercise 10: Build a Responsible AI Checklist for Your Company","text":"<p>Overview &amp; Purpose Translate week\u2019s concepts into an actionable internal policy.</p> <p>Concept Reinforcement - Governance operationalization - Continuous monitoring</p> <p>Real\u2011World Relevance - CRO Software &amp; Micro Manage Software policies - Investor/board confidence</p> <p>Step-by-Step Instructions 1. Sections to include:    - Data Intake Questions (Why do we need this? How long keep it?)    - Bias &amp; Fairness Checks (Which metrics? How often?)    - Privacy Compliance Steps (Consent, retention, deletion).    - Human Review &amp; Overrides (Escalation path).    - Monitoring &amp; Incident Response (Who watches? What triggers alerts?). 2. Draft in Markdown (<code>responsible-ai-checklist.md</code>). 3. Review quarterly.</p> <p>Expected Outcomes &amp; Interpretation - A living document guiding responsible AI work.</p> <p>Extensions &amp; Variations - Add sign-off lines for teams (Data, Legal, Exec). - Link to model cards &amp; audit logs.</p> <p>Notes &amp; Tips - Keep it practical\u2014short enough people actually use it.</p>"},{"location":"week-5/#6-summary-of-week-5","title":"6. Summary of Week 5","text":"<p>This week you:</p> <ul> <li>Mapped the sources of bias (data, labels, algorithms, feedback loops) and learned where to intervene.  </li> <li>Quantified fairness using demographic parity, equal opportunity, and equalized odds\u2014understanding trade-offs.  </li> <li>Practiced mitigation techniques: threshold adjustment, reweighting via Fairlearn, and transparency through SHAP.  </li> <li>Integrated privacy principles (consent, minimization, retention) and learned the basics of major regulations (GDPR, PIPEDA).  </li> <li>Created governance artifacts: model cards, checklists, red-team prompts\u2014turning ethics into repeatable processes.</li> </ul> <p>You now have a repeatable workflow to ask: Is it fair? Is it explainable? Is it compliant?\u2014before shipping models.</p>"},{"location":"week-5/#7-additional-resources","title":"7. Additional Resources","text":"<ul> <li>Fairlearn Documentation: https://fairlearn.org </li> <li>SHAP Documentation: https://shap.readthedocs.io </li> <li>\u201cModel Cards for Model Reporting\u201d \u2013 Google AI (Mitchell et al.)  </li> <li>NIST AI Risk Management Framework (AI RMF) </li> <li>OECD AI Principles </li> <li>The Ethical Algorithm (Michael Kearns &amp; Aaron Roth)  </li> <li>Partnership on AI Resources: https://partnershiponai.org </li> </ul>"}]}