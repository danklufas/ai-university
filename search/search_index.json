{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI University","text":"<p>Welcome to the AI University Curriculum. Use the left menu to pick a week.</p>"},{"location":"cheat-sheet/","title":"Cheat Sheets","text":"<p>Quick Reminder Cheat Sheets</p>"},{"location":"cheat-sheet/#1-everyday-workflow-edit-preview-commit-deploy","title":"1. Everyday Workflow (Edit \u2192 Preview \u2192 Commit \u2192 Deploy)","text":"<p>Local preview</p> <p><pre><code>python -m mkdocs serve\n</code></pre> Open: http://127.0.0.1:8000/ (Leave terminal running; Ctrl+C stops it.)</p> <p>Commit &amp; push <pre><code>git add docs/...\ngit commit -m \"Your message\"\ngit push origin main\n</code></pre> Publish to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#2-common-git-commands","title":"2. Common Git Commands","text":"<pre><code>git status                 # see what changed\ngit add &lt;file&gt;             # stage a file\ngit add .                  # stage everything\ngit commit -m \"message\"    # commit staged changes\ngit push origin main       # push to GitHub\ngit pull origin main       # pull latest from GitHub\n</code></pre> <p>Undo a staged file: <pre><code>git restore --staged &lt;file&gt;\n</code></pre></p>"},{"location":"cheat-sheet/#3-mkdocs-basics","title":"3. MkDocs Basics","text":"<p>Edit pages in <code>docs/</code> Navigation lives in <code>mkdocs.yml</code> under <code>nav:</code></p> <p>Rebuild local site <pre><code>python -m mkdocs serve\n</code></pre></p> <p>Deploy to GitHub Pages <pre><code>python -m mkdocs gh-deploy\n</code></pre></p>"},{"location":"cheat-sheet/#add-a-new-page","title":"Add a new page","text":"<ol> <li>Create <code>docs/new-page.md</code></li> <li>Add to <code>mkdocs.yml</code>: <pre><code>nav:\n  - Home: index.md\n  - Week 1: week-1.md\n  - Week 2: week-2.md\n  - Cheat Sheets: cheat-sheet.md\n  - New Page: new-page.md\n  ```\n\n## 4. VS Code Tips\nToggle terminal: **Ctrl + `**\nCommand Palette: **Ctrl + Shift + P**\nFind/Replace regex: **Ctrl + H**, click `.*`\nReload the window (fix UI glitches): **Developer: Reload Window**\n\n  ## 5. Troubleshooting\n```bash\npython -m mkdocs serve\n</code></pre></li> </ol> <p>Use <code>http://127.0.0.1:8000/</code> (colon, not dot).</p>"},{"location":"cheat-sheet/#github-pages-not-updated","title":"GitHub Pages not updated","text":"<pre><code>python -m mkdocs gh-deploy\n</code></pre> <p>Wait ~30 seconds and refresh the live site.</p>"},{"location":"cheat-sheet/#site-shows-up-in-git-changes","title":"<code>site/</code> shows up in Git changes","text":"<p>Ensure <code>.gitignore</code> contains <code>site/</code>.</p> <p>If it's already tracked: <pre><code>git rm -r --cached site\ngit commit -m \"Remove generated site\"\ngit push origin main\n</code></pre></p>"},{"location":"week-1/","title":"History of AI &amp; Math Foundations","text":""},{"location":"week-1/#1-lesson-overview","title":"1. Lesson Overview","text":"<p>Learning Objectives</p> <p>By the end of this lesson, you will be able to:</p> <ul> <li>Describe three pivotal AI milestones and their lasting impact.</li> <li>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</li> <li>Understand probability fundamentals\u2014random variables, expectation, variance\u2014and compute them by hand and in Python.</li> <li>Grasp key linear algebra concepts\u2014vectors, matrices, dot products, matrix multiplication\u2014and see how they underpin AI algorithms.</li> <li>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</li> </ul>"},{"location":"week-1/#2-core-definitions","title":"2. Core Definitions","text":"Term Definition &amp; Citation Example Artificial Intelligence (AI) \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs.\u201d \u2014 John McCarthy, 1956 A chatbot that interprets questions and crafts human\u2011like responses. Machine Learning (ML) Algorithms that improve performance on tasks by learning from data rather than explicit programming. A regression model that learns to predict steel prices from historical sales. Data Science Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data. Cleaning and visualizing e\u2011commerce logs to uncover purchasing trends."},{"location":"week-1/#3-concept-sections","title":"3. Concept Sections","text":""},{"location":"week-1/#a-ai-milestones","title":"A. AI Milestones","text":"The Dartmouth Workshop (1956) <p>What happened: In summer 1956, John\u202fMcCarthy, Marvin\u202fMinsky, Nathaniel\u202fRochester, and Claude\u202fShannon met at Dartmouth College to ask: \u201cCan machines simulate human intelligence?\u201d They coined \u201cArtificial Intelligence\u201d and proposed studying how machines might \u201clearn from experience,\u201d \u201cmake abstractions,\u201d and \u201cuse language.\u201d</p> <p>Context &amp; significance: - Pre\u20111956, computers = number crunchers. Dartmouth reframed them as potential thinking machines. - Sparked optimism (and funding) that small teams could crack \u201cevery aspect of learning.\u201d</p> <p>First programs:     - Logic Theorist (1955) \u2013 Newell &amp; Simon proved logic theorems with a program.     - General Problem Solver (1957) \u2013 Early universal reasoning attempt.</p> <pre><code>!!! note \"Why this still matters\"\n    - Understanding the **hype \u2192 disappointment \u2192 AI winters** cycle helps you stay realistic about today\u2019s claims.  \n    - Symbolic reasoning/search ideas from this era live on in **knowledge graphs** and **constraint solvers**.\n</code></pre> Expert Systems Era (1970s\u20131980s) <p>Core idea: Encode expert knowledge as IF\u2013THEN rules.</p> <pre><code>IF symptom = fever AND symptom = rash\nTHEN suggest = measles\n</code></pre> <p>MYCIN (1972\u20131980): - ~600 rules to diagnose bacterial infections &amp; suggest antibiotics - Matched/surpassed human experts in blind tests</p> <p>Strengths vs. limits: - \u2705 Transparent logic (traceable to specific rules) - \u274c Hard to scale (thousands of hand\u2011written rules), weak with uncertainty</p> <p>Modern relevance</p> <ul> <li>Rule\u2011based logic still used in finance/healthcare compliance.  </li> <li>Today\u2019s hybrid systems: rules for regulation + ML models for scoring.</li> </ul> Deep Learning Boom (2010s\u2013Present) <p>Key breakthrough \u2013 AlexNet (2012): - 8\u2011layer CNN, cut ImageNet error rate in half (1.2M images, 1,000 classes) - Used ReLU, dropout, and GPU training.</p> <p>Why deep learning emerged: 1. Data: Huge labeled datasets (images, text, speech) 2. Compute: GPUs = fast parallel matrix ops 3. Algorithms: Batch norm, better backprop, new architectures</p> <p>Transformative apps: - Computer vision: self\u2011driving cars, medical imaging - NLP: translation, GPT\u2011style generation - Speech: voice assistants, real\u2011time translation</p> <p>Why this matters for you</p> <ul> <li>Modern frameworks (TensorFlow, PyTorch) are built around neural nets.  </li> <li>Explains why later terms focus on coding deep models &amp; leveraging pretrained architectures quickly.</li> </ul>"},{"location":"week-1/#c-probability-basics","title":"C. Probability Basics","text":"<p>Definition</p> <p>Probability Theory \u2013 \u201cThe mathematical framework for quantifying uncertainty and modeling random phenomena.\u201d</p>"},{"location":"week-1/#c1-gentle-introduction","title":"C1. Gentle Introduction","text":"1. What is Chance? <p>Analogy: Flipping a coin\u2014two outcomes, but you can\u2019t predict which. Key idea: Probability measures how likely something is (0 = impossible, 1 = certain). Example: A fair coin \u2192 P(heads) = 0.5.</p> 2. Simple Data &amp; Averages <p>Real example: Test scores: 80, 90, 70, 100, 60. Mean (average): <pre><code>(80 + 90 + 70 + 100 + 60) / 5 = 80\n</code></pre> Why it matters: The mean tells you what\u2019s \u201ctypical.\u201d</p> 3. Measuring Spread (Variance) <p>Analogy: Scores all near 80% \u2192 small spread; scores all over the place \u2192 big spread. Steps (using the score list above): 1. Subtract the mean (80): e.g. 60 \u2212 80 = \u221220 2. Square them: (\u221220)\u00b2 = 400 3. Average the squares \u2192 variance \u2248 280 4. Square root of variance \u2192 standard deviation \u2248 16.7 Why we care: Spread tells you how consistent or noisy data is\u2014critical for risk or quality control.</p>"},{"location":"week-1/#c2-formal-definitions-deep-dive","title":"C2. Formal Definitions &amp; Deep Dive","text":"1. Random Variables <p>A random variable (RV) assigns numbers to random outcomes.</p> <ul> <li>Discrete RV: countable values (die roll, number of returns)   Example: Fair die \u2192 <pre><code>X \u2208 {1,2,3,4,5,6},   P(X = k) = 1/6\n</code></pre></li> <li>Continuous RV: any value in a range (time between failures)   Example: Exponential distribution for time ( t \u2265 0 ): <pre><code>f(t) = \u03bb e^{\u2212\u03bb t}\n</code></pre></li> </ul> 2. Expectation (Mean) <p>Long\u2011run average outcome if you repeat forever.</p> <ul> <li>Discrete: <pre><code>E[X] = \u03a3 x_i \u00b7 P(X = x_i)\n</code></pre></li> <li>Continuous: <pre><code>E[X] = \u222b x f(x) dx\n</code></pre> Worked example (die): <pre><code>E[X] = (1+2+3+4+5+6) / 6 = 3.5\n</code></pre> Relevance: Loss functions (e.g., MSE) minimize expected error \u2192 expectation is baked into training.</li> </ul> 3. Variance &amp; Standard Deviation <p>Variance: average squared distance from the mean. <pre><code>Var(X) = E[(X \u2212 E[X])^2]\n</code></pre> Std. dev.: <pre><code>\u03c3 = \u221aVar(X)\n</code></pre> Die example: <pre><code>Var \u2248 2.92,  \u03c3 \u2248 1.71\n</code></pre> Why it matters: Tells you how uncertain predictions are, helps build confidence intervals, drives anomaly detection.</p> <p>Why Probability Matters in AI</p> <ul> <li>Model Training: Errors are expectations (means) over data.  </li> <li>Uncertainty: Variance underpins confidence, risk, anomaly flags.  </li> <li>Feature Engineering: Understanding distributions guides transformations (e.g., log scales for skewed data).</li> </ul>"},{"location":"week-1/#d-linear-algebra-basics","title":"D. Linear Algebra Basics","text":"<p>Definition</p> <p>Linear Algebra \u2013 \u201cThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.\u201d</p>"},{"location":"week-1/#d1-gentle-introduction","title":"D1. Gentle Introduction","text":"1. Vectors as Lists <p>Analogy: A grocery list: <code>[2 bananas, 1 loaf bread, 500 g cheese]</code> Key idea: A vector is just a list of numbers representing features.</p> 2. Matrices as Tables <p>Analogy: A seating chart (rows = tables, columns = seats): <pre><code>       S1  S2  S3\n    T1  A   B   C\n    T2  D   E   F\n</code></pre> Key idea: A matrix stacks many vectors into rows or columns.</p> 3. Dot Product Intuition <p>Example (bill splitting): - You &amp; a friend order appetizers <code>[3, 2]</code> and drinks <code>[1, 2]</code>. - Prices: appetizers = \\$5, drinks = \\$2 \u2192 <pre><code>[3, 2] \u00b7 [5, 2] = 3\u00d75 + 2\u00d72 = 19\n</code></pre> Why it matters: Same math as a simple regression prediction (weights \u00d7 features).</p> 4. Real\u2011World Matrix Uses <ul> <li>Recipe scaling: Multiply ingredient matrix by 1.5 to go from 4 to 6 servings.  </li> <li>School timetable: Days \u00d7 hours grid to schedule classes.</li> </ul>"},{"location":"week-1/#d2-formal-definitions-deep-dive","title":"D2. Formal Definitions &amp; Deep Dive","text":"1. Vectors &amp; Their Interpretation <p>A vector x \u2208 \u211d\u207f is an ordered list of n numbers (features). Example: <pre><code>x = [age, monthly_spend, num_orders] = [45, 320.5, 12]\n</code></pre></p> 2. Matrices &amp; Batch Operations <p>A matrix X \u2208 \u211d^{m\u00d7n} stacks m row\u2011vectors of length n. Example (customer table): <pre><code>X = [\n  [45, 320.5, 12],\n  [23, 150.0,  5],\n  ...\n]\n</code></pre></p> 3. Dot Product &amp; Linear Transformations <p>Dot product: <pre><code>a \u00b7 b = \u03a3 (a_i * b_i)\n</code></pre> Use in AI: - Regression:  \u0177 = w \u00b7 x + b - Neural nets:  z = w \u00b7 x + b, then apply activation (e.g., ReLU)</p> 4. Matrix Multiplication <p><pre><code>C = A \u00d7 B,   C_{ij} = \u03a3_k A_{ik} B_{kj}\n</code></pre> Example: Combine/transform features or chain neural network layers.</p> Why Linear Algebra Matters in AI <ul> <li>Speed: GPUs/NumPy rely on vectorized (matrix) ops for efficiency.  </li> <li>Model Insight: Weights, activations, attention maps are matrices/vectors.  </li> <li>Dimensionality Reduction: PCA, SVD use eigenvectors/values to compress data.</li> </ul>"},{"location":"week-1/#4-tools-installation-setup","title":"4. Tools Installation &amp; Setup","text":"<p>You\u2019ll do this once, then reuse the environment all term.</p>"},{"location":"week-1/#a-install-python-anaconda-windows-mac","title":"A. Install Python &amp; Anaconda (Windows &amp; Mac)","text":"<p><pre><code># Visit this in your browser:\nhttps://www.anaconda.com/products/distribution\n</code></pre> 1. Download the Python 3.x installer for your OS. 2. Run the installer, accept defaults. 3. Open Anaconda Navigator (Start Menu on Windows / Applications on Mac).</p>"},{"location":"week-1/#b-launch-jupyter-notebook","title":"B. Launch Jupyter Notebook","text":"<ol> <li>In Anaconda Navigator, click Launch under Jupyter Notebook.  </li> <li>A browser window opens showing your files.  </li> <li>Click New \u2192 Python 3.  </li> <li>Rename it to Week1_AI_Math.ipynb.</li> </ol>"},{"location":"week-1/#c-install-import-numpy-pandas","title":"C. Install &amp; Import NumPy &amp; pandas","text":"<p>In a notebook cell, run: <pre><code>!conda install numpy pandas -y\n</code></pre> Then import: <pre><code>import numpy as np\nimport pandas as pd\n</code></pre></p> <p>Why these tools?</p> <ul> <li>Python/Jupyter: interactive coding &amp; math demos  </li> <li>NumPy: fast vectors/matrices (used everywhere in ML)  </li> <li>pandas: quick data tables, cleaning, summaries</li> </ul> <ol> <li>Step-by-Step Exercises Exercise 1: Die Roll Simulation &amp; Statistics (Template) Exercise Overview &amp; Purpose</li> </ol> <p>What we\u2019re doing: Simulate 1,000 rolls of a fair six\u2011sided die in Python to compute the empirical mean and variance.</p> <p>Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p> <p>Concept Reinforcement</p> <p>Probability &amp; Random Variables</p> <p>Expectation &amp; Variance</p> <p>Sampling Variability</p> <p>Real World Relevance</p> <p>Quality Control (defect rate simulation)</p> <p>Risk Modeling (Monte Carlo portfolio variance)</p> <p>Randomized Algorithms &amp; Game Balancing</p> <p>Step by Step Instructions</p> <p>python Copy Edit import numpy as np</p>"},{"location":"week-1/#simulate-1000-die-rolls","title":"Simulate 1,000 die rolls","text":"<p>rolls = np.random.randint(1, 7, size=1000)</p>"},{"location":"week-1/#compute-statistics","title":"Compute statistics","text":"<p>mean_rolls = rolls.mean() var_rolls  = rolls.var()</p> <p>print(\"Simulated Mean:    \", mean_rolls) print(\"Simulated Variance:\", var_rolls) Notes:</p> <p>np.random.randint(1, 7, size=1000): integers 1\u20136</p> <p>.mean(), .var(): compute empirical mean &amp; variance</p> <p>Expected Outcomes &amp; Interpretation</p> <p>Mean \u2248\u202f3.5, Variance \u2248\u202f2.92 (\u00b1 sampling noise)</p> <p>Larger samples converge closer to theory</p> <p>Extensions &amp; Variations</p> <p>Vary sample size (100, 10,000)</p> <p>Simulate weighted/unfair die</p> <p>Plot histogram with matplotlib</p> <p>Additional Notes &amp; Tips</p> <p>Use np.random.seed(42) for reproducibility</p> <p>Avoid Python loops; prefer NumPy vectorization</p> <p>Exercise\u202f2: Coin Flip Probability Estimation Overview &amp; Purpose Simulate 10,000 coin flips to estimate the probability of heads and tails.</p> <p>Concept Reinforcement</p> <p>Discrete random variables</p> <p>Empirical vs. theoretical probability</p> <p>Real World Relevance</p> <p>A/B testing conversion rates (success/failure)</p> <p>Clinical trial outcomes</p> <p>Step by Step Instructions</p> <p>python Copy Edit import numpy as np</p> <p>np.random.seed(0) flips = np.random.choice(['H','T'], size=10000) p_heads = np.mean(flips == 'H') p_tails = np.mean(flips == 'T') print(f\"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\") Expected Outcomes &amp; Interpretation ~0.5 each, with fluctuations ~\u00b10.01.</p> <p>Extensions &amp; Variations</p> <p>Weighted coin (p=['H':0.3,'T':0.7])</p> <p>Plot bar chart of counts</p> <p>Additional Notes &amp; Tips Use np.random.seed(\u2026) for reproducibility.</p> <p>Exercise\u202f3: Histogram of Die Rolls Overview &amp; Purpose Visualize the distribution of the 1,000 die rolls from Exercise\u202f1.</p> <p>Concept Reinforcement</p> <p>Frequency vs. probability</p> <p>Data visualization basics</p> <p>Real World Relevance</p> <p>Sales distribution by category</p> <p>Error rates by batch</p> <p>Step by Step Instructions</p> <p>python Copy Edit import matplotlib.pyplot as plt</p> <p>plt.hist(rolls, bins=range(1,8), align='left', rwidth=0.8) plt.xlabel('Die Face') plt.ylabel('Frequency') plt.title('Histogram of 1,000 Die Rolls') plt.show() Expected Outcomes &amp; Interpretation Bars roughly equal height for faces\u202f1\u20136.</p> <p>Extensions &amp; Variations</p> <p>Normalized histogram (density=True)</p> <p>Overlay theoretical PMF</p> <p>Additional Notes &amp; Tips Ensure bins=range(1,8) to center on integer faces.</p> <p>Exercise\u202f4: Exponential Distribution Simulation Overview &amp; Purpose Simulate 5,000 samples from an exponential distribution (mean\u202f=\u202f2) and compute mean/variance.</p> <p>Concept Reinforcement</p> <p>Continuous random variables</p> <p>Relationship between distribution parameters and statistics</p> <p>Real World Relevance</p> <p>Time between machine failures</p> <p>Call-center interarrival times</p> <p>Step by Step Instructions</p> <p>python Copy Edit samples = np.random.exponential(scale=2, size=5000) print(\"Empirical Mean:\", samples.mean()) print(\"Empirical Variance:\", samples.var()) Expected Outcomes &amp; Interpretation Mean \u2248\u202f2; variance \u2248\u202f4.</p> <p>Extensions &amp; Variations</p> <p>Change scale (mean) parameter</p> <p>Plot histogram + theoretical PDF</p> <p>Additional Notes &amp; Tips Use np.histogram or matplotlib for PDF overlay.</p> <p>Exercise\u202f5: Normal Distribution Sampling Overview &amp; Purpose Draw 10,000 samples from a standard normal (mean\u202f0, \u03c3\u202f=\u202f1) and verify statistics.</p> <p>Concept Reinforcement</p> <p>Properties of Gaussian distribution</p> <p>Central Limit Theorem preview</p> <p>Real World Relevance</p> <p>Measurement errors</p> <p>Standardized test score modeling</p> <p>Step by Step Instructions</p> <p>python Copy Edit normals = np.random.randn(10000) print(\"Mean:\", normals.mean()) print(\"Variance:\", normals.var()) Expected Outcomes &amp; Interpretation Mean \u2248\u202f0, variance \u2248\u202f1.</p> <p>Extensions &amp; Variations</p> <p>Use np.random.normal(loc, scale, size)</p> <p>QQ plot vs. theoretical normal</p> <p>Additional Notes &amp; Tips Matplotlib\u2019s plt.hist(..., density=True) for PDF shape.</p> <p>Exercise\u202f6: Sampling Distribution of the Mean Overview &amp; Purpose Simulate 1,000 experiments, each of 100 die rolls, record sample means, and examine their distribution.</p> <p>Concept Reinforcement</p> <p>Law of Large Numbers</p> <p>Sampling variability reduction</p> <p>Real World Relevance</p> <p>Polling averages</p> <p>Quality metrics over batches</p> <p>Step by Step Instructions</p> <p>python Copy Edit means = [np.random.randint(1,7,100).mean() for _ in range(1000)] plt.hist(means, bins=20) plt.title('Sampling Distribution of Die Roll Means') plt.show() Expected Outcomes &amp; Interpretation Histogram approximates normal around 3.5 with narrower spread.</p> <p>Extensions &amp; Variations</p> <p>Vary experiment size (n=10, n=1000)</p> <p>Compute standard error (\u03c3/\u221an)</p> <p>Additional Notes &amp; Tips List comprehensions vs. loops for clarity.</p> <p>Exercise\u202f7: Weighted Dice Simulation Overview &amp; Purpose Simulate 1,000 rolls of a biased die with  \ud835\udc43 ( 6 ) = 0.5 P(6)=0.5, others equal.</p> <p>Concept Reinforcement</p> <p>Custom discrete distributions</p> <p>Impact of bias on mean/variance</p> <p>Real World Relevance</p> <p>Biased processes in manufacturing</p> <p>Skewed customer behavior models</p> <p>Step by Step Instructions</p> <p>python Copy Edit faces = [1,2,3,4,5,6] probs = [0.1]*5 + [0.5] rolls_biased = np.random.choice(faces, size=1000, p=probs) print(\"Mean:\", rolls_biased.mean(), \"Variance:\", rolls_biased.var()) Expected Outcomes &amp; Interpretation Mean &gt;\u202f3.5, variance different from fair die.</p> <p>Extensions &amp; Variations</p> <p>Tune probs for different biases</p> <p>Compare histograms side by side</p> <p>Additional Notes &amp; Tips Sum of probs must equal 1.</p> <p>Exercise\u202f8: Vector Addition &amp; Scaling Overview &amp; Purpose Demonstrate vector addition and scalar multiplication with feature vectors.</p> <p>Concept Reinforcement</p> <p>Vector space operations</p> <p>Geometric interpretation</p> <p>Real World Relevance</p> <p>Combining feature influences</p> <p>Scaling normalized data</p> <p>Step by Step Instructions</p> <p>python Copy Edit v1 = np.array([2, 4, 6]) v2 = np.array([1, 3, 5]) sum_v   = v1 + v2      # vector addition scaled_v = 0.5 * v1    # scalar multiplication print(\"Sum:\", sum_v) print(\"Scaled:\", scaled_v) Expected Outcomes &amp; Interpretation Sum = [3, 7, 11]; scaled = [1, 2, 3].</p> <p>Extensions &amp; Variations</p> <p>Compute dot product of sum_v and v2</p> <p>Visualize vectors in 2D/3D</p> <p>Additional Notes &amp; Tips Ensure consistent dimensions.</p> <p>Exercise\u202f9: Matrix Multiplication Demonstration Overview &amp; Purpose Multiply a 2\u00d73 matrix by a 3\u00d72 matrix to reinforce matrix multiplication rules.</p> <p>Concept Reinforcement</p> <p>Shape compatibility</p> <p>Summation over inner index</p> <p>Real World Relevance</p> <p>Transforming feature spaces</p> <p>Composition of network layers</p> <p>Step by Step Instructions</p> <p>python Copy Edit A = np.array([[1,2,3],[4,5,6]]) B = np.array([[7,8],[9,10],[11,12]]) C = A.dot(B) print(\"Result:\\n\", C) Expected Outcomes &amp; Interpretation C = [[58, 64], [139, 154]].</p> <p>Extensions &amp; Variations</p> <p>Reverse multiplication to show error</p> <p>Use @ operator in Python 3.5+</p> <p>Additional Notes &amp; Tips Check shapes via A.shape and B.shape.</p> <p>Exercise\u202f10: PCA on Toy Dataset Overview &amp; Purpose Perform PCA on a small 2\u2011D dataset to reduce to 1\u2011D and visualize variance capture.</p> <p>Concept Reinforcement</p> <p>Eigenvectors/eigenvalues</p> <p>Dimensionality reduction</p> <p>Real World Relevance</p> <p>Compressing image data</p> <p>Feature extraction for clustering</p> <p>Step by Step Instructions</p> <p>python Copy Edit from sklearn.decomposition import PCA import numpy as np</p> <p>X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0]]) pca = PCA(n_components=1) X_pca = pca.fit_transform(X) print(\"Explained variance ratio:\", pca.explained_variance_ratio_) print(\"Projected data:\\n\", X_pca) Expected Outcomes &amp; Interpretation Most variance captured in first component (\u224898%).</p> <p>Extensions &amp; Variations</p> <p>Plot original vs. projected points</p> <p>Try n_components=2</p> <p>Additional Notes &amp; Tips Requires scikit\u2011learn installation.</p> <ol> <li>Summary of Week\u202f1 Throughout this first week, you have:</li> </ol> <p>Traced AI History: From the Dartmouth Workshop to Expert Systems (MYCIN) and the Deep Learning revolution (AlexNet).</p> <p>Built Probability Skills: Learned random variables, expectation, variance\u2014both by hand and in Python (die rolls, coin flips, exponential and normal sampling).</p> <p>Visualized Data: Plotted histograms, demonstrated Central Limit Theorem.</p> <p>Handled Bias: Modeled a weighted die to see effects on distribution.</p> <p>Applied Linear Algebra: Performed vector ops, matrix multiplication, and PCA for dimensionality reduction.</p> <p>These foundational concepts and hands\u2011on exercises prepare you for more advanced AI and ML topics.</p> <ol> <li>Additional Resources Probability Fundamentals: Khan Academy \u201cIntroduction to Probability\u201d</li> </ol> <p>Linear Algebra Visualizations: 3Blue1Brown \u201cEssence of Linear Algebra\u201d series</p> <p>Python Tutorials: Official Python documentation at\u202fpython.org</p>"},{"location":"week-2/","title":"Week 2","text":"<p>testing the editing of this page 1x </p>"}]}