
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../week-2/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Week 1 - AI University Curriculum</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#x" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AI University Curriculum" class="md-header__button md-logo" aria-label="AI University Curriculum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI University Curriculum
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 1
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AI University Curriculum" class="md-nav__button md-logo" aria-label="AI University Curriculum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI University Curriculum
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Week 1
    
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../week-2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Week 2
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheat-sheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p>Week 1: History of AI &amp; Math Foundations
1. Lesson Overview
Learning Objectives
By the end of this lesson, you will be able to:</p>
<p>Describe three pivotal AI milestones and their lasting impact.</p>
<p>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</p>
<p>Understand probability fundamentalsâ€”random variables, expectation, varianceâ€”and compute them by hand and in Python.</p>
<p>Grasp key linear algebra conceptsâ€”vectors, matrices, dot products, matrix multiplicationâ€”and see how they underpin AI algorithms.</p>
<p>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</p>
<ol>
<li>
<p>Core Definitions
Term    Definition &amp; Example
Artificial Intelligence (AI)    â€œThe science and engineering of making intelligent machines, especially intelligent computer programs.â€ â€” John McCarthy, 1956
Example: A chatbot that interprets questions and crafts humanâ€‘like responses.
Machine Learning (ML)   Algorithms that improve performance on tasks by learning from data rather than explicit programming.
Example: A regression model that learns to predict steel prices from historical sales.
Data Science    Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data.
Example: Cleaning and visualizing eâ€‘commerce logs to uncover purchasing trends.</p>
</li>
<li>
<p>Concept Sections
A. AI Milestones
Excerpt (Definition Box):
Artificial Intelligence (AI) â€“ â€œThe science and engineering of making intelligent machines, especially intelligent computer programs.â€ â€” John McCarthy, 1956</p>
</li>
<li>
<p>The Dartmouth Workshop (1956)
In the summer of 1956, Johnâ€¯McCarthy, Marvinâ€¯Minsky, Nathanielâ€¯Rochester, and Claudeâ€¯Shannon convened at Dartmouth College to explore a bold question: â€œCan machines be made to simulate human intelligence?â€ They coined the term â€œArtificial Intelligenceâ€ and launched a twoâ€‘month study to investigate how machines might â€œlearn from experience,â€ â€œmake abstractions,â€ and â€œuse language.â€</p>
</li>
</ol>
<p>Context &amp; Significance:
Before Dartmouth, computers were viewed largely as number crunchers. This workshop reframed them as potential thinking machines, seeding optimism that a small team could tackle â€œevery aspect of learning or any other feature of intelligence.â€</p>
<p>First Programs:</p>
<p>Logic Theorist (1955): Developed by Newell &amp; Simon, it proved theorems in symbolic logicâ€”demonstrating that â€œthinkingâ€ tasks could be mechanized.</p>
<p>General Problem Solver (1957): An early attempt at a universal reasoning engine.</p>
<p>Why It Matters Today:</p>
<p>Cycle of Hype &amp; AI Winters: The booms and busts following Dartmouth teach us to balance ambition with realism when evaluating modern AI breakthroughs.</p>
<p>Legacy in Modern Research: Symbolic reasoning and search algorithms from this era underpin todayâ€™s knowledge graphs and constraint solving systems.</p>
<ol>
<li>Expert Systems Era (1970sâ€“1980s)
As symbolic AI matured, expert systems emergedâ€”ruleâ€‘based programs encoding human expertise as â€œifâ€“thenâ€ statements.</li>
</ol>
<p>Core Idea: Encode domain knowledge in production rules:</p>
<p>java
Copy
Edit
IF symptom = fever AND symptom = rash
THEN suggest = measles
Notable Example â€“ MYCIN (1972â€“1980):</p>
<p>Built at Stanford, MYCIN contained ~600 rules for diagnosing bacterial infections and recommending antibiotics.</p>
<p>It queried patient data (age, symptoms), applied its rule base, and in blind tests matched or outperformed human experts.</p>
<p>Strengths &amp; Limitations:</p>
<p>Strength: Transparent logicâ€”every recommendation traces back to specific rules.</p>
<p>Limitation: Required handâ€‘crafting thousands of rules and handled uncertainty poorly (no probabilistic reasoning).</p>
<p>Modern Relevance:</p>
<p>Ruleâ€‘based approaches inform decision support in finance and healthcare.</p>
<p>Todayâ€™s hybrid systems combine rules with statistical ML (e.g., regulatory checks plus modelâ€‘based scoring).</p>
<ol>
<li>Deep Learning Boom (2010sâ€“Present)
The fieldâ€™s third surge harnessed large datasets and GPU acceleration to train deep neural networksâ€”models with many layers that learn hierarchical features automatically.</li>
</ol>
<p>Key Breakthrough â€“ AlexNet (2012):</p>
<p>An eightâ€‘layer convolutional neural network (CNN) that halved error rates on the ImageNet challenge (1.2â€¯million labeled images, 1,000 categories).</p>
<p>Employed ReLU activations, dropout regularization, and GPUâ€‘based training.</p>
<p>Why Deep Learning Emerged:</p>
<p>Data: Massive labeled datasets (images, text, speech).</p>
<p>Compute: GPUs excel at parallel matrix operations critical for neural nets.</p>
<p>Algorithms: Innovations like batch normalization, architectural search, and optimized backpropagation.</p>
<p>Transformative Applications:</p>
<p>Computer Vision: Object detection (selfâ€‘driving cars), medical imaging (tumor detection).</p>
<p>Natural Language Processing: Language translation, text generation (GPTâ€‘style models).</p>
<p>Speech &amp; Audio: Voice assistants, realâ€‘time translation.</p>
<p>Why It Matters for You:</p>
<p>Modern AI frameworks (TensorFlow, PyTorch) are built around neural networks.</p>
<p>This era explains why subsequent terms focus on coding deep models and leveraging pretrained architectures for rapid deployment.</p>
<p>C. Probability Basics
Excerpt (Definition Box):
Probability Theory â€“ â€œThe mathematical framework for quantifying uncertainty and modeling random phenomena.â€</p>
<p>C1. Introduction
What Is Chance?</p>
<p>Everyday Analogy: Flipping a coin. You know there are two sidesâ€”heads or tailsâ€”but you canâ€™t predict which will land face up.</p>
<p>Key Idea: Probability measures how likely something is to happen, on a scale from 0 (impossible) to 1 (certain).</p>
<p>Example: A fair coin has probability 0.5 of landing heads.</p>
<p>Simple Data &amp; Averages</p>
<p>Real World Example: Your test scores this week: 80%, 90%, 70%, 100%, 60%.</p>
<p>Mean (Average): Add them up and divide by the number of tests:
(
80
+
90
+
70
+
100
+
60
)
/
5
=
80
%
(80+90+70+100+60)/5=80%</p>
<p>Why It Matters: The mean gives a sense of â€œtypicalâ€ performance.</p>
<p>Measuring Spread (Variance)</p>
<p>Analogy: If all scores are close to 80% (say 75%,â€¯80%,â€¯85%), thatâ€™s low spread; if they vary widely (60%,â€¯100%,â€¯70%), thatâ€™s high spread.</p>
<p>Step by Step (Scores Example):</p>
<p>Compute each scoreâ€™s difference from the mean (80): e.g., 60â€“80 = â€“20.</p>
<p>Square these differences to make them positive: (â€“20)Â² = 400.</p>
<p>Average the squared differences: if squares are [400,100,100,400,400], mean = 280.</p>
<p>That average (280) is the variance; its square root (â‰ˆ16.7) is the standard deviation.</p>
<p>Real World Uses:</p>
<p>Weather Forecasts: â€œThereâ€™s a 30% chance of rainâ€ guides umbrella choices.</p>
<p>Quality Control: A factory measures weight of cereal boxes; variance tells if the filling machine is consistent.</p>
<p>C2. Formal Definitions &amp; Deep Dive
Understanding Random Variables
A random variable 
ğ‘‹
X formalizes outcomes of random processes by assigning numeric values.</p>
<p>Discrete RV: Takes countable values (e.g., die rolls, number of returned orders).</p>
<p>Example: Rolling a sixâ€‘sided die â†’ 
ğ‘‹
âˆˆ
{
1
,
2
,
3
,
4
,
5
,
6
}
Xâˆˆ{1,2,3,4,5,6} with 
ğ‘ƒ
(
ğ‘‹
=
ğ‘˜
)
=
1
6
P(X=k)= 
6
1
â€‹
 .</p>
<p>Continuous RV: Takes any value in a continuum (e.g., time between machine failures).</p>
<p>Example: Time (in minutes) between software crashes might follow an exponential distribution:
ğ‘“
(
ğ‘¡
)
=
ğœ†
ğ‘’
âˆ’
ğœ†
ğ‘¡
,
Â 
ğ‘¡
â‰¥
0
f(t)=Î»e 
âˆ’Î»t
 ,Â tâ‰¥0.</p>
<p>Expectation (Mean)
The expectation 
ğ¸
[
ğ‘‹
]
E[X] is the longâ€‘run average if the experiment repeats infinitely.</p>
<p>Formula (Discrete):
ğ¸
[
ğ‘‹
]
=
âˆ‘
ğ‘–
ğ‘¥
ğ‘–
â€‰
ğ‘ƒ
(
ğ‘‹
=
ğ‘¥
ğ‘–
)
E[X]=âˆ‘ 
i
â€‹
 x 
i
â€‹
 P(X=x 
i
â€‹
 )</p>
<p>Formula (Continuous):
ğ¸
[
ğ‘‹
]
=
âˆ«
âˆ’
âˆ
âˆ
ğ‘¥
â€‰
ğ‘“
(
ğ‘¥
)
â€‰
ğ‘‘
ğ‘¥
E[X]=âˆ« 
âˆ’âˆ
âˆ
â€‹
 xf(x)dx</p>
<p>Worked Example (Die):
ğ¸
[
ğ‘‹
]
=
1
+
2
+
3
+
4
+
5
+
6
6
=
3.5
E[X]= 
6
1+2+3+4+5+6
â€‹
 =3.5</p>
<p>Relevance: Loss functions like mean squared error minimize expected error; understanding expectation clarifies why we average squared deviations.</p>
<p>Variance &amp; Standard Deviation</p>
<p>Variance:
V
a
r
(
ğ‘‹
)
=
ğ¸
[
(
ğ‘‹
âˆ’
ğ¸
[
ğ‘‹
]
)
2
]
Var(X)=E[(Xâˆ’E[X]) 
2
 ]</p>
<p>Standard Deviation:
ğœ
=
V
a
r
(
ğ‘‹
)
Ïƒ= 
Var(X)
â€‹</p>
<p>Worked Example (Die):
V
a
r
(
ğ‘‹
)
=
(
1
âˆ’
3.5
)
2
+
â‹¯
+
(
6
âˆ’
3.5
)
2
6
=
17.5
6
â‰ˆ
2.92
,
Â 
ğœ
â‰ˆ
1.71
Var(X)= 
6
(1âˆ’3.5) 
2
 +â‹¯+(6âˆ’3.5) 
2</p>
<p>â€‹
 = 
6
17.5
â€‹
 â‰ˆ2.92,Â Ïƒâ‰ˆ1.71</p>
<p>Relevance: Guides feature scaling, sets confidence intervals, and underpins uncertainty quantification in finance or anomaly detection.</p>
<p>Why These Concepts Matter in AI</p>
<p>Model Training: Loss functions (e.g., MSE) rely on expectation of squared errors.</p>
<p>Uncertainty Quantification: Variance informs risk metrics (VaR, confidence intervals).</p>
<p>Feature Engineering: Distribution shapes dictate transformations (e.g., log scaling skewed data).</p>
<p>D. Linear Algebra Basics
Excerpt (Definition Box):
Linear Algebra â€“ â€œThe branch of mathematics concerned with vectors, vector spaces, and linear transformations.â€</p>
<p>D1. Introduction
Vectors as Lists</p>
<p>Analogy: A grocery list: [2 bananas, 1 loaf bread, 500 g cheese].</p>
<p>Key Idea: A vector is just a list of numbers representing â€œfeatures.â€</p>
<p>Matrices as Tables</p>
<p>Analogy: A seating chart in class: rows are table numbers, columns are seat positions.</p>
<p>mathematica
Copy
Edit
|    | S1 | S2 | S3 |
|----|----|----|----|
| T1 | A  | B  | C  |
| T2 | D  | E  | F  |
Key Idea: A matrix is multiple vectors â€œstackedâ€ into rows or columns.</p>
<p>Dot Product Intuition</p>
<p>Example (Bill Splitting): You and a friend order appetizers [3, 2] plates and drinks [1, 2] each. To compute total cost if plates = $5, drinks = $2:
[
3
,
2
]
â‹…
[
5
,
2
]
=
3
Ã—
5
+
2
Ã—
2
=
15
+
4
=
$
19
[3,2]â‹…[5,2]=3Ã—5+2Ã—2=15+4=$19</p>
<p>Why It Matters: Combines quantities and prices; same math as a regression prediction.</p>
<p>Real World Matrix Use</p>
<p>Recipe scaling: A 4â€‘serving recipeâ€™s ingredients in a matrix, multiply by 1.5 to get 6 servings.</p>
<p>School timetable: Daysâ€¯Ã—â€¯hours grid for scheduling classes.</p>
<p>D2. Formal Definitions &amp; Deep Dive
Vectors &amp; Their Interpretation
A vector 
ğ‘¥
âˆˆ
ğ‘…
ğ‘›
xâˆˆR 
n
  is an ordered list of 
ğ‘›
n numbers representing features or data points.</p>
<p>Example:
ğ‘¥
=
[
age
,
monthly_spend
,
num_orders
]
=
[
45
,
320.5
,
12
]
x=[age,monthly_spend,num_orders]=[45,320.5,12]</p>
<p>Matrices &amp; Batch Operations
A matrix 
ğ‘‹
âˆˆ
ğ‘…
ğ‘š
Ã—
ğ‘›
XâˆˆR 
mÃ—n
  stacks 
ğ‘š
m row vectors of dimension 
ğ‘›
n.</p>
<p>Example:</p>
<h1 id="x">ğ‘‹</h1>
<p>[
45
320.5
12
23
150.0
5
â‹®
â‹®
â‹®
]
X= 
â€‹</p>
<p>45
23
â‹®
â€‹</p>
<p>320.5
150.0
â‹®
â€‹</p>
<p>12
5
â‹®
â€‹</p>
<p>â€‹</p>
<p>Dot Product &amp; Linear Transformations</p>
<p>Dot Product:
ğ‘
â‹…
ğ‘
=
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘
ğ‘–
â€‰
ğ‘
ğ‘–
aâ‹…b=âˆ‘ 
i=1
n
â€‹
 a 
i
â€‹
 b 
i
â€‹</p>
<p>Example: [1,2,3] â‹… [4,5,6] = 32</p>
<p>Use in AI:</p>
<p>Regression: 
ğ‘¦
^
=
ğ‘¤
â‹…
ğ‘¥
+
ğ‘
y
^
â€‹
 =wâ‹…x+b</p>
<p>Neural Nets: Each neuron computes 
ğ‘§
=
ğ‘¤
â‹…
ğ‘¥
+
ğ‘
z=wâ‹…x+b, then applies an activation.</p>
<p>Matrix Multiplication
ğ¶
=
ğ´
Ã—
ğµ
,
ğ¶
ğ‘–
ğ‘—
=
âˆ‘
ğ‘˜
=
1
ğ‘›
ğ´
ğ‘–
ğ‘˜
ğµ
ğ‘˜
ğ‘—
C=AÃ—B,C 
ij
â€‹
 =âˆ‘ 
k=1
n
â€‹
 A 
ik
â€‹
 B 
kj
â€‹</p>
<p>Example: Transforming feature spaces or chaining layers in a deep network.</p>
<p>Relevance for AI Practitioners</p>
<p>Batch Processing: GPUs and NumPy rely on vectorized matrix operations.</p>
<p>Model Introspection: Weight matrices and activation maps in CNNs are built on these operations.</p>
<p>Dimensionality Reduction: PCA uses eigenvectors/eigenvalues of covariance matrices to compress data.</p>
<ol>
<li>Tools Installation &amp; Setup
Windows &amp; Mac</li>
</ol>
<p>A. Install Python &amp; Anaconda
Navigate to: https://www.anaconda.com/products/distribution</p>
<p>Download the Pythonâ€¯3.x installer for your OS.</p>
<p>Run the installer, accept defaults.</p>
<p>Open Anaconda Navigator from your Start menu (Windows) or Applications folder (Mac).</p>
<p>B. Launch Jupyter Notebook
In Anaconda Navigator, click Launch under Jupyter Notebook.</p>
<p>A browser window opens showing your file system.</p>
<p>Click New â†’ Python 3.</p>
<p>Rename the notebook to Week1_AI_Math.ipynb.</p>
<p>C. Install &amp; Import NumPy &amp; pandas
In a notebook cell, run:</p>
<p>bash
Copy
Edit
!conda install numpy pandas -y
Then, in the next cell:</p>
<p>python
Copy
Edit
import numpy as np
import pandas as pd
5. Step-by-Step Exercises
Exercise 1: Die Roll Simulation &amp; Statistics (Template)
Exercise Overview &amp; Purpose</p>
<p>What weâ€™re doing: Simulate 1,000 rolls of a fair sixâ€‘sided die in Python to compute the empirical mean and variance.</p>
<p>Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p>
<p>Concept Reinforcement</p>
<p>Probability &amp; Random Variables</p>
<p>Expectation &amp; Variance</p>
<p>Sampling Variability</p>
<p>Real World Relevance</p>
<p>Quality Control (defect rate simulation)</p>
<p>Risk Modeling (Monte Carlo portfolio variance)</p>
<p>Randomized Algorithms &amp; Game Balancing</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import numpy as np</p>
<h1 id="simulate-1000-die-rolls">Simulate 1,000 die rolls</h1>
<p>rolls = np.random.randint(1, 7, size=1000)</p>
<h1 id="compute-statistics">Compute statistics</h1>
<p>mean_rolls = rolls.mean()
var_rolls  = rolls.var()</p>
<p>print("Simulated Mean:    ", mean_rolls)
print("Simulated Variance:", var_rolls)
Notes:</p>
<p>np.random.randint(1, 7, size=1000): integers 1â€“6</p>
<p>.mean(), .var(): compute empirical mean &amp; variance</p>
<p>Expected Outcomes &amp; Interpretation</p>
<p>Mean â‰ˆâ€¯3.5, Variance â‰ˆâ€¯2.92 (Â± sampling noise)</p>
<p>Larger samples converge closer to theory</p>
<p>Extensions &amp; Variations</p>
<p>Vary sample size (100, 10,000)</p>
<p>Simulate weighted/unfair die</p>
<p>Plot histogram with matplotlib</p>
<p>Additional Notes &amp; Tips</p>
<p>Use np.random.seed(42) for reproducibility</p>
<p>Avoid Python loops; prefer NumPy vectorization</p>
<p>Exerciseâ€¯2: Coin Flip Probability Estimation
Overview &amp; Purpose
Simulate 10,000 coin flips to estimate the probability of heads and tails.</p>
<p>Concept Reinforcement</p>
<p>Discrete random variables</p>
<p>Empirical vs. theoretical probability</p>
<p>Real World Relevance</p>
<p>A/B testing conversion rates (success/failure)</p>
<p>Clinical trial outcomes</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import numpy as np</p>
<p>np.random.seed(0)
flips = np.random.choice(['H','T'], size=10000)
p_heads = np.mean(flips == 'H')
p_tails = np.mean(flips == 'T')
print(f"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}")
Expected Outcomes &amp; Interpretation
~0.5 each, with fluctuations ~Â±0.01.</p>
<p>Extensions &amp; Variations</p>
<p>Weighted coin (p=['H':0.3,'T':0.7])</p>
<p>Plot bar chart of counts</p>
<p>Additional Notes &amp; Tips
Use np.random.seed(â€¦) for reproducibility.</p>
<p>Exerciseâ€¯3: Histogram of Die Rolls
Overview &amp; Purpose
Visualize the distribution of the 1,000 die rolls from Exerciseâ€¯1.</p>
<p>Concept Reinforcement</p>
<p>Frequency vs. probability</p>
<p>Data visualization basics</p>
<p>Real World Relevance</p>
<p>Sales distribution by category</p>
<p>Error rates by batch</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import matplotlib.pyplot as plt</p>
<p>plt.hist(rolls, bins=range(1,8), align='left', rwidth=0.8)
plt.xlabel('Die Face')
plt.ylabel('Frequency')
plt.title('Histogram of 1,000 Die Rolls')
plt.show()
Expected Outcomes &amp; Interpretation
Bars roughly equal height for facesâ€¯1â€“6.</p>
<p>Extensions &amp; Variations</p>
<p>Normalized histogram (density=True)</p>
<p>Overlay theoretical PMF</p>
<p>Additional Notes &amp; Tips
Ensure bins=range(1,8) to center on integer faces.</p>
<p>Exerciseâ€¯4: Exponential Distribution Simulation
Overview &amp; Purpose
Simulate 5,000 samples from an exponential distribution (meanâ€¯=â€¯2) and compute mean/variance.</p>
<p>Concept Reinforcement</p>
<p>Continuous random variables</p>
<p>Relationship between distribution parameters and statistics</p>
<p>Real World Relevance</p>
<p>Time between machine failures</p>
<p>Call-center interarrival times</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
samples = np.random.exponential(scale=2, size=5000)
print("Empirical Mean:", samples.mean())
print("Empirical Variance:", samples.var())
Expected Outcomes &amp; Interpretation
Mean â‰ˆâ€¯2; variance â‰ˆâ€¯4.</p>
<p>Extensions &amp; Variations</p>
<p>Change scale (mean) parameter</p>
<p>Plot histogram + theoretical PDF</p>
<p>Additional Notes &amp; Tips
Use np.histogram or matplotlib for PDF overlay.</p>
<p>Exerciseâ€¯5: Normal Distribution Sampling
Overview &amp; Purpose
Draw 10,000 samples from a standard normal (meanâ€¯0, Ïƒâ€¯=â€¯1) and verify statistics.</p>
<p>Concept Reinforcement</p>
<p>Properties of Gaussian distribution</p>
<p>Central Limit Theorem preview</p>
<p>Real World Relevance</p>
<p>Measurement errors</p>
<p>Standardized test score modeling</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
normals = np.random.randn(10000)
print("Mean:", normals.mean())
print("Variance:", normals.var())
Expected Outcomes &amp; Interpretation
Mean â‰ˆâ€¯0, variance â‰ˆâ€¯1.</p>
<p>Extensions &amp; Variations</p>
<p>Use np.random.normal(loc, scale, size)</p>
<p>QQ plot vs. theoretical normal</p>
<p>Additional Notes &amp; Tips
Matplotlibâ€™s plt.hist(..., density=True) for PDF shape.</p>
<p>Exerciseâ€¯6: Sampling Distribution of the Mean
Overview &amp; Purpose
Simulate 1,000 experiments, each of 100 die rolls, record sample means, and examine their distribution.</p>
<p>Concept Reinforcement</p>
<p>Law of Large Numbers</p>
<p>Sampling variability reduction</p>
<p>Real World Relevance</p>
<p>Polling averages</p>
<p>Quality metrics over batches</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
means = [np.random.randint(1,7,100).mean() for _ in range(1000)]
plt.hist(means, bins=20)
plt.title('Sampling Distribution of Die Roll Means')
plt.show()
Expected Outcomes &amp; Interpretation
Histogram approximates normal around 3.5 with narrower spread.</p>
<p>Extensions &amp; Variations</p>
<p>Vary experiment size (n=10, n=1000)</p>
<p>Compute standard error (Ïƒ/âˆšn)</p>
<p>Additional Notes &amp; Tips
List comprehensions vs. loops for clarity.</p>
<p>Exerciseâ€¯7: Weighted Dice Simulation
Overview &amp; Purpose
Simulate 1,000 rolls of a biased die with 
ğ‘ƒ
(
6
)
=
0.5
P(6)=0.5, others equal.</p>
<p>Concept Reinforcement</p>
<p>Custom discrete distributions</p>
<p>Impact of bias on mean/variance</p>
<p>Real World Relevance</p>
<p>Biased processes in manufacturing</p>
<p>Skewed customer behavior models</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
faces = [1,2,3,4,5,6]
probs = [0.1]*5 + [0.5]
rolls_biased = np.random.choice(faces, size=1000, p=probs)
print("Mean:", rolls_biased.mean(), "Variance:", rolls_biased.var())
Expected Outcomes &amp; Interpretation
Mean &gt;â€¯3.5, variance different from fair die.</p>
<p>Extensions &amp; Variations</p>
<p>Tune probs for different biases</p>
<p>Compare histograms side by side</p>
<p>Additional Notes &amp; Tips
Sum of probs must equal 1.</p>
<p>Exerciseâ€¯8: Vector Addition &amp; Scaling
Overview &amp; Purpose
Demonstrate vector addition and scalar multiplication with feature vectors.</p>
<p>Concept Reinforcement</p>
<p>Vector space operations</p>
<p>Geometric interpretation</p>
<p>Real World Relevance</p>
<p>Combining feature influences</p>
<p>Scaling normalized data</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
v1 = np.array([2, 4, 6])
v2 = np.array([1, 3, 5])
sum_v   = v1 + v2      # vector addition
scaled_v = 0.5 * v1    # scalar multiplication
print("Sum:", sum_v)
print("Scaled:", scaled_v)
Expected Outcomes &amp; Interpretation
Sum = [3, 7, 11]; scaled = [1, 2, 3].</p>
<p>Extensions &amp; Variations</p>
<p>Compute dot product of sum_v and v2</p>
<p>Visualize vectors in 2D/3D</p>
<p>Additional Notes &amp; Tips
Ensure consistent dimensions.</p>
<p>Exerciseâ€¯9: Matrix Multiplication Demonstration
Overview &amp; Purpose
Multiply a 2Ã—3 matrix by a 3Ã—2 matrix to reinforce matrix multiplication rules.</p>
<p>Concept Reinforcement</p>
<p>Shape compatibility</p>
<p>Summation over inner index</p>
<p>Real World Relevance</p>
<p>Transforming feature spaces</p>
<p>Composition of network layers</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
A = np.array([[1,2,3],[4,5,6]])
B = np.array([[7,8],[9,10],[11,12]])
C = A.dot(B)
print("Result:\n", C)
Expected Outcomes &amp; Interpretation
C = [[58, 64], [139, 154]].</p>
<p>Extensions &amp; Variations</p>
<p>Reverse multiplication to show error</p>
<p>Use @ operator in Python 3.5+</p>
<p>Additional Notes &amp; Tips
Check shapes via A.shape and B.shape.</p>
<p>Exerciseâ€¯10: PCA on Toy Dataset
Overview &amp; Purpose
Perform PCA on a small 2â€‘D dataset to reduce to 1â€‘D and visualize variance capture.</p>
<p>Concept Reinforcement</p>
<p>Eigenvectors/eigenvalues</p>
<p>Dimensionality reduction</p>
<p>Real World Relevance</p>
<p>Compressing image data</p>
<p>Feature extraction for clustering</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
from sklearn.decomposition import PCA
import numpy as np</p>
<p>X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0]])
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Projected data:\n", X_pca)
Expected Outcomes &amp; Interpretation
Most variance captured in first component (â‰ˆ98%).</p>
<p>Extensions &amp; Variations</p>
<p>Plot original vs. projected points</p>
<p>Try n_components=2</p>
<p>Additional Notes &amp; Tips
Requires scikitâ€‘learn installation.</p>
<ol>
<li>Summary of Weekâ€¯1
Throughout this first week, you have:</li>
</ol>
<p>Traced AI History: From the Dartmouth Workshop to Expert Systems (MYCIN) and the Deep Learning revolution (AlexNet).</p>
<p>Built Probability Skills: Learned random variables, expectation, varianceâ€”both by hand and in Python (die rolls, coin flips, exponential and normal sampling).</p>
<p>Visualized Data: Plotted histograms, demonstrated Central Limit Theorem.</p>
<p>Handled Bias: Modeled a weighted die to see effects on distribution.</p>
<p>Applied Linear Algebra: Performed vector ops, matrix multiplication, and PCA for dimensionality reduction.</p>
<p>These foundational concepts and handsâ€‘on exercises prepare you for more advanced AI and ML topics.</p>
<ol>
<li>Additional Resources
Probability Fundamentals: Khan Academy â€œIntroduction to Probabilityâ€</li>
</ol>
<p>Linear Algebra Visualizations: 3Blue1Brown â€œEssence of Linear Algebraâ€ series</p>
<p>Python Tutorials: Official Python documentation atâ€¯python.org</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>