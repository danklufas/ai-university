
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../week-2/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Week 1 - AI University Curriculum</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#x" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AI University Curriculum" class="md-header__button md-logo" aria-label="AI University Curriculum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI University Curriculum
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 1
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AI University Curriculum" class="md-nav__button md-logo" aria-label="AI University Curriculum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI University Curriculum
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Week 1
    
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../week-2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Week 2
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cheat-sheet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheat Sheet
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p>Week 1: History of AI &amp; Math Foundations
1. Lesson Overview
Learning Objectives
By the end of this lesson, you will be able to:</p>
<p>Describe three pivotal AI milestones and their lasting impact.</p>
<p>Define Artificial Intelligence (AI), Machine Learning (ML), and Data Science with clear examples.</p>
<p>Understand probability fundamentals—random variables, expectation, variance—and compute them by hand and in Python.</p>
<p>Grasp key linear algebra concepts—vectors, matrices, dot products, matrix multiplication—and see how they underpin AI algorithms.</p>
<p>Install and launch the required tools (Python, Jupyter Notebook, NumPy, pandas) and execute basic code.</p>
<ol>
<li>
<p>Core Definitions
Term    Definition &amp; Example
Artificial Intelligence (AI)    “The science and engineering of making intelligent machines, especially intelligent computer programs.” — John McCarthy, 1956
Example: A chatbot that interprets questions and crafts human‑like responses.
Machine Learning (ML)   Algorithms that improve performance on tasks by learning from data rather than explicit programming.
Example: A regression model that learns to predict steel prices from historical sales.
Data Science    Interdisciplinary practice of using statistics, programming, and domain knowledge to extract insights from data.
Example: Cleaning and visualizing e‑commerce logs to uncover purchasing trends.</p>
</li>
<li>
<p>Concept Sections
A. AI Milestones
Excerpt (Definition Box):
Artificial Intelligence (AI) – “The science and engineering of making intelligent machines, especially intelligent computer programs.” — John McCarthy, 1956</p>
</li>
<li>
<p>The Dartmouth Workshop (1956)
In the summer of 1956, John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon convened at Dartmouth College to explore a bold question: “Can machines be made to simulate human intelligence?” They coined the term “Artificial Intelligence” and launched a two‑month study to investigate how machines might “learn from experience,” “make abstractions,” and “use language.”</p>
</li>
</ol>
<p>Context &amp; Significance:
Before Dartmouth, computers were viewed largely as number crunchers. This workshop reframed them as potential thinking machines, seeding optimism that a small team could tackle “every aspect of learning or any other feature of intelligence.”</p>
<p>First Programs:</p>
<p>Logic Theorist (1955): Developed by Newell &amp; Simon, it proved theorems in symbolic logic—demonstrating that “thinking” tasks could be mechanized.</p>
<p>General Problem Solver (1957): An early attempt at a universal reasoning engine.</p>
<p>Why It Matters Today:</p>
<p>Cycle of Hype &amp; AI Winters: The booms and busts following Dartmouth teach us to balance ambition with realism when evaluating modern AI breakthroughs.</p>
<p>Legacy in Modern Research: Symbolic reasoning and search algorithms from this era underpin today’s knowledge graphs and constraint solving systems.</p>
<ol>
<li>Expert Systems Era (1970s–1980s)
As symbolic AI matured, expert systems emerged—rule‑based programs encoding human expertise as “if–then” statements.</li>
</ol>
<p>Core Idea: Encode domain knowledge in production rules:</p>
<p>java
Copy
Edit
IF symptom = fever AND symptom = rash
THEN suggest = measles
Notable Example – MYCIN (1972–1980):</p>
<p>Built at Stanford, MYCIN contained ~600 rules for diagnosing bacterial infections and recommending antibiotics.</p>
<p>It queried patient data (age, symptoms), applied its rule base, and in blind tests matched or outperformed human experts.</p>
<p>Strengths &amp; Limitations:</p>
<p>Strength: Transparent logic—every recommendation traces back to specific rules.</p>
<p>Limitation: Required hand‑crafting thousands of rules and handled uncertainty poorly (no probabilistic reasoning).</p>
<p>Modern Relevance:</p>
<p>Rule‑based approaches inform decision support in finance and healthcare.</p>
<p>Today’s hybrid systems combine rules with statistical ML (e.g., regulatory checks plus model‑based scoring).</p>
<ol>
<li>Deep Learning Boom (2010s–Present)
The field’s third surge harnessed large datasets and GPU acceleration to train deep neural networks—models with many layers that learn hierarchical features automatically.</li>
</ol>
<p>Key Breakthrough – AlexNet (2012):</p>
<p>An eight‑layer convolutional neural network (CNN) that halved error rates on the ImageNet challenge (1.2 million labeled images, 1,000 categories).</p>
<p>Employed ReLU activations, dropout regularization, and GPU‑based training.</p>
<p>Why Deep Learning Emerged:</p>
<p>Data: Massive labeled datasets (images, text, speech).</p>
<p>Compute: GPUs excel at parallel matrix operations critical for neural nets.</p>
<p>Algorithms: Innovations like batch normalization, architectural search, and optimized backpropagation.</p>
<p>Transformative Applications:</p>
<p>Computer Vision: Object detection (self‑driving cars), medical imaging (tumor detection).</p>
<p>Natural Language Processing: Language translation, text generation (GPT‑style models).</p>
<p>Speech &amp; Audio: Voice assistants, real‑time translation.</p>
<p>Why It Matters for You:</p>
<p>Modern AI frameworks (TensorFlow, PyTorch) are built around neural networks.</p>
<p>This era explains why subsequent terms focus on coding deep models and leveraging pretrained architectures for rapid deployment.</p>
<p>C. Probability Basics
Excerpt (Definition Box):
Probability Theory – “The mathematical framework for quantifying uncertainty and modeling random phenomena.”</p>
<p>C1. Introduction
What Is Chance?</p>
<p>Everyday Analogy: Flipping a coin. You know there are two sides—heads or tails—but you can’t predict which will land face up.</p>
<p>Key Idea: Probability measures how likely something is to happen, on a scale from 0 (impossible) to 1 (certain).</p>
<p>Example: A fair coin has probability 0.5 of landing heads.</p>
<p>Simple Data &amp; Averages</p>
<p>Real World Example: Your test scores this week: 80%, 90%, 70%, 100%, 60%.</p>
<p>Mean (Average): Add them up and divide by the number of tests:
(
80
+
90
+
70
+
100
+
60
)
/
5
=
80
%
(80+90+70+100+60)/5=80%</p>
<p>Why It Matters: The mean gives a sense of “typical” performance.</p>
<p>Measuring Spread (Variance)</p>
<p>Analogy: If all scores are close to 80% (say 75%, 80%, 85%), that’s low spread; if they vary widely (60%, 100%, 70%), that’s high spread.</p>
<p>Step by Step (Scores Example):</p>
<p>Compute each score’s difference from the mean (80): e.g., 60–80 = –20.</p>
<p>Square these differences to make them positive: (–20)² = 400.</p>
<p>Average the squared differences: if squares are [400,100,100,400,400], mean = 280.</p>
<p>That average (280) is the variance; its square root (≈16.7) is the standard deviation.</p>
<p>Real World Uses:</p>
<p>Weather Forecasts: “There’s a 30% chance of rain” guides umbrella choices.</p>
<p>Quality Control: A factory measures weight of cereal boxes; variance tells if the filling machine is consistent.</p>
<p>C2. Formal Definitions &amp; Deep Dive
Understanding Random Variables
A random variable 
𝑋
X formalizes outcomes of random processes by assigning numeric values.</p>
<p>Discrete RV: Takes countable values (e.g., die rolls, number of returned orders).</p>
<p>Example: Rolling a six‑sided die → 
𝑋
∈
{
1
,
2
,
3
,
4
,
5
,
6
}
X∈{1,2,3,4,5,6} with 
𝑃
(
𝑋
=
𝑘
)
=
1
6
P(X=k)= 
6
1
​
 .</p>
<p>Continuous RV: Takes any value in a continuum (e.g., time between machine failures).</p>
<p>Example: Time (in minutes) between software crashes might follow an exponential distribution:
𝑓
(
𝑡
)
=
𝜆
𝑒
−
𝜆
𝑡
,
 
𝑡
≥
0
f(t)=λe 
−λt
 , t≥0.</p>
<p>Expectation (Mean)
The expectation 
𝐸
[
𝑋
]
E[X] is the long‑run average if the experiment repeats infinitely.</p>
<p>Formula (Discrete):
𝐸
[
𝑋
]
=
∑
𝑖
𝑥
𝑖
 
𝑃
(
𝑋
=
𝑥
𝑖
)
E[X]=∑ 
i
​
 x 
i
​
 P(X=x 
i
​
 )</p>
<p>Formula (Continuous):
𝐸
[
𝑋
]
=
∫
−
∞
∞
𝑥
 
𝑓
(
𝑥
)
 
𝑑
𝑥
E[X]=∫ 
−∞
∞
​
 xf(x)dx</p>
<p>Worked Example (Die):
𝐸
[
𝑋
]
=
1
+
2
+
3
+
4
+
5
+
6
6
=
3.5
E[X]= 
6
1+2+3+4+5+6
​
 =3.5</p>
<p>Relevance: Loss functions like mean squared error minimize expected error; understanding expectation clarifies why we average squared deviations.</p>
<p>Variance &amp; Standard Deviation</p>
<p>Variance:
V
a
r
(
𝑋
)
=
𝐸
[
(
𝑋
−
𝐸
[
𝑋
]
)
2
]
Var(X)=E[(X−E[X]) 
2
 ]</p>
<p>Standard Deviation:
𝜎
=
V
a
r
(
𝑋
)
σ= 
Var(X)
​</p>
<p>Worked Example (Die):
V
a
r
(
𝑋
)
=
(
1
−
3.5
)
2
+
⋯
+
(
6
−
3.5
)
2
6
=
17.5
6
≈
2.92
,
 
𝜎
≈
1.71
Var(X)= 
6
(1−3.5) 
2
 +⋯+(6−3.5) 
2</p>
<p>​
 = 
6
17.5
​
 ≈2.92, σ≈1.71</p>
<p>Relevance: Guides feature scaling, sets confidence intervals, and underpins uncertainty quantification in finance or anomaly detection.</p>
<p>Why These Concepts Matter in AI</p>
<p>Model Training: Loss functions (e.g., MSE) rely on expectation of squared errors.</p>
<p>Uncertainty Quantification: Variance informs risk metrics (VaR, confidence intervals).</p>
<p>Feature Engineering: Distribution shapes dictate transformations (e.g., log scaling skewed data).</p>
<p>D. Linear Algebra Basics
Excerpt (Definition Box):
Linear Algebra – “The branch of mathematics concerned with vectors, vector spaces, and linear transformations.”</p>
<p>D1. Introduction
Vectors as Lists</p>
<p>Analogy: A grocery list: [2 bananas, 1 loaf bread, 500 g cheese].</p>
<p>Key Idea: A vector is just a list of numbers representing “features.”</p>
<p>Matrices as Tables</p>
<p>Analogy: A seating chart in class: rows are table numbers, columns are seat positions.</p>
<p>mathematica
Copy
Edit
|    | S1 | S2 | S3 |
|----|----|----|----|
| T1 | A  | B  | C  |
| T2 | D  | E  | F  |
Key Idea: A matrix is multiple vectors “stacked” into rows or columns.</p>
<p>Dot Product Intuition</p>
<p>Example (Bill Splitting): You and a friend order appetizers [3, 2] plates and drinks [1, 2] each. To compute total cost if plates = $5, drinks = $2:
[
3
,
2
]
⋅
[
5
,
2
]
=
3
×
5
+
2
×
2
=
15
+
4
=
$
19
[3,2]⋅[5,2]=3×5+2×2=15+4=$19</p>
<p>Why It Matters: Combines quantities and prices; same math as a regression prediction.</p>
<p>Real World Matrix Use</p>
<p>Recipe scaling: A 4‑serving recipe’s ingredients in a matrix, multiply by 1.5 to get 6 servings.</p>
<p>School timetable: Days × hours grid for scheduling classes.</p>
<p>D2. Formal Definitions &amp; Deep Dive
Vectors &amp; Their Interpretation
A vector 
𝑥
∈
𝑅
𝑛
x∈R 
n
  is an ordered list of 
𝑛
n numbers representing features or data points.</p>
<p>Example:
𝑥
=
[
age
,
monthly_spend
,
num_orders
]
=
[
45
,
320.5
,
12
]
x=[age,monthly_spend,num_orders]=[45,320.5,12]</p>
<p>Matrices &amp; Batch Operations
A matrix 
𝑋
∈
𝑅
𝑚
×
𝑛
X∈R 
m×n
  stacks 
𝑚
m row vectors of dimension 
𝑛
n.</p>
<p>Example:</p>
<h1 id="x">𝑋</h1>
<p>[
45
320.5
12
23
150.0
5
⋮
⋮
⋮
]
X= 
​</p>
<p>45
23
⋮
​</p>
<p>320.5
150.0
⋮
​</p>
<p>12
5
⋮
​</p>
<p>​</p>
<p>Dot Product &amp; Linear Transformations</p>
<p>Dot Product:
𝑎
⋅
𝑏
=
∑
𝑖
=
1
𝑛
𝑎
𝑖
 
𝑏
𝑖
a⋅b=∑ 
i=1
n
​
 a 
i
​
 b 
i
​</p>
<p>Example: [1,2,3] ⋅ [4,5,6] = 32</p>
<p>Use in AI:</p>
<p>Regression: 
𝑦
^
=
𝑤
⋅
𝑥
+
𝑏
y
^
​
 =w⋅x+b</p>
<p>Neural Nets: Each neuron computes 
𝑧
=
𝑤
⋅
𝑥
+
𝑏
z=w⋅x+b, then applies an activation.</p>
<p>Matrix Multiplication
𝐶
=
𝐴
×
𝐵
,
𝐶
𝑖
𝑗
=
∑
𝑘
=
1
𝑛
𝐴
𝑖
𝑘
𝐵
𝑘
𝑗
C=A×B,C 
ij
​
 =∑ 
k=1
n
​
 A 
ik
​
 B 
kj
​</p>
<p>Example: Transforming feature spaces or chaining layers in a deep network.</p>
<p>Relevance for AI Practitioners</p>
<p>Batch Processing: GPUs and NumPy rely on vectorized matrix operations.</p>
<p>Model Introspection: Weight matrices and activation maps in CNNs are built on these operations.</p>
<p>Dimensionality Reduction: PCA uses eigenvectors/eigenvalues of covariance matrices to compress data.</p>
<ol>
<li>Tools Installation &amp; Setup
Windows &amp; Mac</li>
</ol>
<p>A. Install Python &amp; Anaconda
Navigate to: https://www.anaconda.com/products/distribution</p>
<p>Download the Python 3.x installer for your OS.</p>
<p>Run the installer, accept defaults.</p>
<p>Open Anaconda Navigator from your Start menu (Windows) or Applications folder (Mac).</p>
<p>B. Launch Jupyter Notebook
In Anaconda Navigator, click Launch under Jupyter Notebook.</p>
<p>A browser window opens showing your file system.</p>
<p>Click New → Python 3.</p>
<p>Rename the notebook to Week1_AI_Math.ipynb.</p>
<p>C. Install &amp; Import NumPy &amp; pandas
In a notebook cell, run:</p>
<p>bash
Copy
Edit
!conda install numpy pandas -y
Then, in the next cell:</p>
<p>python
Copy
Edit
import numpy as np
import pandas as pd
5. Step-by-Step Exercises
Exercise 1: Die Roll Simulation &amp; Statistics (Template)
Exercise Overview &amp; Purpose</p>
<p>What we’re doing: Simulate 1,000 rolls of a fair six‑sided die in Python to compute the empirical mean and variance.</p>
<p>Why: Reinforces theoretical vs. empirical probability, builds NumPy familiarity, and demonstrates sampling variability.</p>
<p>Concept Reinforcement</p>
<p>Probability &amp; Random Variables</p>
<p>Expectation &amp; Variance</p>
<p>Sampling Variability</p>
<p>Real World Relevance</p>
<p>Quality Control (defect rate simulation)</p>
<p>Risk Modeling (Monte Carlo portfolio variance)</p>
<p>Randomized Algorithms &amp; Game Balancing</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import numpy as np</p>
<h1 id="simulate-1000-die-rolls">Simulate 1,000 die rolls</h1>
<p>rolls = np.random.randint(1, 7, size=1000)</p>
<h1 id="compute-statistics">Compute statistics</h1>
<p>mean_rolls = rolls.mean()
var_rolls  = rolls.var()</p>
<p>print("Simulated Mean:    ", mean_rolls)
print("Simulated Variance:", var_rolls)
Notes:</p>
<p>np.random.randint(1, 7, size=1000): integers 1–6</p>
<p>.mean(), .var(): compute empirical mean &amp; variance</p>
<p>Expected Outcomes &amp; Interpretation</p>
<p>Mean ≈ 3.5, Variance ≈ 2.92 (± sampling noise)</p>
<p>Larger samples converge closer to theory</p>
<p>Extensions &amp; Variations</p>
<p>Vary sample size (100, 10,000)</p>
<p>Simulate weighted/unfair die</p>
<p>Plot histogram with matplotlib</p>
<p>Additional Notes &amp; Tips</p>
<p>Use np.random.seed(42) for reproducibility</p>
<p>Avoid Python loops; prefer NumPy vectorization</p>
<p>Exercise 2: Coin Flip Probability Estimation
Overview &amp; Purpose
Simulate 10,000 coin flips to estimate the probability of heads and tails.</p>
<p>Concept Reinforcement</p>
<p>Discrete random variables</p>
<p>Empirical vs. theoretical probability</p>
<p>Real World Relevance</p>
<p>A/B testing conversion rates (success/failure)</p>
<p>Clinical trial outcomes</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import numpy as np</p>
<p>np.random.seed(0)
flips = np.random.choice(['H','T'], size=10000)
p_heads = np.mean(flips == 'H')
p_tails = np.mean(flips == 'T')
print(f"P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}")
Expected Outcomes &amp; Interpretation
~0.5 each, with fluctuations ~±0.01.</p>
<p>Extensions &amp; Variations</p>
<p>Weighted coin (p=['H':0.3,'T':0.7])</p>
<p>Plot bar chart of counts</p>
<p>Additional Notes &amp; Tips
Use np.random.seed(…) for reproducibility.</p>
<p>Exercise 3: Histogram of Die Rolls
Overview &amp; Purpose
Visualize the distribution of the 1,000 die rolls from Exercise 1.</p>
<p>Concept Reinforcement</p>
<p>Frequency vs. probability</p>
<p>Data visualization basics</p>
<p>Real World Relevance</p>
<p>Sales distribution by category</p>
<p>Error rates by batch</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
import matplotlib.pyplot as plt</p>
<p>plt.hist(rolls, bins=range(1,8), align='left', rwidth=0.8)
plt.xlabel('Die Face')
plt.ylabel('Frequency')
plt.title('Histogram of 1,000 Die Rolls')
plt.show()
Expected Outcomes &amp; Interpretation
Bars roughly equal height for faces 1–6.</p>
<p>Extensions &amp; Variations</p>
<p>Normalized histogram (density=True)</p>
<p>Overlay theoretical PMF</p>
<p>Additional Notes &amp; Tips
Ensure bins=range(1,8) to center on integer faces.</p>
<p>Exercise 4: Exponential Distribution Simulation
Overview &amp; Purpose
Simulate 5,000 samples from an exponential distribution (mean = 2) and compute mean/variance.</p>
<p>Concept Reinforcement</p>
<p>Continuous random variables</p>
<p>Relationship between distribution parameters and statistics</p>
<p>Real World Relevance</p>
<p>Time between machine failures</p>
<p>Call-center interarrival times</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
samples = np.random.exponential(scale=2, size=5000)
print("Empirical Mean:", samples.mean())
print("Empirical Variance:", samples.var())
Expected Outcomes &amp; Interpretation
Mean ≈ 2; variance ≈ 4.</p>
<p>Extensions &amp; Variations</p>
<p>Change scale (mean) parameter</p>
<p>Plot histogram + theoretical PDF</p>
<p>Additional Notes &amp; Tips
Use np.histogram or matplotlib for PDF overlay.</p>
<p>Exercise 5: Normal Distribution Sampling
Overview &amp; Purpose
Draw 10,000 samples from a standard normal (mean 0, σ = 1) and verify statistics.</p>
<p>Concept Reinforcement</p>
<p>Properties of Gaussian distribution</p>
<p>Central Limit Theorem preview</p>
<p>Real World Relevance</p>
<p>Measurement errors</p>
<p>Standardized test score modeling</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
normals = np.random.randn(10000)
print("Mean:", normals.mean())
print("Variance:", normals.var())
Expected Outcomes &amp; Interpretation
Mean ≈ 0, variance ≈ 1.</p>
<p>Extensions &amp; Variations</p>
<p>Use np.random.normal(loc, scale, size)</p>
<p>QQ plot vs. theoretical normal</p>
<p>Additional Notes &amp; Tips
Matplotlib’s plt.hist(..., density=True) for PDF shape.</p>
<p>Exercise 6: Sampling Distribution of the Mean
Overview &amp; Purpose
Simulate 1,000 experiments, each of 100 die rolls, record sample means, and examine their distribution.</p>
<p>Concept Reinforcement</p>
<p>Law of Large Numbers</p>
<p>Sampling variability reduction</p>
<p>Real World Relevance</p>
<p>Polling averages</p>
<p>Quality metrics over batches</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
means = [np.random.randint(1,7,100).mean() for _ in range(1000)]
plt.hist(means, bins=20)
plt.title('Sampling Distribution of Die Roll Means')
plt.show()
Expected Outcomes &amp; Interpretation
Histogram approximates normal around 3.5 with narrower spread.</p>
<p>Extensions &amp; Variations</p>
<p>Vary experiment size (n=10, n=1000)</p>
<p>Compute standard error (σ/√n)</p>
<p>Additional Notes &amp; Tips
List comprehensions vs. loops for clarity.</p>
<p>Exercise 7: Weighted Dice Simulation
Overview &amp; Purpose
Simulate 1,000 rolls of a biased die with 
𝑃
(
6
)
=
0.5
P(6)=0.5, others equal.</p>
<p>Concept Reinforcement</p>
<p>Custom discrete distributions</p>
<p>Impact of bias on mean/variance</p>
<p>Real World Relevance</p>
<p>Biased processes in manufacturing</p>
<p>Skewed customer behavior models</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
faces = [1,2,3,4,5,6]
probs = [0.1]*5 + [0.5]
rolls_biased = np.random.choice(faces, size=1000, p=probs)
print("Mean:", rolls_biased.mean(), "Variance:", rolls_biased.var())
Expected Outcomes &amp; Interpretation
Mean &gt; 3.5, variance different from fair die.</p>
<p>Extensions &amp; Variations</p>
<p>Tune probs for different biases</p>
<p>Compare histograms side by side</p>
<p>Additional Notes &amp; Tips
Sum of probs must equal 1.</p>
<p>Exercise 8: Vector Addition &amp; Scaling
Overview &amp; Purpose
Demonstrate vector addition and scalar multiplication with feature vectors.</p>
<p>Concept Reinforcement</p>
<p>Vector space operations</p>
<p>Geometric interpretation</p>
<p>Real World Relevance</p>
<p>Combining feature influences</p>
<p>Scaling normalized data</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
v1 = np.array([2, 4, 6])
v2 = np.array([1, 3, 5])
sum_v   = v1 + v2      # vector addition
scaled_v = 0.5 * v1    # scalar multiplication
print("Sum:", sum_v)
print("Scaled:", scaled_v)
Expected Outcomes &amp; Interpretation
Sum = [3, 7, 11]; scaled = [1, 2, 3].</p>
<p>Extensions &amp; Variations</p>
<p>Compute dot product of sum_v and v2</p>
<p>Visualize vectors in 2D/3D</p>
<p>Additional Notes &amp; Tips
Ensure consistent dimensions.</p>
<p>Exercise 9: Matrix Multiplication Demonstration
Overview &amp; Purpose
Multiply a 2×3 matrix by a 3×2 matrix to reinforce matrix multiplication rules.</p>
<p>Concept Reinforcement</p>
<p>Shape compatibility</p>
<p>Summation over inner index</p>
<p>Real World Relevance</p>
<p>Transforming feature spaces</p>
<p>Composition of network layers</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
A = np.array([[1,2,3],[4,5,6]])
B = np.array([[7,8],[9,10],[11,12]])
C = A.dot(B)
print("Result:\n", C)
Expected Outcomes &amp; Interpretation
C = [[58, 64], [139, 154]].</p>
<p>Extensions &amp; Variations</p>
<p>Reverse multiplication to show error</p>
<p>Use @ operator in Python 3.5+</p>
<p>Additional Notes &amp; Tips
Check shapes via A.shape and B.shape.</p>
<p>Exercise 10: PCA on Toy Dataset
Overview &amp; Purpose
Perform PCA on a small 2‑D dataset to reduce to 1‑D and visualize variance capture.</p>
<p>Concept Reinforcement</p>
<p>Eigenvectors/eigenvalues</p>
<p>Dimensionality reduction</p>
<p>Real World Relevance</p>
<p>Compressing image data</p>
<p>Feature extraction for clustering</p>
<p>Step by Step Instructions</p>
<p>python
Copy
Edit
from sklearn.decomposition import PCA
import numpy as np</p>
<p>X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0]])
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Projected data:\n", X_pca)
Expected Outcomes &amp; Interpretation
Most variance captured in first component (≈98%).</p>
<p>Extensions &amp; Variations</p>
<p>Plot original vs. projected points</p>
<p>Try n_components=2</p>
<p>Additional Notes &amp; Tips
Requires scikit‑learn installation.</p>
<ol>
<li>Summary of Week 1
Throughout this first week, you have:</li>
</ol>
<p>Traced AI History: From the Dartmouth Workshop to Expert Systems (MYCIN) and the Deep Learning revolution (AlexNet).</p>
<p>Built Probability Skills: Learned random variables, expectation, variance—both by hand and in Python (die rolls, coin flips, exponential and normal sampling).</p>
<p>Visualized Data: Plotted histograms, demonstrated Central Limit Theorem.</p>
<p>Handled Bias: Modeled a weighted die to see effects on distribution.</p>
<p>Applied Linear Algebra: Performed vector ops, matrix multiplication, and PCA for dimensionality reduction.</p>
<p>These foundational concepts and hands‑on exercises prepare you for more advanced AI and ML topics.</p>
<ol>
<li>Additional Resources
Probability Fundamentals: Khan Academy “Introduction to Probability”</li>
</ol>
<p>Linear Algebra Visualizations: 3Blue1Brown “Essence of Linear Algebra” series</p>
<p>Python Tutorials: Official Python documentation at python.org</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>